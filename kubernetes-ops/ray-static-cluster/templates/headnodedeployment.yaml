apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: ray
  name: {{ required "A valid .clusterName value is required!" .Values.clusterName }}-ray-head
spec:
  # Do not change this - Ray currently only supports one head node per cluster.
  replicas: 1
  selector:
    matchLabels:
      component: {{ required "A valid .clusterName value is required!" .Values.clusterName }}-ray-head
      type: ray
  template:
    metadata:
      labels:
        component: {{ required "A valid .clusterName value is required!" .Values.clusterName }}-ray-head
        type: ray
    spec:
      # Use Daft service account that should be linked to correct IAM Role via IRSA
      serviceAccountName: {{ required "A valid .clusterName value is required!" .Values.clusterName }}-daft

      # If the head node goes down, the entire cluster (including all worker
      # nodes) will go down as well. If you want Kubernetes to bring up a new
      # head node in this case, set this to "Always," else set it to "Never."
      restartPolicy: Always

      # This volume allocates shared memory for Ray to use for its plasma
      # object store. If you do not provide this, Ray will fall back to
      # /tmp which cause slowdowns if is not a shared memory volume.
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory

      containers:
        - name: ray-head
          image: {{ .Values.rayImage }}
          imagePullPolicy: IfNotPresent
          command: [ "/bin/bash", "-c", "--" ]
          args:
            - "ray start --head --port=6379 --redis-shard-ports=6380,6381 --num-cpus=$MY_CPU_REQUEST --object-manager-port=22345 --node-manager-port=22346 --dashboard-host=0.0.0.0 --block"
          ports:
            - containerPort: 6379 # Redis port
            - containerPort: 10001 # Used by Ray Client
            - containerPort: 8265 # Used by Ray Dashboard

          # This volume allocates shared memory for Ray to use for its plasma
          # object store. If you do not provide this, Ray will fall back to
          # /tmp which cause slowdowns if is not a shared memory volume.
          volumeMounts:
            - mountPath: /dev/shm
              name: dshm
          env:
            # This is used in the ray start command so that Ray can spawn the
            # correct number of processes. Omitting this may lead to degraded
            # performance.
            - name: MY_CPU_REQUEST
              value: {{ .Values.workerCPURequest | quote }}
          # Pod anti-affinities unfortunately don't trigger cluster autoscaler scale-ups
          # so we employ a hack here to request for almost the whole node's CPUs to ensure
          # that only one Ray pod is run per node
          resources:
            requests:
              cpu: {{ .Values.workerCPURequest | add -1 | quote }}
              memory: "{{ .Values.workerMemRequestG }}G"
            limits:
              cpu: {{ .Values.workerCPURequest | add -1 | quote }}
              memory: "{{ .Values.workerMemRequestG }}G"
      
      # Use tolerations and nodeSelector to schedule Ray pods on nodes dedicated to running Ray
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "ray"
          effect: "NoSchedule"
      nodeSelector:
        dedicated: ray
