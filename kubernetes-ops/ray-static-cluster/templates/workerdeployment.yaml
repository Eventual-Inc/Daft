apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: ray
  name: {{ required "A valid .clusterName value is required!" .Values.clusterName }}-ray-worker
spec:
  # Change this to scale the number of worker nodes started in the Ray cluster.
  replicas: {{ .Values.workerReplicaCount }}
  selector:
    matchLabels:
      component: ray-worker
      type: ray
  template:
    metadata:
      labels:
        component: ray-worker
        type: ray
    spec:
      # Use Daft service account that should be linked to correct IAM Role via IRSA
      serviceAccountName: {{ required "A valid .clusterName value is required!" .Values.clusterName }}-daft
      restartPolicy: Always
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
      containers:
      - name: ray-worker
        image: {{ .Values.rayImage }}
        imagePullPolicy: IfNotPresent
        command: ["/bin/bash", "-c", "--"]
        args:
          - ray start --num-cpus=$MY_CPU_REQUEST --address={{ required "A valid .clusterName value is required!" .Values.clusterName }}-ray-head-svc:6379 --object-manager-port=22345 --node-manager-port=22346 --block
        # This volume allocates shared memory for Ray to use for its plasma
        # object store. If you do not provide this, Ray will fall back to
        # /tmp which cause slowdowns if is not a shared memory volume.
        volumeMounts:
          - mountPath: /dev/shm
            name: dshm
        env:
          # This is used in the ray start command so that Ray can spawn the
          # correct number of processes. Omitting this may lead to degraded
          # performance.
           - name: MY_CPU_REQUEST
             value: {{ .Values.workerCPURequest | quote }}
        # Pod anti-affinities unfortunately don't trigger cluster autoscaler scale-ups
        # so we employ a hack here to request for almost the whole node's CPUs to ensure
        # that only one Ray pod is run per node
        resources:
          requests:
            cpu: {{ .Values.workerCPURequest | add -1 | quote }}
            memory: "{{ .Values.workerMemRequestGi | add -4 }}Gi"

      # Use tolerations and nodeSelector to schedule Ray pods on nodes dedicated to running Ray
      tolerations:
        - key: "dedicated"
          operator: "Equal"
          value: "ray"
          effect: "NoSchedule"
      nodeSelector:
        dedicated: ray
