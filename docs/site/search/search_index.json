{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to Daft!</p> <p>Daft is a unified data engine for data engineering, analytics, and ML/AI. It exposes both SQL and Python DataFrame interfaces as first-class citizens and is written in Rust. Daft provides a snappy and delightful local interactive experience, but also seamlessly scales to petabyte-scale distributed workloads.</p>"},{"location":"#use-cases","title":"Use Cases","text":"<p> Data Engineering</p> <p>Combine the performance of DuckDB, Pythonic UX of Polars and scalability of Apache Spark for data engineering from MB to PB scale</p> <ul> <li>Scale ETL workflows effortlessly from local to distributed environments</li> <li>Enjoy a Python-first experience without JVM dependency hell</li> <li>Leverage native integrations with cloud storage, open catalogs, and data formats</li> </ul> <p> Data Analytics</p> <p>Blend the snappiness of DuckDB with the scalability of Spark/Trino for unified local and distributed analytics</p> <ul> <li>Utilize complementary SQL and Python interfaces for versatile analytics</li> <li>Perform snappy local exploration with DuckDB-like performance</li> <li>Seamlessly scale to the cloud, outperforming distributed engines like Spark and Trino</li> </ul> <p> ML/AI</p> <p>Streamline ML/AI workflows with efficient dataloading from open formats like Parquet and JPEG</p> <ul> <li>Load data efficiently from open formats directly into PyTorch or NumPy</li> <li>Schedule large-scale model batch inference on distributed GPU clusters</li> <li>Optimize data curation with advanced clustering, deduplication, and filtering</li> </ul>"},{"location":"#technology","title":"Technology","text":"<p>Daft boasts strong integrations with technologies common across these workloads:</p> <ul> <li>Cloud Object Storage: Record-setting I/O performance for integrations with S3 cloud storage, battle-tested at exabyte-scale at Amazon</li> <li>ML/AI Python Ecosystem: First-class integrations with PyTorch and NumPy for efficient interoperability with your ML/AI stack</li> <li>Data Catalogs/Table Formats: Capabilities to effectively query table formats such as Apache Iceberg, Delta Lake and Apache Hudi</li> <li>Seamless Data Interchange: Zero-copy integration with Apache Arrow</li> <li>Multimodal/ML Data: Native functionality for data modalities such as tensors, images, URLs, long-form text and embeddings</li> </ul>"},{"location":"#learning-daft","title":"Learning Daft","text":"<p>This user guide aims to help Daft users master the usage of Daft for all your data needs.</p> <p>Looking to get started with Daft ASAP?</p> <p>The Daft User Guide is a useful resource to take deeper dives into specific Daft concepts, but if you are ready to jump into code you may wish to take a look at these resources:</p> <ol> <li> <p>10 minute Quickstart: Itching to run some Daft code? Hit the ground running with our 10 minute quickstart notebook.</p> </li> <li> <p>API Documentation: Searchable documentation and reference material to Daft\u2019s public API.</p> </li> </ol>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li> <p> Installing Daft</p> <p>Install Daft from your terminal and discover more advanced installation options.</p> </li> <li> <p> Quickstart</p> <p>Install Daft, create your first DataFrame, and get started with common DataFrame operations.</p> </li> <li> <p> Terminology</p> <p>Learn about the terminology related to Daft, such as DataFrames, Expressions, Query Plans, and more.</p> </li> <li> <p> Architecture</p> <p>Understand the different components to Daft under-the-hood.</p> </li> </ul>"},{"location":"#daft-in-depth","title":"Daft in Depth","text":"<ul> <li> <p> DataFrame Operations</p> <p>Learn how to perform core DataFrame operations in Daft, including selection, filtering, joining, and sorting.</p> </li> <li> <p> Expressions</p> <p>Daft expressions enable computations on DataFrame columns using Python or SQL for various operations.</p> </li> <li> <p> Reading Data</p> <p>How to use Daft to read data from diverse sources like files, databases, and URLs.</p> </li> <li> <p> Writing Data</p> <p>How to use Daft to write data DataFrames to files or other destinations.</p> </li> <li> <p> DataTypes</p> <p>Daft DataTypes define the types of data in a DataFrame, from simple primitives to complex structures.</p> </li> <li> <p> SQL</p> <p>Daft supports SQL for constructing query plans and expressions, while integrating with Python expressions.</p> </li> <li> <p> Aggregations and Grouping</p> <p>Daft supports aggregations and grouping across entire DataFrames and within grouped subsets of data.</p> </li> <li> <p> User-Defined Functions (UDFs)</p> <p>Daft allows you to define custom UDFs to process data at scale with flexibility in input and output.</p> </li> <li> <p> Multimodal Data</p> <p>Daft is built to work with multimodal data types, including URLs and images.</p> </li> </ul>"},{"location":"#more-resources","title":"More Resources","text":"<ul> <li> Advanced Daft</li> <li> DataFrame Comparison</li> <li> Tutorials</li> <li> Benchmarks</li> </ul>"},{"location":"#contribute-to-daft","title":"Contribute to Daft","text":"<p>If you're interested in hands-on learning about Daft internals and would like to contribute to our project, join us on Github \ud83d\ude80</p> <p>Take a look at the many issues tagged with <code>good first issue</code> in our repo. If there are any that interest you, feel free to chime in on the issue itself or join us in our Distributed Data Slack Community and send us a message in #daft-dev. Daft team members will be happy to assign any issue to you and provide any guidance if needed!</p>"},{"location":"#frequently-asked-questions","title":"Frequently Asked Questions","text":"<p>todo(docs): Add answers to each and more questions if necessary</p> What does Daft do well? (or What should I use Daft for?) <p>Daft is the right tool for you if you are working with:</p> <ul> <li>Large datasets that don't fit into memory or would benefit from parallelization</li> <li>Multimodal data types such as images, JSON, vector embeddings, and tensors</li> <li>Formats that support data skipping through automatic partition pruning and stats-based file pruning for filter predicates</li> <li>ML workloads that would benefit from interact computation within a DataFrame (via UDFs)</li> </ul> What should I not use Daft for? How do I know if Daft is the right framework for me? <p>See DataFrame Comparison</p> What is the difference between Daft and Ray? What is the difference between Daft and Spark? How does Daft perform at large scales vs other data engines? <p>See Benchmarks</p> What is the technical architecture of Daft? <p>See Technical Architecture</p> Does Daft perform any telemetry? <p>See Telemetry</p>"},{"location":"10min/","title":"10 minutes Quickstart","text":"<p>\ud83d\udca1 Hint</p> <p>\u2728\u2728 Run this notebook on Google Colab \u2728\u2728</p> <p>You can run this notebook yourself with Google Colab.</p> In\u00a0[\u00a0]: Copied! <pre>%pip install getdaft\n</pre> %pip install getdaft <p>And then import Daft and some of its classes which we'll need later on:</p> In\u00a0[1]: Copied! <pre>import daft\nfrom daft import DataType, udf\n</pre> import daft from daft import DataType, udf In\u00a0[2]: Copied! <pre>import datetime\n\ndf = daft.from_pydict(\n    {\n        \"integers\": [1, 2, 3, 4],\n        \"floats\": [1.5, 2.5, 3.5, 4.5],\n        \"bools\": [True, True, False, False],\n        \"strings\": [\"a\", \"b\", \"c\", \"d\"],\n        \"bytes\": [b\"a\", b\"b\", b\"c\", b\"d\"],\n        \"dates\": [\n            datetime.date(1994, 1, 1),\n            datetime.date(1994, 1, 2),\n            datetime.date(1994, 1, 3),\n            datetime.date(1994, 1, 4),\n        ],\n        \"lists\": [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]],\n        \"nulls\": [None, None, None, None],\n    }\n)\n\ndf\n</pre> import datetime  df = daft.from_pydict(     {         \"integers\": [1, 2, 3, 4],         \"floats\": [1.5, 2.5, 3.5, 4.5],         \"bools\": [True, True, False, False],         \"strings\": [\"a\", \"b\", \"c\", \"d\"],         \"bytes\": [b\"a\", b\"b\", b\"c\", b\"d\"],         \"dates\": [             datetime.date(1994, 1, 1),             datetime.date(1994, 1, 2),             datetime.date(1994, 1, 3),             datetime.date(1994, 1, 4),         ],         \"lists\": [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]],         \"nulls\": [None, None, None, None],     } )  df Out[2]: integersInt64floatsFloat64boolsBooleanstringsUtf8bytesBinarydatesDatelistsList[Int64]nullsNull 11.5trueab\"a\"1994-01-01[1, 1, 1]None 22.5truebb\"b\"1994-01-02[2, 2, 2]None 33.5falsecb\"c\"1994-01-03[3, 3, 3]None 44.5falsedb\"d\"1994-01-04[4, 4, 4]None (Showing first 4 of 4 rows) <p>Nice. If you've worked with DataFrame libraries like pandas, Dask or Spark this should look familiar.</p> <p>Daft is built for multimodal data type support. Daft DataFrames can contain more data types than other DataFrame APIs like pandas, Spark or Dask. Daft columns can contain URLs, images, tensors and Python classes. You'll get to work with some of these data types in a moment.</p> <p>For a complete list of supported data types see: API Reference: DataTypes</p> <p>Let's read in a Parquet file from a public S3 bucket. Note that this Parquet file is partitioned on the <code>country</code> column. This will be important later on.</p> In\u00a0[3]: Copied! <pre># Set IO Configurations to use anonymous data access mode\ndaft.set_planning_config(default_io_config=daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True)))\n\ndf = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf\n</pre> # Set IO Configurations to use anonymous data access mode daft.set_planning_config(default_io_config=daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True)))  df = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\") df Out[3]: first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean (No data to display: Dataframe not materialized) In\u00a0[4]: Copied! <pre>df.collect()\n</pre> df.collect() <pre>ScanWithTask [Stage:1]:   0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> Out[4]: first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean ShandraShamas571967-01-02United Kingdomtrue ZayaZaphora401984-04-07United Kingdomtrue WolfgangWinter232001-02-12GermanyNone ErnestoEvergreen341990-04-03Canadatrue JamesJale621962-03-24Canadatrue (Showing first 5 of 5 rows) <p>You can also take a look at just the first few rows with the {meth}<code>df.show() &lt;daft.DataFrame.show&gt;</code> method:</p> In\u00a0[5]: Copied! <pre>df.show(3)\n</pre> df.show(3) first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean ShandraShamas571967-01-02United Kingdomtrue ZayaZaphora401984-04-07United Kingdomtrue WolfgangWinter232001-02-12GermanyNone (Showing first 3 of 5 rows) <p>Use <code>.show</code> for quick visualisation in an interactive notebook.</p> <p>You can select specific columns from your DataFrame with the {meth}<code>df.select() &lt;daft.DataFrame.select&gt;</code>  method:</p> In\u00a0[6]: Copied! <pre>df.select(\"first_name\", \"has_dog\").show()\n</pre> df.select(\"first_name\", \"has_dog\").show() first_nameUtf8has_dogBoolean Shandratrue Zayatrue WolfgangNone Ernestotrue Jamestrue (Showing first 5 of 5 rows) In\u00a0[7]: Copied! <pre>df.limit(1).show()\n</pre> df.limit(1).show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean ShandraShamas571967-01-02United Kingdomtrue (Showing first 1 of 1 rows) <p>To drop columns from the dataframe, call {meth}<code>df.exclude() &lt;daft.DataFrame.exclude&gt;</code>:</p> In\u00a0[8]: Copied! <pre>df.exclude(\"DoB\").show()\n</pre> df.exclude(\"DoB\").show() first_nameUtf8last_nameUtf8ageInt64countryUtf8has_dogBoolean ShandraShamas57United Kingdomtrue ZayaZaphora40United Kingdomtrue WolfgangWinter23GermanyNone ErnestoEvergreen34Canadatrue JamesJale62Canadatrue (Showing first 5 of 5 rows) <p>See: Expressions</p> <p>Expressions are an API for defining computation that needs to happen over your columns.</p> <p>For example, use the {meth}<code>daft.col() &lt;daft.col&gt;</code> expression together with the <code>with_column</code> method to create a new column <code>full_name</code>, joining the contents of the <code>last_name</code> column to the <code>first_name</code> column:</p> In\u00a0[9]: Copied! <pre>df = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\"))\ndf.select(\"full_name\", \"age\", \"country\", \"has_dog\").show()\n</pre> df = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\")) df.select(\"full_name\", \"age\", \"country\", \"has_dog\").show() full_nameUtf8ageInt64countryUtf8has_dogBoolean Shandra Shamas57United Kingdomtrue Zaya Zaphora40United Kingdomtrue Wolfgang Winter23GermanyNone Ernesto Evergreen34Canadatrue James Jale62Canadatrue (Showing first 5 of 5 rows) <p>Alternatively, you can also run your column transforms using Expressions directly inside your <code>select</code> call:</p> In\u00a0[17]: Copied! <pre>df.select((daft.col(\"first_name\") + \" \" + daft.col(\"last_name\")), \"age\", \"country\").show()\n</pre> df.select((daft.col(\"first_name\") + \" \" + daft.col(\"last_name\")), \"age\", \"country\").show() first_nameUtf8ageInt64countryUtf8 Shandra Shamas57United Kingdom Zaya Zaphora40United Kingdom Wolfgang Winter23Germany Ernesto Evergreen34Canada James Jale62Canada (Showing first 5 of 5 rows) <p>Some Expression methods are only allowed on certain types and are accessible through \"method accessors\" (see: Expression Accessor Properties).</p> <p>For example, the <code>.dt.year()</code> expression is only valid when run on a <code>datetime</code> column.</p> <p>Below we use an Expression to extract the year from a <code>datetime</code> column:</p> In\u00a0[10]: Copied! <pre>df_year = df.with_column(\"DoB_year\", df[\"DoB\"].dt.year())\ndf_year.show()\n</pre> df_year = df.with_column(\"DoB_year\", df[\"DoB\"].dt.year()) df_year.show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBooleanfull_nameUtf8DoB_yearInt32 ShandraShamas571967-01-02United KingdomtrueShandra Shamas1967 ZayaZaphora401984-04-07United KingdomtrueZaya Zaphora1984 WolfgangWinter232001-02-12GermanyNoneWolfgang Winter2001 ErnestoEvergreen341990-04-03CanadatrueErnesto Evergreen1990 JamesJale621962-03-24CanadatrueJames Jale1962 (Showing first 5 of 5 rows) In\u00a0[11]: Copied! <pre>df.sort(df[\"age\"], desc=False).show()\n</pre> df.sort(df[\"age\"], desc=False).show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBooleanfull_nameUtf8 WolfgangWinter232001-02-12GermanyNoneWolfgang Winter ErnestoEvergreen341990-04-03CanadatrueErnesto Evergreen ZayaZaphora401984-04-07United KingdomtrueZaya Zaphora ShandraShamas571967-01-02United KingdomtrueShandra Shamas JamesJale621962-03-24CanadatrueJames Jale (Showing first 5 of 5 rows) In\u00a0[12]: Copied! <pre># select only columns for grouping\ngrouping_df = df.select(df[\"country\"], df[\"first_name\"].alias(\"counts\"))\n\n# groupby country column and count the number of countries\ngrouping_df.groupby(df[\"country\"]).count().show()\n</pre> # select only columns for grouping grouping_df = df.select(df[\"country\"], df[\"first_name\"].alias(\"counts\"))  # groupby country column and count the number of countries grouping_df.groupby(df[\"country\"]).count().show() countryUtf8countsUInt64 Canada2 Germany1 United Kingdom2 (Showing first 3 of 3 rows) <p>Note that we can use {meth}<code>.alias() &lt;daft.Expression.alias&gt;</code> to quickly rename columns.</p> In\u00a0[13]: Copied! <pre>missing_data_df = daft.from_pydict(\n    {\n        \"floats\": [1.5, None, float(\"nan\")],\n    }\n)\nmissing_data_df = missing_data_df.with_column(\"floats_is_null\", missing_data_df[\"floats\"].is_null()).with_column(\n    \"floats_is_nan\", missing_data_df[\"floats\"].float.is_nan()\n)\n\nmissing_data_df.show()\n</pre> missing_data_df = daft.from_pydict(     {         \"floats\": [1.5, None, float(\"nan\")],     } ) missing_data_df = missing_data_df.with_column(\"floats_is_null\", missing_data_df[\"floats\"].is_null()).with_column(     \"floats_is_nan\", missing_data_df[\"floats\"].float.is_nan() )  missing_data_df.show() floatsFloat64floats_is_nullBooleanfloats_is_nanBoolean 1.5falsefalse NonetrueNone NaNfalsetrue (Showing first 3 of 3 rows) <p>Let's correct the one missing value in our dataset:</p> In\u00a0[14]: Copied! <pre>df = df.with_column(\"has_dog\", df[\"has_dog\"].is_null().if_else(True, df[\"has_dog\"]))\ndf.show()\n</pre> df = df.with_column(\"has_dog\", df[\"has_dog\"].is_null().if_else(True, df[\"has_dog\"])) df.show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBooleanfull_nameUtf8 ShandraShamas571967-01-02United KingdomtrueShandra Shamas ZayaZaphora401984-04-07United KingdomtrueZaya Zaphora WolfgangWinter232001-02-12GermanytrueWolfgang Winter ErnestoEvergreen341990-04-03CanadatrueErnesto Evergreen JamesJale621962-03-24CanadatrueJames Jale (Showing first 5 of 5 rows) In\u00a0[16]: Copied! <pre>df.where(df[\"age\"] &gt; 35).show()\n</pre> df.where(df[\"age\"] &gt; 35).show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBooleanfull_nameUtf8 JamesJale621962-03-24CanadatrueJames Jale ShandraShamas571967-01-02United KingdomtrueShandra Shamas ZayaZaphora401984-04-07United KingdomtrueZaya Zaphora (Showing first 3 of 3 rows) <p>Filtering can give you powerful optimization when you are working with partitioned files or tables. Daft will use the predicate to read only the necessary partitions, skipping any data that is not relevant.</p> <p>For example, our Parquet file is partitioned on the <code>country</code> column. This means that queries with a <code>country</code> predicate will benefit from query optimization:</p> In\u00a0[17]: Copied! <pre>df.where(df[\"country\"] == \"Canada\").show()\n</pre> df.where(df[\"country\"] == \"Canada\").show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBooleanfull_nameUtf8 ErnestoEvergreen341990-04-03CanadatrueErnesto Evergreen JamesJale621962-03-24CanadatrueJames Jale (Showing first 2 of 2 rows) <p>Daft only needs to read in 1 file for this query, instead of 3.</p> In\u00a0[18]: Copied! <pre>df2 = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf2.where(df[\"country\"] == \"Canada\").explain(show_all=True)\n</pre> df2 = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\") df2.where(df[\"country\"] == \"Canada\").explain(show_all=True) <pre>== Unoptimized Logical Plan ==\n\n* Filter: col(country) == lit(\"Canada\")\n|\n* GlobScanOperator\n|   Glob paths = [s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-\n|     partitioned.pq/**]\n|   Coerce int96 timestamp unit = Nanoseconds\n|   IO config = S3 config = { Max connections = 8, Retry initial backoff ms = 1000,\n|     Connect timeout ms = 30000, Read timeout ms = 30000, Max retries = 25, Retry\n|     mode = adaptive, Anonymous = false, Use SSL = true, Verify SSL = true, Check\n|     hostname SSL = true, Requester pays = false, Force Virtual Addressing = false },\n|     Azure config = { Anonymous = false, Use SSL = true }, GCS config = { Anonymous =\n|     false }, HTTP config = { user_agent = daft/0.0.1 }\n|   Use multithreading = true\n|   File schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n|   Partitioning keys = []\n|   Output schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n\n\n== Optimized Logical Plan ==\n\n* GlobScanOperator\n|   Glob paths = [s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-\n|     partitioned.pq/**]\n|   Coerce int96 timestamp unit = Nanoseconds\n|   IO config = S3 config = { Max connections = 8, Retry initial backoff ms = 1000,\n|     Connect timeout ms = 30000, Read timeout ms = 30000, Max retries = 25, Retry\n|     mode = adaptive, Anonymous = false, Use SSL = true, Verify SSL = true, Check\n|     hostname SSL = true, Requester pays = false, Force Virtual Addressing = false },\n|     Azure config = { Anonymous = false, Use SSL = true }, GCS config = { Anonymous =\n|     false }, HTTP config = { user_agent = daft/0.0.1 }\n|   Use multithreading = true\n|   File schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n|   Partitioning keys = []\n|   Filter pushdown = col(country) == lit(\"Canada\")\n|   Output schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n\n\n== Physical Plan ==\n\n* TabularScan:\n|   Num Scan Tasks = 1\n|   Estimated Scan Bytes = 6336\n|   Clustering spec = { Num partitions = 1 }\n\n</pre> <p>Because we are filtering our DataFrame on the partition column <code>country</code>, Daft can optimize the Logical Plan and save us time and computing resources by only reading a single partition from disk.</p> In\u00a0[19]: Copied! <pre>df_dogs = daft.from_pydict(\n    {\n        \"urls\": [\n            \"https://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpg\",\n            \"https://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpg\",\n            \"https://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpg\",\n            \"https://live.staticflickr.com/65535/53671838039_b97411a441_o.jpg\",\n            \"https://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpg\",\n        ],\n        \"full_name\": [\n            \"Ernesto Evergreen\",\n            \"James Jale\",\n            \"Wolfgang Winter\",\n            \"Shandra Shamas\",\n            \"Zaya Zaphora\",\n        ],\n        \"dog_name\": [\"Ernie\", \"Jackie\", \"Wolfie\", \"Shaggie\", \"Zadie\"],\n    }\n)\n</pre> df_dogs = daft.from_pydict(     {         \"urls\": [             \"https://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpg\",             \"https://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpg\",             \"https://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpg\",             \"https://live.staticflickr.com/65535/53671838039_b97411a441_o.jpg\",             \"https://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpg\",         ],         \"full_name\": [             \"Ernesto Evergreen\",             \"James Jale\",             \"Wolfgang Winter\",             \"Shandra Shamas\",             \"Zaya Zaphora\",         ],         \"dog_name\": [\"Ernie\", \"Jackie\", \"Wolfie\", \"Shaggie\", \"Zadie\"],     } ) <p>Let's join and drop some columns to keep the output easy to read:</p> In\u00a0[20]: Copied! <pre>df_family = df.join(df_dogs, on=\"full_name\").exclude(\"first_name\", \"last_name\", \"DoB\", \"country\", \"age\")\ndf_family.show()\n</pre> df_family = df.join(df_dogs, on=\"full_name\").exclude(\"first_name\", \"last_name\", \"DoB\", \"country\", \"age\") df_family.show() has_dogBooleanfull_nameUtf8urlsUtf8dog_nameUtf8 trueErnesto Evergreenhttps://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpgErnie trueJames Jalehttps://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpgJackie trueWolfgang Winterhttps://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpgWolfie trueShandra Shamashttps://live.staticflickr.com/65535/53671838039_b97411a441_o.jpgShaggie trueZaya Zaphorahttps://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpgZadie (Showing first 5 of 5 rows) <p>Let's just quickly re-order the columns for easier reading:</p> In\u00a0[21]: Copied! <pre>df_family = df_family.select(\"full_name\", \"has_dog\", \"dog_name\", \"urls\")\ndf_family.show()\n</pre> df_family = df_family.select(\"full_name\", \"has_dog\", \"dog_name\", \"urls\") df_family.show() full_nameUtf8has_dogBooleandog_nameUtf8urlsUtf8 Ernesto EvergreentrueErniehttps://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpg James JaletrueJackiehttps://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpg Wolfgang WintertrueWolfiehttps://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpg Shandra ShamastrueShaggiehttps://live.staticflickr.com/65535/53671838039_b97411a441_o.jpg Zaya ZaphoratrueZadiehttps://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpg (Showing first 5 of 5 rows) <p>Daft is built to work comfortably with multimodal data types, including URLs and Images.</p> <p>You can use the {meth}<code>url.download() &lt;daft.Expression.url.download&gt;</code> expression to download the bytes from a URL. Let's store them in a new column using the <code>with_column</code> method:</p> In\u00a0[22]: Copied! <pre>df_family = df_family.with_column(\"image_bytes\", df_dogs[\"urls\"].url.download(on_error=\"null\"))\ndf_family.show()\n</pre> df_family = df_family.with_column(\"image_bytes\", df_dogs[\"urls\"].url.download(on_error=\"null\")) df_family.show() full_nameUtf8has_dogBooleandog_nameUtf8urlsUtf8image_bytesBinary Ernesto EvergreentrueErniehttps://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... James JaletrueJackiehttps://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... Wolfgang WintertrueWolfiehttps://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... Shandra ShamastrueShaggiehttps://live.staticflickr.com/65535/53671838039_b97411a441_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... Zaya ZaphoratrueZadiehttps://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... (Showing first 5 of 5 rows) <p>Great! But where's the fluffiness? \ud83d\ude41</p> <p>Let's turn the bytes into human-readable images using <code>image.decode</code>:</p> In\u00a0[23]: Copied! <pre>df_family = df_family.with_column(\"image\", daft.col(\"image_bytes\").image.decode())\ndf_family.show()\n</pre> df_family = df_family.with_column(\"image\", daft.col(\"image_bytes\").image.decode()) df_family.show() full_nameUtf8has_dogBooleandog_nameUtf8urlsUtf8image_bytesBinaryimageImage[MIXED] Ernesto EvergreentrueErniehttps://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... James JaletrueJackiehttps://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... Wolfgang WintertrueWolfiehttps://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... Shandra ShamastrueShaggiehttps://live.staticflickr.com/65535/53671838039_b97411a441_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... Zaya ZaphoratrueZadiehttps://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpgb\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... (Showing first 5 of 5 rows) <p>Woof! \ud83d\udc36</p> <p>You can use User-Defined Functions (UDFs) to run computations over multiple rows or columns.</p> <p>As the final part of this Quickstart, you'll build a Machine Learning model to classify our new fluffy friends by breed.</p> <p>Daft enables you to do all this right within our DataFrame, using UDFs.</p> <p>Working with PyTorch adds some complexity but you can just run the cells below to perform the classification.</p> <p>First, make sure to install and import some extra dependencies:</p> In\u00a0[\u00a0]: Copied! <pre>%pip install validators matplotlib Pillow torch torchvision\n</pre> %pip install validators matplotlib Pillow torch torchvision In\u00a0[24]: Copied! <pre># import additional libraries, these are necessary for PyTorch\n\nimport torch\n</pre> # import additional libraries, these are necessary for PyTorch  import torch <p>Then, go ahead and define your <code>ClassifyImages</code> UDF.</p> <p>Models are expensive to initialize and load, so we want to do this as few times as possible, and share a model across multiple invocations.</p> In\u00a0[25]: Copied! <pre>@udf(return_dtype=DataType.fixed_size_list(dtype=DataType.string(), size=2))\nclass ClassifyImages:\n    def __init__(self):\n        # Perform expensive initializations - create and load the pre-trained model\n        self.model = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resnet50\", pretrained=True)\n        self.utils = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_convnets_processing_utils\")\n        self.model.eval().to(torch.device(\"cpu\"))\n\n    def __call__(self, images_urls):\n        uris = images_urls.to_pylist()\n        batch = torch.cat([self.utils.prepare_input_from_uri(uri) for uri in uris]).to(torch.device(\"cpu\"))\n\n        with torch.no_grad():\n            output = torch.nn.functional.softmax(self.model(batch), dim=1)\n\n        results = self.utils.pick_n_best(predictions=output, n=1)\n        return [result[0] for result in results]\n</pre> @udf(return_dtype=DataType.fixed_size_list(dtype=DataType.string(), size=2)) class ClassifyImages:     def __init__(self):         # Perform expensive initializations - create and load the pre-trained model         self.model = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resnet50\", pretrained=True)         self.utils = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_convnets_processing_utils\")         self.model.eval().to(torch.device(\"cpu\"))      def __call__(self, images_urls):         uris = images_urls.to_pylist()         batch = torch.cat([self.utils.prepare_input_from_uri(uri) for uri in uris]).to(torch.device(\"cpu\"))          with torch.no_grad():             output = torch.nn.functional.softmax(self.model(batch), dim=1)          results = self.utils.pick_n_best(predictions=output, n=1)         return [result[0] for result in results] <p>Nice, now you're all set to call this function on the <code>urls</code> column and store the outputs in a new column we'll call <code>classify breeds</code>:</p> In\u00a0[26]: Copied! <pre>classified_images_df = df_family.with_column(\"classify_breed\", ClassifyImages(daft.col(\"urls\")))\n\nclassified_images_df.select(\"dog_name\", \"image\", \"classify_breed\").show()\n</pre> classified_images_df = df_family.with_column(\"classify_breed\", ClassifyImages(daft.col(\"urls\")))  classified_images_df.select(\"dog_name\", \"image\", \"classify_breed\").show() dog_nameUtf8imageImage[MIXED]classify_breedFixedSizeList[Utf8; 2] Ernie[boxer, 52.3%] Jackie[American Staffordshire terrier, Staffordshire terrier, American pit bull terrier, pit bull terrier, 42.4%] Wolfie[collie, 49.6%] Shaggie[standard schnauzer, 29.6%] Zadie[Rottweiler, 78.6%] (Showing first 5 of 5 rows) <p>Nice work!</p> <p>It looks like our pre-trained model is more familiar with some specific breeds. You could do further work to fine-tune this model to improve performance.</p> In\u00a0[39]: Copied! <pre>written_df = df.write_parquet(\"my-dataframe.parquet\")\n</pre> written_df = df.write_parquet(\"my-dataframe.parquet\") <pre>                                                                   \r</pre> <p>Note that writing your dataframe is a blocking operation that executes your DataFrame. It will return a new <code>DataFrame</code> that contains the filepaths to the written data:</p> In\u00a0[40]: Copied! <pre>written_df\n</pre> written_df Out[40]: pathUtf8 my-dataframe.parquet/36bdcc36-9fec-4be8-b22e-a792cc5c6c4c-0.parquet (Showing first 1 of 1 rows)"},{"location":"10min/#10-minutes-quickstart","title":"10 minutes Quickstart\u00b6","text":"<p>This is a short introduction to all the main functionality in Daft, geared towards new users.</p>"},{"location":"10min/#what-is-daft","title":"What is Daft?\u00b6","text":"<p>Daft is a distributed query engine built for running ETL, analytics, and ML/AI workloads at scale. Daft is implemented in Rust (fast!) and exposes a familiar Python dataframe API (friendly!).</p> <p>In this Quickstart you will learn the basics of Daft\u2019s DataFrame API and the features that set it apart from frameworks like pandas, pySpark, Dask and Ray. You will build a database of dog owners and their fluffy companions and see how you can use Daft to download images from URLs, run an ML classifier and call custom UDFs, all within an interactive DataFrame interface. Woof! \ud83d\udc36</p>"},{"location":"10min/#when-should-i-use-daft","title":"When Should I use Daft?\u00b6","text":"<p>Daft is the right tool for you if you are working with any of the following:</p> <ul> <li>Large datasets that don't fit into memory or would benefit from parallelization</li> <li>Multimodal data types such as images, JSON, vector embeddings, and tensors</li> <li>Formats that support data skipping through automatic partition pruning and stats-based file pruning for filter predicates</li> <li>ML workloads that would benefit from interactive computation within DataFrame (via UDFs)</li> </ul> <p>Read more about how Daft compares to other DataFrames in our FAQ.</p> <p>Let's jump in! \ud83e\ude82</p>"},{"location":"10min/#install-and-import-daft","title":"Install and Import Daft\u00b6","text":"<p>You can install Daft using <code>pip</code>:</p>"},{"location":"10min/#create-your-first-daft-dataframe","title":"Create your first Daft DataFrame\u00b6","text":"<p>See also: API Reference: DataFrame Construction</p> <p>To begin, let's create a DataFrame from a dictionary of columns:</p>"},{"location":"10min/#multimodal-data-types","title":"Multimodal Data Types\u00b6","text":""},{"location":"10min/#data-sources","title":"Data Sources\u00b6","text":"<p>You can also load DataFrames from other sources, such as:</p> <ol> <li>CSV files: {func}<code>daft.read_csv(\"s3://bucket/*.csv\") &lt;daft.read_csv&gt;</code></li> <li>Parquet files: {func}<code>daft.read_parquet(\"/path/*.parquet\") &lt;daft.read_parquet&gt;</code></li> <li>JSON line-delimited files: {func}<code>daft.read_json(\"/path/*.parquet\") &lt;daft.read_json&gt;</code></li> <li>Files on disk: {func}<code>daft.from_glob_path(\"/path/*.jpeg\") &lt;daft.from_glob_path&gt;</code></li> </ol> <p>Daft automatically supports local paths as well as paths to object storage such as AWS S3:</p> <pre><code>df = daft.read_json(\"s3://path/to/bucket/file.jsonl\")\n</code></pre> <p>See User Guide: Integrations to learn more about working with other formats like Delta Lake and Iceberg.</p>"},{"location":"10min/#executing-and-displaying-data","title":"Executing and Displaying Data\u00b6","text":"<p>Daft DataFrames are lazy by default. This means that the contents will not be computed (\"materialized\") unless you explicitly tell Daft to do so. This is best practice for working with larger-than-memory datasets and parallel/distributed architectures.</p> <p>The file we have just loaded only has 5 rows. You can materialize the whole DataFrame in memory easily using the {meth}<code>df.collect() &lt;daft.DataFrame.collect&gt;</code> method:</p>"},{"location":"10min/#basic-dataframe-operations","title":"Basic DataFrame Operations\u00b6","text":"<p>Let's take a look at some of the most common DataFrame operations.</p>"},{"location":"10min/#selecting-columns","title":"Selecting Columns\u00b6","text":""},{"location":"10min/#excluding-data","title":"Excluding Data\u00b6","text":"<p>You can limit the number of rows in a dataframe by calling {meth}<code>df.limit() &lt;daft.DataFrame.limit&gt;</code>. Use <code>limit</code> and not <code>show</code> when you want to return a limited number of rows for further transformation.</p>"},{"location":"10min/#transforming-columns-with-expressions","title":"Transforming Columns with Expressions\u00b6","text":""},{"location":"10min/#other-dataframe-operations","title":"Other DataFrame Operations\u00b6","text":""},{"location":"10min/#sorting-data","title":"Sorting Data\u00b6","text":"<p>You can sort a dataframe with {meth}<code>df.sort() &lt;daft.DataFrame.sort&gt;</code>, which we do so here in ascending order:</p>"},{"location":"10min/#grouping-and-aggregating-data","title":"Grouping and Aggregating Data\u00b6","text":"<p>You can group and aggregate your data using the {meth}<code>df.groupby() &lt;daft.DataFrame.groupby&gt;</code> method:</p> <p>Groupby aggregation operations over a dataset happens in 2 phases:</p> <ol> <li>Splitting the data into groups based on some criteria using {meth}<code>df.groupby() &lt;daft.DataFrame.groupby&gt;</code></li> <li>Specifying how to aggregate the data for each group using {meth}<code>GroupedDataFrame.agg() &lt;daft.DataFrame.dataframe.GroupedDataFrame.agg&gt;</code></li> </ol> <p>For example:</p>"},{"location":"10min/#missing-data","title":"Missing Data\u00b6","text":"<p>All columns in Daft are \"nullable\" by default. Unlike other frameworks such as Pandas, Daft differentiates between \"null\" (missing) and \"nan\" (stands for not a number - a special value indicating an invalid float).</p>"},{"location":"10min/#filtering-data","title":"Filtering Data\u00b6","text":"<p>You can filter rows in your DataFrame with a predicate using the {meth}<code>df.where() &lt;daft.DataFrame.where&gt;</code> method:</p>"},{"location":"10min/#query-planning","title":"Query Planning\u00b6","text":"<p>Daft is lazy: computations on your DataFrame are not executed immediately. Instead, Daft creates a <code>LogicalPlan</code> which defines the operations that need to happen to materialize the requested result. Think of this LogicalPlan as a recipe.</p> <p>You can examine this logical plan using {meth}<code>df.explain() &lt;daft.DataFrame.explain&gt;</code>:</p>"},{"location":"10min/#more-advanced-operations","title":"More Advanced Operations\u00b6","text":"<p>You've made it half-way! Time to bring in some fluffy beings \ud83d\udc36</p> <p>Let's bring all of the elements together to see how you can use Daft to:</p> <ul> <li>perform more advanced operations like joins</li> <li>work with multimodal data like Python classes, URLs, and Images,</li> <li>apply custom User-Defined Functions to your columns,</li> <li>and run ML workloads within your DataFrame,</li> </ul>"},{"location":"10min/#merging-dataframes","title":"Merging DataFrames\u00b6","text":"<p>DataFrames can be joined with {meth}<code>df.join() &lt;daft.DataFrame.join&gt;</code>.</p> <p>Let's use a join to reunite our <code>owners</code> with their sweet fluffy companions. We'll create a <code>dogs</code> DataFrame from a Python dictionary and then join this to our existing dataframe with the owners data.</p>"},{"location":"10min/#working-with-multimodal-data","title":"Working with Multimodal Data\u00b6","text":""},{"location":"10min/#user-defined-functions","title":"User-Defined Functions\u00b6","text":"<p>See: UDF User Guide</p>"},{"location":"10min/#ml-workloads","title":"ML Workloads\u00b6","text":"<p>We'll define a function that uses a pre-trained PyTorch model: ResNet50 to classify the dog pictures. We'll pass the contents of the image <code>urls</code> column and send the classification predictions to a new column <code>classify_breed</code>.</p>"},{"location":"10min/#writing-data","title":"Writing Data\u00b6","text":"<p>See: Writing Data</p> <p>Writing data will execute your DataFrame and write the results out to the specified backend. For example, to write data out to Parquet with {meth}<code>df.write_parquet() &lt;daft.DataFrame.write_parquet&gt;</code>:</p>"},{"location":"10min/#whats-next","title":"What's Next?\u00b6","text":"<p>Now that you have a basic sense of Daft's functionality and features, take a look at some of the other resources to help you get the most out of Daft:</p> <ul> <li>The Daft User Guide for more information on specific topics</li> <li>Hands-on Tutorials in Google Colab on:<ul> <li>Image Classification</li> <li>NLP Similarity Search / Vector Embedding</li> <li>Querying Images</li> <li>Image Generation with GPUs</li> </ul> </li> </ul>"},{"location":"10min/#contributing","title":"Contributing\u00b6","text":"<p>Excited about Daft and want to contribute? Join us on Github \ud83d\ude80</p>"},{"location":"core_concepts/","title":"Core Concepts","text":"<p>todo(docs): Created a mega Core Concepts page to improve user journey. Right now <code>toc_depth=3</code> for visibility into subheadings, but are there now too many subheadings? Can we combine/condense some sections? My concern is that <code>toc_depth=2</code> is not informative unless we turn off <code>toc.integrate</code> and have a right-hand menu for all the subheadings.</p> <p>Learn about the core concepts that Daft is built on!</p>"},{"location":"core_concepts/#dataframe","title":"DataFrame","text":"<p>todo(docs): Check that this page makes sense. Can we have a 1-1 mapping of \"Common data operations that you would perform on DataFrames are: ...\" to its respective section?</p> <p>todo(docs): I reused some of these sections in the Quickstart (create df, execute df and view data, select rows, select columns) but the examples in the quickstart are different. Should we still keep those sections on this page?</p> <p>If you are coming from other DataFrame libraries such as Pandas or Polars, here are some key differences about Daft DataFrames:</p> <ol> <li> <p>Distributed: When running in a distributed cluster, Daft splits your data into smaller \"chunks\" called Partitions. This allows Daft to process your data in parallel across multiple machines, leveraging more resources to work with large datasets.</p> </li> <li> <p>Lazy: When you write operations on a DataFrame, Daft doesn't execute them immediately. Instead, it creates a plan (called a query plan) of what needs to be done. This plan is optimized and only executed when you specifically request the results, which can lead to more efficient computations.</p> </li> <li> <p>Multimodal: Unlike traditional tables that usually contain simple data types like numbers and text, Daft DataFrames can handle complex data types in its columns. This includes things like images, audio files, or even custom Python objects.</p> </li> </ol> <p>For a full comparison between Daft and other DataFrame Libraries, see DataFrame Comparison.</p> <p>Common data operations that you would perform on DataFrames are:</p> <ol> <li>Filtering rows: Use <code>df.where(...)</code> to keep only the rows that meet certain conditions.</li> <li>Creating new columns: Use <code>df.with_column(...)</code> to add a new column based on calculations from existing ones.</li> <li>Joining DataFrames: Use <code>df.join(other_df, ...)</code> to combine two DataFrames based on common columns.</li> <li>Sorting: Use <code>df.sort(...)</code> to arrange your data based on values in one or more columns.</li> <li>Grouping and aggregating: Use <code>df.groupby(...).agg(...)</code> to summarize your data by groups.</li> </ol>"},{"location":"core_concepts/#creating-a-dataframe","title":"Creating a Dataframe","text":"<p>See Also</p> <p>Reading Data and Writing Data - a more in-depth guide on various options for reading and writing data to and from Daft DataFrames from in-memory data (Python, Arrow), files (Parquet, CSV, JSON), SQL Databases and Data Catalogs</p> <p>Let's create our first Dataframe from a Python dictionary of columns.</p> \ud83d\udc0d Python <pre><code>import daft\n\ndf = daft.from_pydict({\n    \"A\": [1, 2, 3, 4],\n    \"B\": [1.5, 2.5, 3.5, 4.5],\n    \"C\": [True, True, False, False],\n    \"D\": [None, None, None, None],\n})\n</code></pre> <p>Examine your Dataframe by printing it:</p> <pre><code>df\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B       \u2506 C       \u2506 D    \u2502\n\u2502 ---   \u2506 ---     \u2506 ---     \u2506 ---  \u2502\n\u2502 Int64 \u2506 Float64 \u2506 Boolean \u2506 Null \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1.5     \u2506 true    \u2506 None \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2     \u2506 2.5     \u2506 true    \u2506 None \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 3.5     \u2506 false   \u2506 None \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4     \u2506 4.5     \u2506 false   \u2506 None \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 4 of 4 rows)\n</code></pre> <p>Congratulations - you just created your first DataFrame! It has 4 columns, \"A\", \"B\", \"C\", and \"D\". Let's try to select only the \"A\", \"B\", and \"C\" columns:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = df.select(\"A\", \"B\", \"C\")\ndf\n</code></pre> <pre><code>df = daft.sql(\"SELECT A, B, C FROM df\")\ndf\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B       \u2506 C       \u2502\n\u2502 ---   \u2506 ---     \u2506 ---     \u2502\n\u2502 Int64 \u2506 Float64 \u2506 Boolean \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(No data to display: Dataframe not materialized)\n</code></pre> <p>But wait - why is it printing the message <code>(No data to display: Dataframe not materialized)</code> and where are the rows of each column?</p>"},{"location":"core_concepts/#executing-dataframe-and-viewing-data","title":"Executing DataFrame and Viewing Data","text":"<p>The reason that our DataFrame currently does not display its rows is that Daft DataFrames are lazy. This just means that Daft DataFrames will defer all its work until you tell it to execute.</p> <p>In this case, Daft is just deferring the work required to read the data and select columns, however in practice this laziness can be very useful for helping Daft optimize your queries before execution!</p> <p>Info</p> <p>When you call methods on a Daft Dataframe, it defers the work by adding to an internal \"plan\". You can examine the current plan of a DataFrame by calling <code>df.explain()</code>!</p> <p>Passing the <code>show_all=True</code> argument will show you the plan after Daft applies its query optimizations and the physical (lower-level) plan.</p> <pre><code>Plan Output\n\n== Unoptimized Logical Plan ==\n\n* Project: col(A), col(B), col(C)\n|\n* Source:\n|   Number of partitions = 1\n|   Output schema = A#Int64, B#Float64, C#Boolean, D#Null\n\n\n== Optimized Logical Plan ==\n\n* Project: col(A), col(B), col(C)\n|\n* Source:\n|   Number of partitions = 1\n|   Output schema = A#Int64, B#Float64, C#Boolean, D#Null\n\n\n== Physical Plan ==\n\n* Project: col(A), col(B), col(C)\n|   Clustering spec = { Num partitions = 1 }\n|\n* InMemoryScan:\n|   Schema = A#Int64, B#Float64, C#Boolean, D#Null,\n|   Size bytes = 65,\n|   Clustering spec = { Num partitions = 1 }\n</code></pre> <p>We can tell Daft to execute our DataFrame and store the results in-memory using <code>df.collect()</code>:</p> \ud83d\udc0d Python <pre><code>df.collect()\ndf\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B       \u2506 C       \u2506 D    \u2502\n\u2502 ---   \u2506 ---     \u2506 ---     \u2506 ---  \u2502\n\u2502 Int64 \u2506 Float64 \u2506 Boolean \u2506 Null \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1.5     \u2506 true    \u2506 None \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2     \u2506 2.5     \u2506 true    \u2506 None \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 3.5     \u2506 false   \u2506 None \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4     \u2506 4.5     \u2506 false   \u2506 None \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 4 of 4 rows)\n</code></pre> <p>Now your DataFrame object <code>df</code> is materialized - Daft has executed all the steps required to compute the results, and has cached the results in memory so that it can display this preview.</p> <p>Any subsequent operations on <code>df</code> will avoid recomputations, and just use this materialized result!</p>"},{"location":"core_concepts/#when-should-i-materialize-my-dataframe","title":"When should I materialize my DataFrame?","text":"<p>If you \"eagerly\" call <code>df.collect()</code> immediately on every DataFrame, you may run into issues:</p> <ol> <li>If data is too large at any step, materializing all of it may cause memory issues</li> <li>Optimizations are not possible since we cannot \"predict future operations\"</li> </ol> <p>However, data science is all about experimentation and trying different things on the same data. This means that materialization is crucial when working interactively with DataFrames, since it speeds up all subsequent experimentation on that DataFrame.</p> <p>We suggest materializing DataFrames using <code>df.collect()</code> when they contain expensive operations (e.g. sorts or expensive function calls) and have to be called multiple times by downstream code:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = df.sort(\"A\")  # expensive sort\ndf.collect()  # materialize the DataFrame\n\n# All subsequent work on df avoids recomputing previous steps\ndf.sum(\"B\").show()\ndf.mean(\"B\").show()\ndf.with_column(\"try_this\", df[\"A\"] + 1).show(5)\n</code></pre> <pre><code>df = daft.sql(\"SELECT * FROM df ORDER BY A\")\ndf.collect()\n\n# All subsequent work on df avoids recomputing previous steps\ndaft.sql(\"SELECT sum(B) FROM df\").show()\ndaft.sql(\"SELECT mean(B) FROM df\").show()\ndaft.sql(\"SELECT *, (A + 1) AS try_this FROM df\").show(5)\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 B       \u2502\n\u2502 ---     \u2502\n\u2502 Float64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 12      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 1 of 1 rows)\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 B       \u2502\n\u2502 ---     \u2502\n\u2502 Float64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 1 of 1 rows)\n\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B       \u2506 C       \u2506 try_this \u2502\n\u2502 ---   \u2506 ---     \u2506 ---     \u2506 ---      \u2502\n\u2502 Int64 \u2506 Float64 \u2506 Boolean \u2506 Int64    \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1.5     \u2506 true    \u2506 2        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2     \u2506 2.5     \u2506 true    \u2506 3        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 3.5     \u2506 false   \u2506 4        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4     \u2506 4.5     \u2506 false   \u2506 5        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 4 of 4 rows)\n</code></pre> <p>In many other cases however, there are better options than materializing your entire DataFrame with <code>df.collect()</code>:</p> <ol> <li>Peeking with df.show(N): If you only want to \"peek\" at the first few rows of your data for visualization purposes, you can use <code>df.show(N)</code>, which processes and shows only the first <code>N</code> rows.</li> <li>Writing to disk: The <code>df.write_*</code> methods will process and write your data to disk per-partition, avoiding materializing it all in memory at once.</li> <li>Pruning data: You can materialize your DataFrame after performing a <code>df.limit()</code>, <code>df.where()</code> or <code>df.select()</code> operation which processes your data or prune it down to a smaller size.</li> </ol>"},{"location":"core_concepts/#schemas-and-types","title":"Schemas and Types","text":"<p>Notice also that when we printed our DataFrame, Daft displayed its schema. Each column of your DataFrame has a name and a type, and all data in that column will adhere to that type!</p> <p>Daft can display your DataFrame's schema without materializing it. Under the hood, it performs intelligent sampling of your data to determine the appropriate schema, and if you make any modifications to your DataFrame it can infer the resulting types based on the operation.</p> <p>Note</p> <p>Under the hood, Daft represents data in the Apache Arrow format, which allows it to efficiently represent and work on data using high-performance kernels which are written in Rust.</p>"},{"location":"core_concepts/#running-computation-with-expressions","title":"Running Computation with Expressions","text":"<p>To run computations on data in our DataFrame, we use Expressions.</p> <p>The following statement will <code>df.show()</code> a DataFrame that has only one column - the column <code>A</code> from our original DataFrame but with every row incremented by 1.</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df.select(df[\"A\"] + 1).show()\n</code></pre> <pre><code>daft.sql(\"SELECT A + 1 FROM df\").show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2502\n\u2502 ---   \u2502\n\u2502 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 5     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 4 of 4 rows)\n</code></pre> <p>Info</p> <p>A common pattern is to create a new columns using <code>DataFrame.with_column</code>:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code># Creates a new column named \"foo\" which takes on values\n# of column \"A\" incremented by 1\ndf = df.with_column(\"foo\", df[\"A\"] + 1)\ndf.show()\n</code></pre> <pre><code># Creates a new column named \"foo\" which takes on values\n# of column \"A\" incremented by 1\ndf = daft.sql(\"SELECT *, A + 1 AS foo FROM df\")\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B       \u2506 C       \u2506 foo   \u2502\n\u2502 ---   \u2506 ---     \u2506 ---     \u2506 ---   \u2502\n\u2502 Int64 \u2506 Float64 \u2506 Boolean \u2506 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1.5     \u2506 true    \u2506 2     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2     \u2506 2.5     \u2506 true    \u2506 3     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 3.5     \u2506 false   \u2506 4     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4     \u2506 4.5     \u2506 false   \u2506 5     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 4 of 4 rows)\n</code></pre> <p>Congratulations, you have just written your first Expression: <code>df[\"A\"] + 1</code>! Expressions are a powerful way of describing computation on columns. For more details, check out the next section on Expressions.</p>"},{"location":"core_concepts/#selecting-rows","title":"Selecting Rows","text":"<p>We can limit the rows to the first <code>N</code> rows using <code>df.limit(N)</code>:</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\n    \"A\": [1, 2, 3, 4, 5],\n    \"B\": [6, 7, 8, 9, 10],\n})\n\ndf.limit(3).show()\n</code></pre> Output<pre><code>+---------+---------+\n|       A |       B |\n|   Int64 |   Int64 |\n+=========+=========+\n|       1 |       6 |\n+---------+---------+\n|       2 |       7 |\n+---------+---------+\n|       3 |       8 |\n+---------+---------+\n(Showing first 3 rows)\n</code></pre> <p>We can also filter rows using <code>df.where()</code>, which takes an input a Logical Expression predicate:</p> \ud83d\udc0d Python <pre><code>df.where(df[\"A\"] &gt; 3).show()\n</code></pre> Output<pre><code>+---------+---------+\n|       A |       B |\n|   Int64 |   Int64 |\n+=========+=========+\n|       4 |       9 |\n+---------+---------+\n|       5 |      10 |\n+---------+---------+\n(Showing first 2 rows)\n</code></pre>"},{"location":"core_concepts/#selecting-columns","title":"Selecting Columns","text":"<p>Select specific columns in a DataFrame using <code>df.select()</code>, which also takes Expressions as an input.</p> \ud83d\udc0d Python <pre><code>import daft\n\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\n\ndf.select(\"A\").show()\n</code></pre> Output<pre><code>+---------+\n|       A |\n|   Int64 |\n+=========+\n|       1 |\n+---------+\n|       2 |\n+---------+\n|       3 |\n+---------+\n(Showing first 3 rows)\n</code></pre> <p>A useful alias for <code>df.select()</code> is indexing a DataFrame with a list of column names or Expressions:</p> \ud83d\udc0d Python <pre><code>df[[\"A\", \"B\"]].show()\n</code></pre> Output<pre><code>+---------+---------+\n|       A |       B |\n|   Int64 |   Int64 |\n+=========+=========+\n|       1 |       4 |\n+---------+---------+\n|       2 |       5 |\n+---------+---------+\n|       3 |       6 |\n+---------+---------+\n(Showing first 3 rows)\n</code></pre> <p>Sometimes, it may be useful to exclude certain columns from a DataFrame. This can be done with <code>df.exclude()</code>:</p> \ud83d\udc0d Python <pre><code>df.exclude(\"A\").show()\n</code></pre> Output<pre><code>+---------+\n|       B |\n|   Int64 |\n+=========+\n|       4 |\n+---------+\n|       5 |\n+---------+\n|       6 |\n+---------+\n(Showing first 3 rows)\n</code></pre> <p>Adding a new column can be achieved with <code>df.with_column()</code>:</p> \ud83d\udc0d Python <pre><code>df.with_column(\"C\", df[\"A\"] + df[\"B\"]).show()\n</code></pre> Output<pre><code>+---------+---------+---------+\n|       A |       B |       C |\n|   Int64 |   Int64 |   Int64 |\n+=========+=========+=========+\n|       1 |       4 |       5 |\n+---------+---------+---------+\n|       2 |       5 |       7 |\n+---------+---------+---------+\n|       3 |       6 |       9 |\n+---------+---------+---------+\n(Showing first 3 rows)\n</code></pre>"},{"location":"core_concepts/#selecting-columns-using-wildcards","title":"Selecting Columns Using Wildcards","text":"<p>We can select multiple columns at once using wildcards. The expression <code>.col(*)</code> selects every column in a DataFrame, and you can operate on this expression in the same way as a single column:</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndf.select(col(\"*\") * 3).show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B     \u2502\n\u2502 ---   \u2506 ---   \u2502\n\u2502 Int64 \u2506 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3     \u2506 12    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 6     \u2506 15    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 9     \u2506 18    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>We can also select multiple columns within structs using <code>col(\"struct.*\")</code>:</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\n    \"A\": [\n        {\"B\": 1, \"C\": 2},\n        {\"B\": 3, \"C\": 4}\n    ]\n})\ndf.select(col(\"A.*\")).show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 B     \u2506 C     \u2502\n\u2502 ---   \u2506 ---   \u2502\n\u2502 Int64 \u2506 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 2     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 4     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Under the hood, wildcards work by finding all of the columns that match, then copying the expression several times and replacing the wildcard. This means that there are some caveats:</p> <ul> <li>Only one wildcard is allowed per expression tree. This means that <code>col(\"*\") + col(\"*\")</code> and similar expressions do not work.</li> <li>Be conscious about duplicated column names. Any code like <code>df.select(col(\"*\"), col(\"*\") + 3)</code> will not work because the wildcards expand into the same column names.</li> </ul> <p>For the same reason, <code>col(\"A\") + col(\"*\")</code> will not work because the name on the left-hand side is inherited, meaning all the output columns are named <code>A</code>, causing an error if there is more than one.   However, <code>col(\"*\") + col(\"A\")</code> will work fine.</p>"},{"location":"core_concepts/#combining-dataframes","title":"Combining DataFrames","text":"<p>Two DataFrames can be column-wise joined using <code>df.join()</code>.</p> <p>This requires a \"join key\", which can be supplied as the <code>on</code> argument if both DataFrames have the same name for their key columns, or the <code>left_on</code> and <code>right_on</code> argument if the key column has different names in each DataFrame.</p> <p>Daft also supports multi-column joins if you have a join key comprising of multiple columns!</p> \ud83d\udc0d Python <pre><code>df1 = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndf2 = daft.from_pydict({\"A\": [1, 2, 3], \"C\": [7, 8, 9]})\n\ndf1.join(df2, on=\"A\").show()\n</code></pre> Output<pre><code>+---------+---------+---------+\n|       A |       B |       C |\n|   Int64 |   Int64 |   Int64 |\n+=========+=========+=========+\n|       1 |       4 |       7 |\n+---------+---------+---------+\n|       2 |       5 |       8 |\n+---------+---------+---------+\n|       3 |       6 |       9 |\n+---------+---------+---------+\n(Showing first 3 rows)\n</code></pre>"},{"location":"core_concepts/#reordering-rows","title":"Reordering Rows","text":"<p>Rows in a DataFrame can be reordered based on some column using <code>df.sort()</code>. Daft also supports multi-column sorts for sorting on multiple columns at once.</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\n    \"A\": [1, 2, 3],\n    \"B\": [6, 7, 8],\n})\n\ndf.sort(\"A\", desc=True).show()\n</code></pre> Output<pre><code>+---------+---------+\n|       A |       B |\n|   Int64 |   Int64 |\n+=========+=========+\n|       3 |       8 |\n+---------+---------+\n|       2 |       7 |\n+---------+---------+\n|       1 |       6 |\n+---------+---------+\n(Showing first 3 rows)\n</code></pre>"},{"location":"core_concepts/#exploding-columns","title":"Exploding Columns","text":"<p>The <code>df.explode()</code> method can be used to explode a column containing a list of values into multiple rows. All other rows will be duplicated.</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\n    \"A\": [1, 2, 3],\n    \"B\": [[1, 2, 3], [4, 5, 6], [7, 8, 9]],\n})\n\ndf.explode(\"B\").show()\n</code></pre> Output<pre><code>+---------+---------+\n|       A |       B |\n|   Int64 |   Int64 |\n+=========+=========+\n|       1 |       1 |\n+---------+---------+\n|       1 |       2 |\n+---------+---------+\n|       1 |       3 |\n+---------+---------+\n|       2 |       4 |\n+---------+---------+\n|       2 |       5 |\n+---------+---------+\n|       2 |       6 |\n+---------+---------+\n|       3 |       7 |\n+---------+---------+\n|       3 |       8 |\n+---------+---------+\n(Showing first 8 rows)\n</code></pre>"},{"location":"core_concepts/#expressions","title":"Expressions","text":"<p>Expressions are how you can express computations that should be run over columns of data.</p>"},{"location":"core_concepts/#creating-expressions","title":"Creating Expressions","text":""},{"location":"core_concepts/#referring-to-a-column-in-a-dataframe","title":"Referring to a column in a DataFrame","text":"<p>Most commonly you will be creating expressions by using the <code>daft.col</code> function.</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code># Refers to column \"A\"\ndaft.col(\"A\")\n</code></pre> <pre><code>daft.sql_expr(\"A\")\n</code></pre> Output<pre><code>col(A)\n</code></pre> <p>The above code creates an Expression that refers to a column named <code>\"A\"</code>.</p>"},{"location":"core_concepts/#using-sql","title":"Using SQL","text":"<p>Daft can also parse valid SQL as expressions.</p> \u2699\ufe0f SQL <pre><code>daft.sql_expr(\"A + 1\")\n</code></pre> Output<pre><code>col(A) + lit(1)\n</code></pre> <p>The above code will create an expression representing \"the column named 'x' incremented by 1\". For many APIs, <code>sql_expr</code> will actually be applied for you as syntactic sugar!</p>"},{"location":"core_concepts/#literals","title":"Literals","text":"<p>You may find yourself needing to hardcode a \"single value\" oftentimes as an expression. Daft provides a <code>lit()</code> helper to do so:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>from daft import lit\n\n# Refers to an expression which always evaluates to 42\nlit(42)\n</code></pre> <pre><code># Refers to an expression which always evaluates to 42\ndaft.sql_expr(\"42\")\n</code></pre> <p>Output<pre><code>lit(42)\n</code></pre> This special :func:<code>~daft.expressions.lit</code> expression we just created evaluates always to the value <code>42</code>.</p>"},{"location":"core_concepts/#wildcard-expressions","title":"Wildcard Expressions","text":"<p>You can create expressions on multiple columns at once using a wildcard. The expression <code>col(\"*\")</code>) selects every column in a DataFrame, and you can operate on this expression in the same way as a single column:</p> \ud83d\udc0d Python <pre><code>import daft\nfrom daft import col\n\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\ndf.select(col(\"*\") * 3).show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B     \u2502\n\u2502 ---   \u2506 ---   \u2502\n\u2502 Int64 \u2506 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 3     \u2506 12    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 6     \u2506 15    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 9     \u2506 18    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>Wildcards also work very well for accessing all members of a struct column:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>import daft\nfrom daft import col\n\ndf = daft.from_pydict({\n    \"person\": [\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25},\n        {\"name\": \"Charlie\", \"age\": 35}\n    ]\n})\n\n# Access all fields of the 'person' struct\ndf.select(col(\"person.*\")).show()\n</code></pre> <pre><code>import daft\n\ndf = daft.from_pydict({\n    \"person\": [\n        {\"name\": \"Alice\", \"age\": 30},\n        {\"name\": \"Bob\", \"age\": 25},\n        {\"name\": \"Charlie\", \"age\": 35}\n    ]\n})\n\n# Access all fields of the 'person' struct using SQL\ndaft.sql(\"SELECT person.* FROM df\").show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 name     \u2506 age   \u2502\n\u2502 ---      \u2506 ---   \u2502\n\u2502 String   \u2506 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 Alice    \u2506 30    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Bob      \u2506 25    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 Charlie  \u2506 35    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>In this example, we use the wildcard <code>*</code> to access all fields of the <code>person</code> struct column. This is equivalent to selecting each field individually (<code>person.name</code>, <code>person.age</code>), but is more concise and flexible, especially when dealing with structs that have many fields.</p>"},{"location":"core_concepts/#composing-expressions","title":"Composing Expressions","text":""},{"location":"core_concepts/#numeric-expressions","title":"Numeric Expressions","text":"<p>Since column \"A\" is an integer, we can run numeric computation such as addition, division and checking its value. Here are some examples where we create new columns using the results of such computations:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code># Add 1 to each element in column \"A\"\ndf = df.with_column(\"A_add_one\", df[\"A\"] + 1)\n\n# Divide each element in column A by 2\ndf = df.with_column(\"A_divide_two\", df[\"A\"] / 2.)\n\n# Check if each element in column A is more than 1\ndf = df.with_column(\"A_gt_1\", df[\"A\"] &gt; 1)\n\ndf.collect()\n</code></pre> <pre><code>df = daft.sql(\"\"\"\n    SELECT\n        *,\n        A + 1 AS A_add_one,\n        A / 2.0 AS A_divide_two,\n        A &gt; 1 AS A_gt_1\n    FROM df\n\"\"\")\ndf.collect()\n</code></pre> Output<pre><code>+---------+-------------+----------------+-----------+\n|       A |   A_add_one |   A_divide_two | A_gt_1    |\n|   Int64 |       Int64 |        Float64 | Boolean   |\n+=========+=============+================+===========+\n|       1 |           2 |            0.5 | false     |\n+---------+-------------+----------------+-----------+\n|       2 |           3 |            1   | true      |\n+---------+-------------+----------------+-----------+\n|       3 |           4 |            1.5 | true      |\n+---------+-------------+----------------+-----------+\n(Showing first 3 of 3 rows)\n</code></pre> <p>Notice that the returned types of these operations are also well-typed according to their input types. For example, calling <code>df[\"A\"] &gt; 1</code> returns a column of type <code>Boolean</code>.</p> <p>Both the <code>Float</code> and <code>Int</code> types are numeric types, and inherit many of the same arithmetic Expression operations. You may find the full list of numeric operations in the Expressions API Reference.</p>"},{"location":"core_concepts/#string-expressions","title":"String Expressions","text":"<p>Daft also lets you have columns of strings in a DataFrame. Let's take a look!</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\"B\": [\"foo\", \"bar\", \"baz\"]})\ndf.show()\n</code></pre> Output<pre><code>+--------+\n| B      |\n| Utf8   |\n+========+\n| foo    |\n+--------+\n| bar    |\n+--------+\n| baz    |\n+--------+\n(Showing first 3 rows)\n</code></pre> <p>Unlike the numeric types, the string type does not support arithmetic operations such as <code>*</code> and <code>/</code>. The one exception to this is the <code>+</code> operator, which is overridden to concatenate two string expressions as is commonly done in Python. Let's try that!</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = df.with_column(\"B2\", df[\"B\"] + \"foo\")\ndf.show()\n</code></pre> <pre><code>df = daft.sql(\"SELECT *, B + 'foo' AS B2 FROM df\")\ndf.show()\n</code></pre> Output<pre><code>+--------+--------+\n| B      | B2     |\n| Utf8   | Utf8   |\n+========+========+\n| foo    | foofoo |\n+--------+--------+\n| bar    | barfoo |\n+--------+--------+\n| baz    | bazfoo |\n+--------+--------+\n(Showing first 3 rows)\n</code></pre> <p>There are also many string operators that are accessed through a separate <code>.str.*</code> \"method namespace\".</p> <p>For example, to check if each element in column \"B\" contains the substring \"a\", we can use the <code>.str.contains</code> method:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = df.with_column(\"B2_contains_B\", df[\"B2\"].str.contains(df[\"B\"]))\ndf.show()\n</code></pre> <pre><code>df = daft.sql(\"SELECT *, contains(B2, B) AS B2_contains_B FROM df\")\ndf.show()\n</code></pre> Output<pre><code>+--------+--------+-----------------+\n| B      | B2     | B2_contains_B   |\n| Utf8   | Utf8   | Boolean         |\n+========+========+=================+\n| foo    | foofoo | true            |\n+--------+--------+-----------------+\n| bar    | barfoo | true            |\n+--------+--------+-----------------+\n| baz    | bazfoo | true            |\n+--------+--------+-----------------+\n(Showing first 3 rows)\n</code></pre> <p>You may find a full list of string operations in the Expressions API Reference.</p>"},{"location":"core_concepts/#url-expressions","title":"URL Expressions","text":"<p>One special case of a String column you may find yourself working with is a column of URL strings.</p> <p>Daft provides the <code>.url.*</code> method namespace with functionality for working with URL strings. For example, to download data from URLs:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = daft.from_pydict({\n    \"urls\": [\n        \"https://www.google.com\",\n        \"s3://daft-public-data/open-images/validation-images/0001eeaf4aed83f9.jpg\",\n    ],\n})\ndf = df.with_column(\"data\", df[\"urls\"].url.download())\ndf.collect()\n</code></pre> <pre><code>df = daft.from_pydict({\n    \"urls\": [\n        \"https://www.google.com\",\n        \"s3://daft-public-data/open-images/validation-images/0001eeaf4aed83f9.jpg\",\n    ],\n})\ndf = daft.sql(\"\"\"\n    SELECT\n        urls,\n        url_download(urls) AS data\n    FROM df\n\"\"\")\ndf.collect()\n</code></pre> Output<pre><code>+----------------------+----------------------+\n| urls                 | data                 |\n| Utf8                 | Binary               |\n+======================+======================+\n| https://www.google.c | b'&lt;!doctype          |\n| om                   | html&gt;&lt;html           |\n|                      | itemscope=\"\" itemtyp |\n|                      | e=\"http://sche...    |\n+----------------------+----------------------+\n| s3://daft-public-    | b'\\xff\\xd8\\xff\\xe0\\x |\n| data/open-           | 00\\x10JFIF\\x00\\x01\\x |\n| images/validation-   | 01\\x01\\x00H\\x00H\\... |\n| images/0001e...      |                      |\n+----------------------+----------------------+\n(Showing first 2 of 2 rows)\n</code></pre> <p>This works well for URLs which are HTTP paths to non-HTML files (e.g. jpeg), local filepaths or even paths to a file in an object store such as AWS S3 as well!</p>"},{"location":"core_concepts/#json-expressions","title":"JSON Expressions","text":"<p>If you have a column of JSON strings, Daft provides the <code>.json.*</code> method namespace to run JQ-style filters on them. For example, to extract a value from a JSON object:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = daft.from_pydict({\n    \"json\": [\n        '{\"a\": 1, \"b\": 2}',\n        '{\"a\": 3, \"b\": 4}',\n    ],\n})\ndf = df.with_column(\"a\", df[\"json\"].json.query(\".a\"))\ndf.collect()\n</code></pre> <pre><code>df = daft.from_pydict({\n    \"json\": [\n        '{\"a\": 1, \"b\": 2}',\n        '{\"a\": 3, \"b\": 4}',\n    ],\n})\ndf = daft.sql(\"\"\"\n    SELECT\n        json,\n        json_query(json, '.a') AS a\n    FROM df\n\"\"\")\ndf.collect()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 json             \u2506 a    \u2502\n\u2502 ---              \u2506 ---  \u2502\n\u2502 Utf8             \u2506 Utf8 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 {\"a\": 1, \"b\": 2} \u2506 1    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 {\"a\": 3, \"b\": 4} \u2506 3    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 2 of 2 rows)\n</code></pre> <p>Daft uses jaq as the underlying executor, so you can find the full list of supported filters in the jaq documentation.</p>"},{"location":"core_concepts/#logical-expressions","title":"Logical Expressions","text":"<p>Logical Expressions are an expression that refers to a column of type <code>Boolean</code>, and can only take on the values True or False.</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\"C\": [True, False, True]})\n</code></pre> <p>Daft supports logical operations such as <code>&amp;</code> (and) and <code>|</code> (or) between logical expressions.</p>"},{"location":"core_concepts/#comparisons","title":"Comparisons","text":"<p>Many of the types in Daft support comparisons between expressions that returns a Logical Expression.</p> <p>For example, here we can compare if each element in column \"A\" is equal to elements in column \"B\":</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 4]})\n\ndf = df.with_column(\"A_eq_B\", df[\"A\"] == df[\"B\"])\n\ndf.collect()\n</code></pre> <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 4]})\n\ndf = daft.sql(\"\"\"\n    SELECT\n        A,\n        B,\n        A = B AS A_eq_B\n    FROM df\n\"\"\")\n\ndf.collect()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B     \u2506 A_eq_B  \u2502\n\u2502 ---   \u2506 ---   \u2506 ---     \u2502\n\u2502 Int64 \u2506 Int64 \u2506 Boolean \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1     \u2506 true    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2     \u2506 2     \u2506 true    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 4     \u2506 false   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 3 of 3 rows)\n</code></pre> <p>Other useful comparisons can be found in the Expressions API Reference.</p>"},{"location":"core_concepts/#if-else-pattern","title":"If Else Pattern","text":"<p>The <code>.if_else()</code> method is a useful expression to have up your sleeve for choosing values between two other expressions based on a logical expression:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [0, 2, 4]})\n\n# Pick values from column A if the value in column A is bigger\n# than the value in column B. Otherwise, pick values from column B.\ndf = df.with_column(\n    \"A_if_bigger_else_B\",\n    (df[\"A\"] &gt; df[\"B\"]).if_else(df[\"A\"], df[\"B\"]),\n)\n\ndf.collect()\n</code></pre> <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [0, 2, 4]})\n\ndf = daft.sql(\"\"\"\n    SELECT\n        A,\n        B,\n        CASE\n            WHEN A &gt; B THEN A\n            ELSE B\n        END AS A_if_bigger_else_B\n    FROM df\n\"\"\")\n\ndf.collect()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B     \u2506 A_if_bigger_else_B \u2502\n\u2502 ---   \u2506 ---   \u2506 ---                \u2502\n\u2502 Int64 \u2506 Int64 \u2506 Int64              \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 0     \u2506 1                  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2     \u2506 2     \u2506 2                  \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 4     \u2506 4                  \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 3 of 3 rows)\n</code></pre> <p>This is a useful expression for cleaning your data!</p>"},{"location":"core_concepts/#temporal-expressions","title":"Temporal Expressions","text":"<p>Daft provides rich support for working with temporal data types like Timestamp and Duration. Let's explore some common temporal operations:</p>"},{"location":"core_concepts/#basic-temporal-operations","title":"Basic Temporal Operations","text":"<p>You can perform arithmetic operations with timestamps and durations, such as adding a duration to a timestamp or calculating the duration between two timestamps:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>import datetime\n\ndf = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Add 10 seconds to each timestamp\ndf = df.with_column(\n    \"plus_10_seconds\",\n    df[\"timestamp\"] + datetime.timedelta(seconds=10)\n)\n\ndf.show()\n</code></pre> <pre><code>import datetime\n\ndf = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Add 10 seconds to each timestamp and calculate duration between timestamps\ndf = daft.sql(\"\"\"\n    SELECT\n        timestamp,\n        timestamp + INTERVAL '10 seconds' as plus_10_seconds,\n    FROM df\n\"\"\")\n\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 timestamp                     \u2506 plus_10_seconds               \u2502\n\u2502 ---                           \u2506 ---                           \u2502\n\u2502 Timestamp(Microseconds, None) \u2506 Timestamp(Microseconds, None) \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-01-01 00:01:01           \u2506 2021-01-01 00:01:11           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-01 00:01:59           \u2506 2021-01-01 00:02:09           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-01 00:02:00           \u2506 2021-01-01 00:02:10           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"core_concepts/#temporal-component-extraction","title":"Temporal Component Extraction","text":"<p>The <code>.dt.*</code> method namespace provides extraction methods for the components of a timestamp, such as year, month, day, hour, minute, and second:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Extract year, month, day, hour, minute, and second from the timestamp\ndf = df.with_columns({\n    \"year\": df[\"timestamp\"].dt.year(),\n    \"month\": df[\"timestamp\"].dt.month(),\n    \"day\": df[\"timestamp\"].dt.day(),\n    \"hour\": df[\"timestamp\"].dt.hour(),\n    \"minute\": df[\"timestamp\"].dt.minute(),\n    \"second\": df[\"timestamp\"].dt.second()\n})\n\ndf.show()\n</code></pre> <pre><code>df = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 1, 0, 1, 1),\n        datetime.datetime(2021, 1, 1, 0, 1, 59),\n        datetime.datetime(2021, 1, 1, 0, 2, 0),\n    ]\n})\n\n# Extract year, month, day, hour, minute, and second from the timestamp\ndf = daft.sql(\"\"\"\n    SELECT\n        timestamp,\n        year(timestamp) as year,\n        month(timestamp) as month,\n        day(timestamp) as day,\n        hour(timestamp) as hour,\n        minute(timestamp) as minute,\n        second(timestamp) as second\n    FROM df\n\"\"\")\n\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 timestamp                     \u2506 year  \u2506 month  \u2506 day    \u2506 hour   \u2506 minute \u2506 second \u2502\n\u2502 ---                           \u2506 ---   \u2506 ---    \u2506 ---    \u2506 ---    \u2506 ---    \u2506 ---    \u2502\n\u2502 Timestamp(Microseconds, None) \u2506 Int32 \u2506 UInt32 \u2506 UInt32 \u2506 UInt32 \u2506 UInt32 \u2506 UInt32 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-01-01 00:01:01           \u2506 2021  \u2506 1      \u2506 1      \u2506 0      \u2506 1      \u2506 1      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-01 00:01:59           \u2506 2021  \u2506 1      \u2506 1      \u2506 0      \u2506 1      \u2506 59     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-01 00:02:00           \u2506 2021  \u2506 1      \u2506 1      \u2506 0      \u2506 2      \u2506 0      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"core_concepts/#time-zone-operations","title":"Time Zone Operations","text":"<p>You can parse strings as timestamps with time zones and convert between different time zones:</p> \ud83d\udc0d Python\u2699\ufe0f SQL <pre><code>df = daft.from_pydict({\n    \"timestamp_str\": [\n        \"2021-01-01 00:00:00.123 +0800\",\n        \"2021-01-02 12:30:00.456 +0800\"\n    ]\n})\n\n# Parse the timestamp string with time zone and convert to New York time\ndf = df.with_column(\n    \"ny_time\",\n    df[\"timestamp_str\"].str.to_datetime(\n        \"%Y-%m-%d %H:%M:%S%.3f %z\",\n        timezone=\"America/New_York\"\n    )\n)\n\ndf.show()\n</code></pre> <pre><code>df = daft.from_pydict({\n    \"timestamp_str\": [\n        \"2021-01-01 00:00:00.123 +0800\",\n        \"2021-01-02 12:30:00.456 +0800\"\n    ]\n})\n\n# Parse the timestamp string with time zone and convert to New York time\ndf = daft.sql(\"\"\"\n    SELECT\n        timestamp_str,\n        to_datetime(timestamp_str, '%Y-%m-%d %H:%M:%S%.3f %z', 'America/New_York') as ny_time\n    FROM df\n\"\"\")\n\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 timestamp_str                 \u2506 ny_time                                           \u2502\n\u2502 ---                           \u2506 ---                                               \u2502\n\u2502 Utf8                          \u2506 Timestamp(Milliseconds, Some(\"America/New_York\")) \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-01-01 00:00:00.123 +0800 \u2506 2020-12-31 11:00:00.123 EST                       \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-02 12:30:00.456 +0800 \u2506 2021-01-01 23:30:00.456 EST                       \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"core_concepts/#temporal-truncation","title":"Temporal Truncation","text":"<p>The <code>.dt.truncate()</code> method allows you to truncate timestamps to specific time units. This can be useful for grouping data by time periods. For example, to truncate timestamps to the nearest hour:</p> \ud83d\udc0d Python <pre><code>df = daft.from_pydict({\n    \"timestamp\": [\n        datetime.datetime(2021, 1, 7, 0, 1, 1),\n        datetime.datetime(2021, 1, 8, 0, 1, 59),\n        datetime.datetime(2021, 1, 9, 0, 30, 0),\n        datetime.datetime(2021, 1, 10, 1, 59, 59),\n    ]\n})\n\n# Truncate timestamps to the nearest hour\ndf = df.with_column(\n    \"hour_start\",\n    df[\"timestamp\"].dt.truncate(\"1 hour\")\n)\n\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 timestamp                     \u2506 hour_start                    \u2502\n\u2502 ---                           \u2506 ---                           \u2502\n\u2502 Timestamp(Microseconds, None) \u2506 Timestamp(Microseconds, None) \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2021-01-07 00:01:01           \u2506 2021-01-07 00:00:00           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-08 00:01:59           \u2506 2021-01-08 00:00:00           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-09 00:30:00           \u2506 2021-01-09 00:00:00           \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2021-01-10 01:59:59           \u2506 2021-01-10 01:00:00           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>todo(docs): Should this section also have sql examples?</p> <p>Daft can read data from a variety of sources, and write data to many destinations.</p>"},{"location":"core_concepts/#reading-data","title":"Reading Data","text":""},{"location":"core_concepts/#from-files","title":"From Files","text":"<p>DataFrames can be loaded from file(s) on some filesystem, commonly your local filesystem or a remote cloud object store such as AWS S3.</p> <p>Additionally, Daft can read data from a variety of container file formats, including CSV, line-delimited JSON and Parquet.</p> <p>Daft supports file paths to a single file, a directory of files, and wildcards. It also supports paths to remote object storage such as AWS S3.</p> \ud83d\udc0d Python <pre><code>import daft\n\n# You can read a single CSV file from your local filesystem\ndf = daft.read_csv(\"path/to/file.csv\")\n\n# You can also read folders of CSV files, or include wildcards to select for patterns of file paths\ndf = daft.read_csv(\"path/to/*.csv\")\n\n# Other formats such as parquet and line-delimited JSON are also supported\ndf = daft.read_parquet(\"path/to/*.parquet\")\ndf = daft.read_json(\"path/to/*.json\")\n\n# Remote filesystems such as AWS S3 are also supported, and can be specified with their protocols\ndf = daft.read_csv(\"s3://mybucket/path/to/*.csv\")\n</code></pre> <p>To learn more about each of these constructors, as well as the options that they support, consult the API documentation on <code>creating DataFrames from files</code>.</p>"},{"location":"core_concepts/#from-data-catalogs","title":"From Data Catalogs","text":"<p>If you use catalogs such as Apache Iceberg or Hive, you may wish to consult our user guide on integrations with Data Catalogs: <code>Daft integration with Data Catalogs</code>.</p>"},{"location":"core_concepts/#from-file-paths","title":"From File Paths","text":"<p>Daft also provides an easy utility to create a DataFrame from globbing a path. You can use the <code>daft.from_glob_path</code> method which will read a DataFrame of globbed filepaths.</p> \ud83d\udc0d Python <pre><code>df = daft.from_glob_path(\"s3://mybucket/path/to/images/*.jpeg\")\n\n# +----------+------+-----+\n# | name     | size | ... |\n# +----------+------+-----+\n#   ...\n</code></pre> <p>This is especially useful for reading things such as a folder of images or documents into Daft. A common pattern is to then download data from these files into your DataFrame as bytes, using the <code>.url.download()</code> method.</p>"},{"location":"core_concepts/#from-memory","title":"From Memory","text":"<p>For testing, or small datasets that fit in memory, you may also create DataFrames using Python lists and dictionaries.</p> \ud83d\udc0d Python <pre><code># Create DataFrame using a dictionary of {column_name: list_of_values}\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [\"foo\", \"bar\", \"baz\"]})\n\n# Create DataFrame using a list of rows, where each row is a dictionary of {column_name: value}\ndf = daft.from_pylist([{\"A\": 1, \"B\": \"foo\"}, {\"A\": 2, \"B\": \"bar\"}, {\"A\": 3, \"B\": \"baz\"}])\n</code></pre> <p>To learn more, consult the API documentation on <code>creating DataFrames from in-memory data structures</code>.</p>"},{"location":"core_concepts/#from-databases","title":"From Databases","text":"<p>Daft can also read data from a variety of databases, including PostgreSQL, MySQL, Trino, and SQLite using the <code>daft.read_sql</code> method. In order to partition the data, you can specify a partition column, which will allow Daft to read the data in parallel.</p> \ud83d\udc0d Python <pre><code># Read from a PostgreSQL database\nuri = \"postgresql://user:password@host:port/database\"\ndf = daft.read_sql(\"SELECT * FROM my_table\", uri)\n\n# Read with a partition column\ndf = daft.read_sql(\"SELECT * FROM my_table\", partition_col=\"date\", uri)\n</code></pre> <p>To learn more, consult the <code>SQL User Guide</code> or the API documentation on <code>daft.read_sql</code>.</p>"},{"location":"core_concepts/#reading-a-column-of-urls","title":"Reading a column of URLs","text":"<p>Daft provides a convenient way to read data from a column of URLs using the <code>.url.download()</code> method. This is particularly useful when you have a DataFrame with a column containing URLs pointing to external resources that you want to fetch and incorporate into your DataFrame.</p> <p>Here's an example of how to use this feature:</p> \ud83d\udc0d Python <pre><code># Assume we have a DataFrame with a column named 'image_urls'\ndf = daft.from_pydict({\n    \"image_urls\": [\n        \"https://example.com/image1.jpg\",\n        \"https://example.com/image2.jpg\",\n        \"https://example.com/image3.jpg\"\n    ]\n})\n\n# Download the content from the URLs and create a new column 'image_data'\ndf = df.with_column(\"image_data\", df[\"image_urls\"].url.download())\ndf.show()\n</code></pre> Output<pre><code>+------------------------------------+------------------------------------+\n| image_urls                         | image_data                         |\n| Utf8                               | Binary                             |\n+====================================+====================================+\n| https://example.com/image1.jpg     | b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF...' |\n+------------------------------------+------------------------------------+\n| https://example.com/image2.jpg     | b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF...' |\n+------------------------------------+------------------------------------+\n| https://example.com/image3.jpg     | b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF...' |\n+------------------------------------+------------------------------------+\n\n(Showing first 3 of 3 rows)\n</code></pre> <p>This approach allows you to efficiently download and process data from a large number of URLs in parallel, leveraging Daft's distributed computing capabilities.</p>"},{"location":"core_concepts/#writing-data","title":"Writing Data","text":"<p>Writing data will execute your DataFrame and write the results out to the specified backend. The <code>df.write_*(...)</code> methods are used to write DataFrames to files or other destinations.</p> \ud83d\udc0d Python <pre><code># Write to various file formats in a local folder\ndf.write_csv(\"path/to/folder/\")\ndf.write_parquet(\"path/to/folder/\")\n\n# Write DataFrame to a remote filesystem such as AWS S3\ndf.write_csv(\"s3://mybucket/path/\")\n</code></pre> <p>Note</p> <p>Because Daft is a distributed DataFrame library, by default it will produce multiple files (one per partition) at your specified destination. Writing your dataframe is a blocking operation that executes your DataFrame. It will return a new <code>DataFrame</code> that contains the filepaths to the written data.</p>"},{"location":"core_concepts/#datatypes","title":"DataTypes","text":"<p>All columns in a Daft DataFrame have a DataType (also often abbreviated as <code>dtype</code>).</p> <p>All elements of a column are of the same dtype, or they can be the special Null value (indicating a missing value).</p> <p>Daft provides simple DataTypes that are ubiquituous in many DataFrames such as numbers, strings and dates - all the way up to more complex types like tensors and images.</p> <p>Tip</p> <p>For a full overview on all the DataTypes that Daft supports, see the DataType API Reference.</p>"},{"location":"core_concepts/#numeric-datatypes","title":"Numeric DataTypes","text":"<p>Numeric DataTypes allows Daft to represent numbers. These numbers can differ in terms of the number of bits used to represent them (8, 16, 32 or 64 bits) and the semantic meaning of those bits (float vs integer vs unsigned integers).</p> <p>Examples:</p> <ol> <li><code>DataType.int8()</code>: represents an 8-bit signed integer (-128 to 127)</li> <li><code>DataType.float32()</code>: represents a 32-bit float (a float number with about 7 decimal digits of precision)</li> </ol> <p>Columns/expressions with these datatypes can be operated on with many numeric expressions such as <code>+</code> and <code>*</code>.</p> <p>See also: Numeric Expressions</p>"},{"location":"core_concepts/#logical-datatypes","title":"Logical DataTypes","text":"<p>The <code>Boolean</code> DataType represents values which are boolean values: <code>True</code>, <code>False</code> or <code>Null</code>.</p> <p>Columns/expressions with this dtype can be operated on using logical expressions such as <code>&amp;</code> and <code>.if_else()</code>.</p> <p>See also: Logical Expressions</p>"},{"location":"core_concepts/#string-types","title":"String Types","text":"<p>Daft has string types, which represent a variable-length string of characters.</p> <p>As a convenience method, string types also support the <code>+</code> Expression, which has been overloaded to support concatenation of elements between two <code>DataType.string()</code> columns.</p> <ol> <li><code>DataType.string()</code>: represents a string of UTF-8 characters</li> <li><code>DataType.binary()</code>: represents a string of bytes</li> </ol> <p>See also: String Expressions</p>"},{"location":"core_concepts/#temporal-datatypes","title":"Temporal DataTypes","text":"<p>Temporal DataTypes represent data that have to do with time.</p> <p>Examples:</p> <ol> <li><code>DataType.date()</code>: represents a Date (year, month and day)</li> <li><code>DataType.timestamp()</code>: represents a Timestamp (particular instance in time)</li> </ol> <p>See also: Temporal Expressions</p>"},{"location":"core_concepts/#nested-datatypes","title":"Nested DataTypes","text":"<p>Nested DataTypes wrap other DataTypes, allowing you to compose types into complex data structures.</p> <p>Examples:</p> <ol> <li><code>DataType.list(child_dtype)</code>: represents a list where each element is of the child <code>dtype</code></li> <li><code>DataType.struct({\"field_name\": child_dtype})</code>: represents a structure that has children <code>dtype</code>s, each mapped to a field name</li> </ol>"},{"location":"core_concepts/#python-datatype","title":"Python DataType","text":"<p>The <code>DataType.python()</code> DataType represent items that are Python objects.</p> <p>Warning</p> <p>Daft does not impose any invariants about what Python types these objects are. To Daft, these are just generic Python objects!</p> <p>Python is AWESOME because it's so flexible, but it's also slow and memory inefficient! Thus we recommend:</p> <ol> <li>Cast early!: Casting your Python data into native Daft DataTypes if possible - this results in much more efficient downstream data serialization and computation.</li> <li>Use Python UDFs: If there is no suitable Daft representation for your Python objects, use Python UDFs to process your Python data and extract the relevant data to be returned as native Daft DataTypes!</li> </ol> <p>Note</p> <p>If you work with Python classes for a generalizable use-case (e.g. documents, protobufs), it may be that these types are good candidates for \"promotion\" into a native Daft type! Please get in touch with the Daft team and we would love to work together on building your type into canonical Daft types.</p>"},{"location":"core_concepts/#complex-datatypes","title":"Complex DataTypes","text":"<p>Daft supports many more interesting complex DataTypes, for example:</p> <ul> <li><code>DataType.tensor()</code>: Multi-dimensional (potentially uniformly-shaped) tensors of data</li> <li><code>DataType.embedding()</code>: Lower-dimensional vector representation of data (e.g. words)</li> <li><code>DataType.image()</code>: NHWC images</li> </ul> <p>Daft abstracts away the in-memory representation of your data and provides kernels for many common operations on top of these data types. For supported image operations see the image expressions API reference. For more complex algorithms, you can also drop into a Python UDF to process this data using your custom Python libraries.</p> <p>Please add suggestions for new DataTypes to our Github Discussions page!</p>"},{"location":"core_concepts/#sql","title":"SQL","text":"<p>Daft supports Structured Query Language (SQL) as a way of constructing query plans (represented in Python as a <code>daft.DataFrame</code>) and expressions (<code>daft.Expression</code>).</p> <p>SQL is a human-readable way of constructing these query plans, and can often be more ergonomic than using DataFrames for writing queries.</p> <p>Daft's SQL support is new and is constantly being improved on!</p> <p>Please give us feedback or submit an issue and we'd love to hear more about what you would like.</p>"},{"location":"core_concepts/#running-sql-on-dataframes","title":"Running SQL on DataFrames","text":"<p>Daft's <code>daft.sql</code> function will automatically detect any <code>daft.DataFrame</code> objects in your current Python environment to let you query them easily by name.</p> \u2699\ufe0f SQL <pre><code># Note the variable name `my_special_df`\nmy_special_df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\n\n# Use the SQL table name \"my_special_df\" to refer to the above DataFrame!\nsql_df = daft.sql(\"SELECT A, B FROM my_special_df\")\n\nsql_df.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B     \u2502\n\u2502 ---   \u2506 ---   \u2502\n\u2502 Int64 \u2506 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 2     \u2506 2     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 3     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 3 of 3 rows)\n</code></pre> <p>In the above example, we query the DataFrame called <code>\"my_special_df\"</code> by simply referring to it in the SQL command. This produces a new DataFrame <code>sql_df</code> which can natively integrate with the rest of your Daft query.</p>"},{"location":"core_concepts/#reading-data-from-sql","title":"Reading data from SQL","text":"<p>Warning</p> <p>This feature is a WIP and will be coming soon! We will support reading common datasources directly from SQL:</p> \ud83d\udc0d Python <pre><code>daft.sql(\"SELECT * FROM read_parquet('s3://...')\")\ndaft.sql(\"SELECT * FROM read_delta_lake('s3://...')\")\n</code></pre> <p>Today, a workaround for this is to construct your dataframe in Python first and use it from SQL instead:</p> \ud83d\udc0d Python <pre><code>df = daft.read_parquet(\"s3://...\")\ndaft.sql(\"SELECT * FROM df\")\n</code></pre> <p>We appreciate your patience with us and hope to deliver this crucial feature soon!</p>"},{"location":"core_concepts/#sql-expressions","title":"SQL Expressions","text":"<p>SQL has the concept of expressions as well. Here is an example of a simple addition expression, adding columns \"a\" and \"b\" in SQL to produce a new column C.</p> <p>We also present here the equivalent query for SQL and DataFrame. Notice how similar the concepts are!</p> \u2699\ufe0f SQL\ud83d\udc0d Python <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\ndf = daft.sql(\"SELECT A + B as C FROM df\")\ndf.show()\n</code></pre> <pre><code>expr = (daft.col(\"A\") + daft.col(\"B\")).alias(\"C\")\n\ndf = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\ndf = df.select(expr)\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 C     \u2502\n\u2502 ---   \u2502\n\u2502 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 2     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 4     \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 6     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 3 of 3 rows)\n</code></pre> <p>In the above query, both the SQL version of the query and the DataFrame version of the query produce the same result.</p> <p>Under the hood, they run the same Expression <code>col(\"A\") + col(\"B\")</code>!</p> <p>One really cool trick you can do is to use the <code>daft.sql_expr</code> function as a helper to easily create Expressions. The following are equivalent:</p> \u2699\ufe0f SQL\ud83d\udc0d Python <pre><code>sql_expr = daft.sql_expr(\"A + B as C\")\nprint(\"SQL expression:\", sql_expr)\n</code></pre> <pre><code>py_expr = (daft.col(\"A\") + daft.col(\"B\")).alias(\"C\")\nprint(\"Python expression:\", py_expr)\n</code></pre> Output<pre><code>SQL expression: col(A) + col(B) as C\nPython expression: col(A) + col(B) as C\n</code></pre> <p>This means that you can pretty much use SQL anywhere you use Python expressions, making Daft extremely versatile at mixing workflows which leverage both SQL and Python.</p> <p>As an example, consider the filter query below and compare the two equivalent Python and SQL queries:</p> \u2699\ufe0f SQL\ud83d\udc0d Python <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\n\n# Daft automatically converts this string using `daft.sql_expr`\ndf = df.where(\"A &lt; 2\")\n\ndf.show()\n</code></pre> <pre><code>df = daft.from_pydict({\"A\": [1, 2, 3], \"B\": [1, 2, 3]})\n\n# Using Daft's Python Expression API\ndf = df.where(df[\"A\"] &lt; 2)\n\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 A     \u2506 B     \u2502\n\u2502 ---   \u2506 ---   \u2502\n\u2502 Int64 \u2506 Int64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 1     \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 1 of 1 rows)\n</code></pre> <p>Pretty sweet! Of course, this support for running Expressions on your columns extends well beyond arithmetic as we'll see in the next section on SQL Functions.</p>"},{"location":"core_concepts/#sql-functions","title":"SQL Functions","text":"<p>SQL also has access to all of Daft's powerful <code>daft.Expression</code> functionality through SQL functions.</p> <p>However, unlike the Python Expression API which encourages method-chaining (e.g. <code>col(\"a\").url.download().image.decode()</code>), in SQL you have to do function nesting instead (e.g. <code>\"image_decode(url_download(a))\"</code>).</p> <p>Note</p> <p>A full catalog of the available SQL Functions in Daft is available in the <code>../api_docs/sql</code>.</p> <p>Note that it closely mirrors the Python API, with some function naming differences vs the available Python methods. We also have some aliased functions for ANSI SQL-compliance or familiarity to users coming from other common SQL dialects such as PostgreSQL and SparkSQL to easily find their functionality.</p> <p>Here is an example of an equivalent function call in SQL vs Python:</p> \u2699\ufe0f SQL\ud83d\udc0d Python <pre><code>df = daft.from_pydict({\"urls\": [\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n]})\ndf = daft.sql(\"SELECT image_decode(url_download(urls)) FROM df\")\ndf.show()\n</code></pre> <pre><code>df = daft.from_pydict({\"urls\": [\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n    \"https://user-images.githubusercontent.com/17691182/190476440-28f29e87-8e3b-41c4-9c28-e112e595f558.png\",\n]})\ndf = df.select(daft.col(\"urls\").url.download().image.decode())\ndf.show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 urls         \u2502\n\u2502 ---          \u2502\n\u2502 Image[MIXED] \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 &lt;Image&gt;      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 &lt;Image&gt;      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 &lt;Image&gt;      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 3 of 3 rows)\n</code></pre>"},{"location":"core_concepts/#aggregations-and-grouping","title":"Aggregations and Grouping","text":"<p>Some operations such as the sum or the average of a column are called aggregations. Aggregations are operations that reduce the number of rows in a column.</p>"},{"location":"core_concepts/#global-aggregations","title":"Global Aggregations","text":"<p>An aggregation can be applied on an entire DataFrame, for example to get the mean on a specific column:</p> \ud83d\udc0d Python <pre><code>import daft\n\ndf = daft.from_pydict({\n    \"class\": [\"a\", \"a\", \"b\", \"b\"],\n    \"score\": [10, 20., 30., 40],\n})\n\ndf.mean(\"score\").show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 score   \u2502\n\u2502 ---     \u2502\n\u2502 Float64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 25      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 1 of 1 rows)\n</code></pre> <p>For a full list of available Dataframe aggregations, see Aggregations.</p> <p>Aggregations can also be mixed and matched across columns, via the <code>agg</code> method:</p> \ud83d\udc0d Python <pre><code>df.agg(\n    df[\"score\"].mean().alias(\"mean_score\"),\n    df[\"score\"].max().alias(\"max_score\"),\n    df[\"class\"].count().alias(\"class_count\"),\n).show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 mean_score \u2506 max_score \u2506 class_count \u2502\n\u2502 ---        \u2506 ---       \u2506 ---         \u2502\n\u2502 Float64    \u2506 Float64   \u2506 UInt64      \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 25         \u2506 40        \u2506 4           \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 1 of 1 rows)\n</code></pre> <p>For a full list of available aggregation expressions, see Aggregation Expressions</p>"},{"location":"core_concepts/#grouped-aggregations","title":"Grouped Aggregations","text":"<p>Aggregations can also be called on a \"Grouped DataFrame\". For the above example, perhaps we want to get the mean \"score\" not for the entire DataFrame, but for each \"class\".</p> <p>Let's run the mean of column \"score\" again, but this time grouped by \"class\":</p> \ud83d\udc0d Python <pre><code>df.groupby(\"class\").mean(\"score\").show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 class \u2506 score   \u2502\n\u2502 ---   \u2506 ---     \u2502\n\u2502 Utf8  \u2506 Float64 \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a     \u2506 15      \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b     \u2506 35      \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 2 of 2 rows)\n</code></pre> <p>To run multiple aggregations on a Grouped DataFrame, you can use the <code>agg</code> method:</p> \ud83d\udc0d Python <pre><code>df.groupby(\"class\").agg(\n    df[\"score\"].mean().alias(\"mean_score\"),\n    df[\"score\"].max().alias(\"max_score\"),\n).show()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 class \u2506 mean_score \u2506 max_score \u2502\n\u2502 ---   \u2506 ---        \u2506 ---       \u2502\n\u2502 Utf8  \u2506 Float64    \u2506 Float64   \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 a     \u2506 15         \u2506 20        \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 b     \u2506 35         \u2506 40        \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 2 of 2 rows)\n</code></pre>"},{"location":"core_concepts/#user-defined-functions-udf","title":"User-Defined Functions (UDF)","text":"<p>A key piece of functionality in Daft is the ability to flexibly define custom functions that can run computations on any data in your dataframe. This section walks you through the different types of UDFs that Daft allows you to run.</p> <p>Let's first create a dataframe that will be used as a running example throughout this tutorial!</p> \ud83d\udc0d Python <pre><code>import daft\nimport numpy as np\n\ndf = daft.from_pydict({\n    # the `image` column contains images represented as 2D numpy arrays\n    \"image\": [np.ones((128, 128)) for i in range(16)],\n    # the `crop` column contains a box to crop from our image, represented as a list of integers: [x1, x2, y1, y2]\n    \"crop\": [[0, 1, 0, 1] for i in range(16)],\n})\n</code></pre>"},{"location":"core_concepts/#per-column-per-row-functions-using-apply","title":"Per-column per-row functions using <code>.apply</code>","text":"<p>You can use <code>.apply</code> to run a Python function on every row in a column.</p> <p>For example, the following example creates a new <code>flattened_image</code> column by calling <code>.flatten()</code> on every object in the <code>image</code> column.</p> \ud83d\udc0d Python <pre><code>df.with_column(\n    \"flattened_image\",\n    df[\"image\"].apply(lambda img: img.flatten(), return_dtype=daft.DataType.python())\n).show(2)\n</code></pre> Output<pre><code>+----------------------+---------------+---------------------+\n| image                | crop          | flattened_image     |\n| Python               | List[Int64]   | Python              |\n+======================+===============+=====================+\n| [[1. 1. 1. ... 1. 1. | [0, 1, 0, 1]  | [1. 1. 1. ... 1. 1. |\n| 1.]  [1. 1. 1. ...   |               | 1.]                 |\n| 1. 1. 1.]  [1. 1.... |               |                     |\n+----------------------+---------------+---------------------+\n| [[1. 1. 1. ... 1. 1. | [0, 1, 0, 1]  | [1. 1. 1. ... 1. 1. |\n| 1.]  [1. 1. 1. ...   |               | 1.]                 |\n| 1. 1. 1.]  [1. 1.... |               |                     |\n+----------------------+---------------+---------------------+\n(Showing first 2 rows)\n</code></pre> <p>Note here that we use the <code>return_dtype</code> keyword argument to specify that our returned column type is a Python column!</p>"},{"location":"core_concepts/#multi-column-per-partition-functions-using-udf","title":"Multi-column per-partition functions using <code>@udf</code>","text":"<p><code>.apply</code> is great for convenience, but has two main limitations:</p> <ol> <li>It can only run on single columns</li> <li>It can only run on single items at a time</li> </ol> <p>Daft provides the <code>@udf</code> decorator for defining your own UDFs that process multiple columns or multiple rows at a time.</p> <p>For example, let's try writing a function that will crop all our images in the <code>image</code> column by its corresponding value in the <code>crop</code> column:</p> \ud83d\udc0d Python <pre><code>@daft.udf(return_dtype=daft.DataType.python())\ndef crop_images(images, crops, padding=0):\n    cropped = []\n    for img, crop in zip(images.to_pylist(), crops.to_pylist()):\n        x1, x2, y1, y2 = crop\n        cropped_img = img[x1:x2 + padding, y1:y2 + padding]\n        cropped.append(cropped_img)\n    return cropped\n\ndf = df.with_column(\n    \"cropped\",\n    crop_images(df[\"image\"], df[\"crop\"], padding=1),\n)\ndf.show(2)\n</code></pre> Output<pre><code>+----------------------+---------------+--------------------+\n| image                | crop          | cropped            |\n| Python               | List[Int64]   | Python             |\n+======================+===============+====================+\n| [[1. 1. 1. ... 1. 1. | [0, 1, 0, 1]  | [[1. 1.]  [1. 1.]] |\n| 1.]  [1. 1. 1. ...   |               |                    |\n| 1. 1. 1.]  [1. 1.... |               |                    |\n+----------------------+---------------+--------------------+\n| [[1. 1. 1. ... 1. 1. | [0, 1, 0, 1]  | [[1. 1.]  [1. 1.]] |\n| 1.]  [1. 1. 1. ...   |               |                    |\n| 1. 1. 1.]  [1. 1.... |               |                    |\n+----------------------+---------------+--------------------+\n(Showing first 2 rows)\n</code></pre> <p>There's a few things happening here, let's break it down:</p> <ol> <li> <p><code>crop_images</code> is a normal Python function. It takes as input:</p> <p>a. A list of images: <code>images</code></p> <p>b. A list of cropping boxes: <code>crops</code></p> <p>c. An integer indicating how much padding to apply to the right and bottom of the cropping: <code>padding</code></p> </li> <li> <p>To allow Daft to pass column data into the <code>images</code> and <code>crops</code> arguments, we decorate the function with <code>@udf</code></p> <p>a. <code>return_dtype</code> defines the returned data type. In this case, we return a column containing Python objects of numpy arrays</p> <p>b. At runtime, because we call the UDF on the <code>image</code> and <code>crop</code> columns, the UDF will receive a <code>daft.Series</code> object for each argument.</p> </li> <li> <p>We can create a new column in our DataFrame by applying our UDF on the <code>\"image\"</code> and <code>\"crop\"</code> columns inside of a <code>df.with_column()</code> call.</p> </li> </ol>"},{"location":"core_concepts/#udf-inputs","title":"UDF Inputs","text":"<p>When you specify an Expression as an input to a UDF, Daft will calculate the result of that Expression and pass it into your function as a <code>daft.Series</code> object.</p> <p>The Daft <code>daft.Series</code> is just an abstraction on a \"column\" of data! You can obtain several different data representations from a <code>daft.Series</code>:</p> <ol> <li>PyArrow Arrays (<code>pa.Array</code>): <code>s.to_arrow()</code></li> <li>Python lists (<code>list</code>): <code>s.to_pylist()</code></li> </ol> <p>Depending on your application, you may choose a different data representation that is more performant or more convenient!</p> <p>Info</p> <p>Certain array formats have some restrictions around the type of data that they can handle:</p> <ol> <li> <p>Null Handling: In Pandas and Numpy, nulls are represented as NaNs for numeric types, and Nones for non-numeric types. Additionally, the existence of nulls will trigger a type casting from integer to float arrays. If null handling is important to your use-case, we recommend using one of the other available options.</p> </li> <li> <p>Python Objects: PyArrow array formats cannot support Python columns.</p> </li> </ol> <p>We recommend using Python lists if performance is not a major consideration, and using the arrow-native formats such as PyArrow arrays and numpy arrays if performance is important.</p>"},{"location":"core_concepts/#return-types","title":"Return Types","text":"<p>The <code>return_dtype</code> argument specifies what type of column your UDF will return. Types can be specified using the <code>daft.DataType</code> class.</p> <p>Your UDF function itself needs to return a batch of columnar data, and can do so as any one of the following array types:</p> <ol> <li>Numpy Arrays (<code>np.ndarray</code>)</li> <li>PyArrow Arrays (<code>pa.Array</code>)</li> <li>Python lists (<code>list</code>)</li> </ol> <p>Note that if the data you have returned is not castable to the return_dtype that you specify (e.g. if you return a list of floats when you've specified a <code>return_dtype=DataType.bool()</code>), Daft will throw a runtime error!</p>"},{"location":"core_concepts/#class-udfs","title":"Class UDFs","text":"<p>UDFs can also be created on Classes, which allow for initialization on some expensive state that can be shared between invocations of the class, for example downloading data or creating a model.</p> \ud83d\udc0d Python <pre><code>@daft.udf(return_dtype=daft.DataType.int64())\nclass RunModel:\n\n    def __init__(self):\n        # Perform expensive initializations\n        self._model = create_model()\n\n    def __call__(self, features_col):\n        return self._model(features_col)\n</code></pre> <p>Running Class UDFs are exactly the same as running their functional cousins.</p> \ud83d\udc0d Python <pre><code>df = df.with_column(\"image_classifications\", RunModel(df[\"images\"]))\n</code></pre>"},{"location":"core_concepts/#resource-requests","title":"Resource Requests","text":"<p>Sometimes, you may want to request for specific resources for your UDF. For example, some UDFs need one GPU to run as they will load a model onto the GPU.</p> <p>To do so, you can create your UDF and assign it a resource request:</p> \ud83d\udc0d Python <pre><code>@daft.udf(return_dtype=daft.DataType.int64(), num_gpus=1)\nclass RunModelWithOneGPU:\n\n    def __init__(self):\n        # Perform expensive initializations\n        self._model = create_model()\n\n    def __call__(self, features_col):\n        return self._model(features_col)\n</code></pre> <pre><code>df = df.with_column(\n    \"image_classifications\",\n    RunModelWithOneGPU(df[\"images\"]),\n)\n</code></pre> <p>In the above example, if Daft ran on a Ray cluster consisting of 8 GPUs and 64 CPUs, Daft would be able to run 8 replicas of your UDF in parallel, thus massively increasing the throughput of your UDF!</p> <p>UDFs can also be parametrized with new resource requests after being initialized.</p> \ud83d\udc0d Python <pre><code>RunModelWithTwoGPUs = RunModelWithOneGPU.override_options(num_gpus=2)\ndf = df.with_column(\n    \"image_classifications\",\n    RunModelWithTwoGPUs(df[\"images\"]),\n)\n</code></pre>"},{"location":"core_concepts/#example-udfs-in-ml-workloads","title":"Example: UDFs in ML Workloads","text":"<p>We\u2019ll define a function that uses a pre-trained PyTorch model: ResNet50 to classify the dog pictures. We\u2019ll pass the contents of the image <code>urls</code> column and send the classification predictions to a new column <code>classify_breed</code>.</p> <p>Working with PyTorch adds some complexity but you can just run the cells below to perform the classification.</p> <p>First, make sure to install and import some extra dependencies:</p> <pre><code>%pip install validators matplotlib Pillow torch torchvision\n</code></pre> \ud83d\udc0d Python <pre><code># import additional libraries, these are necessary for PyTorch\nimport torch\n</code></pre> <p>Define your <code>ClassifyImages</code> UDF. Models are expensive to initialize and load, so we want to do this as few times as possible, and share a model across multiple invocations.</p> \ud83d\udc0d Python <pre><code>@udf(return_dtype=DataType.fixed_size_list(dtype=DataType.string(), size=2))\nclass ClassifyImages:\n    def __init__(self):\n        # Perform expensive initializations - create and load the pre-trained model\n        self.model = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_resnet50\", pretrained=True)\n        self.utils = torch.hub.load(\"NVIDIA/DeepLearningExamples:torchhub\", \"nvidia_convnets_processing_utils\")\n        self.model.eval().to(torch.device(\"cpu\"))\n\n    def __call__(self, images_urls):\n        uris = images_urls.to_pylist()\n        batch = torch.cat([self.utils.prepare_input_from_uri(uri) for uri in uris]).to(torch.device(\"cpu\"))\n\n        with torch.no_grad():\n            output = torch.nn.functional.softmax(self.model(batch), dim=1)\n\n        results = self.utils.pick_n_best(predictions=output, n=1)\n        return [result[0] for result in results]\n</code></pre> <p>Now you're ready to call this function on the <code>urls</code> column and store the outputs in a new column we'll call <code>classify_breed</code>:</p> \ud83d\udc0d Python <pre><code>classified_images_df = df_family.with_column(\"classify_breed\", ClassifyImages(daft.col(\"urls\")))\nclassified_images_df.select(\"dog_name\", \"image\", \"classify_breed\").show()\n</code></pre> <p>todo(docs): Insert table of dog urls? or new UDF example?</p>"},{"location":"core_concepts/#multimodal-data","title":"Multimodal Data","text":"<p>Daft is built to work comfortably with multimodal data types, including URLs and images. You can use the <code>url.download()</code> expression to download the bytes from a URL. Let\u2019s store them in a new column using the <code>with_column</code> method:</p> <p>todo(docs): This example is originally from 10min quickstart</p> \ud83d\udc0d Python <pre><code>df_family = df_family.with_column(\"image_bytes\", df_dogs[\"urls\"].url.download(on_error=\"null\"))\ndf_family.show()\n</code></pre> Output<pre><code>+-------------------+---------+----------+------------------------------------------------------------------+--------------------------------------------+\n| full_name         | has_dog | dog_name | urls                                                             | image_bytes                                |\n| Utf8              | Boolean | Utf8     | Utf8                                                             | Binary                                     |\n+-------------------+---------+----------+------------------------------------------------------------------+--------------------------------------------+\n| Ernesto Evergreen | true    | Ernie    | https://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpg | b\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... |\n| James Jale        | true    | Jackie   | https://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpg | b\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... |\n| Wolfgang Winter   | true    | Wolfie   | https://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpg | b\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... |\n| Shandra Shamas    | true    | Shaggie  | https://live.staticflickr.com/65535/53671838039_b97411a441_o.jpg | b\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... |\n| Zaya Zaphora      | true    | Zadie    | https://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpg | b\"\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\"... |\n+-------------------+---------+----------+------------------------------------------------------------------+--------------------------------------------+\n(Showing first 5 of 5 rows)\n</code></pre> <p>Let\u2019s turn the bytes into human-readable images using <code>image.decode()</code>:</p> \ud83d\udc0d Python <pre><code>df_family = df_family.with_column(\"image\", daft.col(\"image_bytes\").image.decode())\ndf_family.show()\n</code></pre> <p>todo(docs): This example is originally from 10min quickstart. Insert dog images or create new example?</p>"},{"location":"core_concepts/#whats-next","title":"What's Next?","text":""},{"location":"core_concepts/#integrations","title":"Integrations","text":"<ul> <li> Ray</li> <li>Unity Catalog</li> <li>Apache Iceberg</li> <li>Delta Lake</li> <li> Microsoft Azure</li> <li> Amazon Web Services (AWS)</li> <li>SQL</li> <li> Hugging Face Datasets</li> </ul>"},{"location":"core_concepts/#migration-guide","title":"Migration Guide","text":"<ul> <li> Coming from Dask</li> </ul>"},{"location":"core_concepts/#tutorials","title":"Tutorials","text":"<ul> <li> MNIST Digit Classification</li> <li> Running LLMs on the Red Pajamas Dataset</li> <li> Querying Images with UDFs</li> <li> Image Generation on GPUs</li> </ul>"},{"location":"core_concepts/#advanced","title":"Advanced","text":"<ul> <li> Managing Memory Usage</li> <li> Partitioning</li> <li> Distributed Computing</li> </ul>"},{"location":"distributed/","title":"Distributed Computing","text":"<p>todo(docs): add daft launcher docs and review order of information</p> <p>By default, Daft runs using your local machine's resources and your operations are thus limited by the CPUs, memory and GPUs available to you in your single local development machine.</p> <p>However, Daft has strong integrations with Ray which is a distributed computing framework for distributing computations across a cluster of machines. Here is a snippet showing how you can connect Daft to a Ray cluster:</p> \ud83d\udc0d Python <pre><code>import daft\n\ndaft.context.set_runner_ray()\n</code></pre> <p>By default, if no address is specified Daft will spin up a Ray cluster locally on your machine. If you are running Daft on a powerful machine (such as an AWS P3 machine which is equipped with multiple GPUs) this is already very useful because Daft can parallelize its execution of computation across your CPUs and GPUs. However, if instead you already have your own Ray cluster running remotely, you can connect Daft to it by supplying an address:</p> \ud83d\udc0d Python <pre><code>daft.context.set_runner_ray(address=\"ray://url-to-mycluster\")\n</code></pre> <p>For more information about the <code>address</code> keyword argument, please see the Ray documentation on initialization.</p> <p>If you want to start a single node ray cluster on your local machine, you can do the following:</p> <pre><code>&gt; pip install ray[default]\n&gt; ray start --head --port=6379\n</code></pre> <p>This should output something like:</p> <pre><code>Usage stats collection is enabled. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.\n\nLocal node IP: 127.0.0.1\n\n--------------------\nRay runtime started.\n--------------------\n\n...\n</code></pre> <p>You can take the IP address and port and pass it to Daft:</p> \ud83d\udc0d Python <pre><code>&gt;&gt;&gt; import daft\n&gt;&gt;&gt; daft.context.set_runner_ray(\"127.0.0.1:6379\")\nDaftContext(_daft_execution_config=&lt;daft.daft.PyDaftExecutionConfig object at 0x100fbd1f0&gt;, _daft_planning_config=&lt;daft.daft.PyDaftPlanningConfig object at 0x100fbd270&gt;, _runner_config=_RayRunnerConfig(address='127.0.0.1:6379', max_task_backlog=None), _disallow_set_runner=True, _runner=None)\n&gt;&gt;&gt; df = daft.from_pydict({\n...   'text': ['hello', 'world']\n... })\n2024-07-29 15:49:26,610 INFO worker.py:1567 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n2024-07-29 15:49:26,622 INFO worker.py:1752 -- Connected to Ray cluster.\n&gt;&gt;&gt; print(df)\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 text  \u2502\n\u2502 ---   \u2502\n\u2502 Utf8  \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 hello \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 world \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 2 of 2 rows)\n</code></pre>"},{"location":"distributed/#daft-launcher","title":"Daft Launcher","text":"<p>Daft Launcher is a convenient command-line tool that provides simple abstractions over Ray, enabling a quick uptime for users to leverage Daft for distributed computations. Rather than worrying about the complexities of managing Ray, users can simply run a few CLI commands to spin up a cluster, submit a job, observe the status of jobs and clusters, and spin down a cluster.</p>"},{"location":"distributed/#prerequisites","title":"Prerequisites","text":"<p>The following should be installed on your machine:</p> <ul> <li> <p>The AWS CLI tool. (Assuming you're using AWS as your cloud provider)</p> </li> <li> <p>A python package manager. We recommend using <code>uv</code> to manage everything (i.e., dependencies, as well as the python version itself). It's much cleaner and faster than <code>pip</code>.</p> </li> </ul>"},{"location":"distributed/#install-daft-launcher","title":"Install Daft Launcher","text":"<p>Run the following commands in your terminal to initialize your project:</p> <pre><code># Create a project directory\ncd some/working/directory\nmkdir launch-test\ncd launch-test\n\n# Initialize the project\nuv init --python 3.12\nuv venv\nsource .venv/bin/activate\n\n# Install Daft Launcher\nuv pip install \"daft-launcher\"\n</code></pre> <p>In your virtual environment, you should have Daft launcher installed \u2014 you can verify this by running <code>daft --version</code> which will return the latest version of Daft launcher available. You should also have a basic working directly that may look something like this:</p> <pre><code>/\n|- .venv/\n|- hello.py\n|- pyproject.toml\n|- README.md\n|- .python-version\n</code></pre>"},{"location":"distributed/#configure-aws-credentials","title":"Configure AWS Credentials","text":"<p>Establish an SSO connection to configure your AWS credentials:</p> <pre><code># Configure your SSO\naws configure sso\n\n# Login to your SSO\naws sso login\n</code></pre> <p>These commands should open your browsers. Accept the prompted requests and then return to your terminal, you should see a success message from the AWS CLI tool. At this point, your AWS CLI tool has been configured and your environment is fully setup.</p>"},{"location":"distributed/#initialize-configuration-file","title":"Initialize Configuration File","text":"<p>Initialize a default configuration file to store default values that you can later tune, and they are denoted as required and optional respectively.</p> <pre><code># Initialize the default .daft.toml configuration file\ndaft init-config\n\n# Optionally you can also specify a custom name for your file\ndaft init-config my-custom-config.toml\n</code></pre> <p>Fill out the required values in your <code>.daft.toml</code> file. Optional configurations will have a default value pre-defined.</p> <pre><code>[setup]\n\n# (required)\n# The name of the cluster.\nname = ...\n\n# (required)\n# The cloud provider that this cluster will be created in.\n# Has to be one of the following:\n# - \"aws\"\n# - \"gcp\"\n# - \"azure\"\nprovider = ...\n\n# (optional; default = None)\n# The IAM instance profile ARN which will provide this cluster with the necessary permissions to perform whatever actions.\n# Please note that if you don't specify this field, Ray will create an automatic instance profile for you.\n# That instance profile will be minimal and may restrict some of the feature of Daft.\niam_instance_profile_arn = ...\n\n# (required)\n# The AWS region in which to place this cluster.\nregion = ...\n\n# (optional; default = \"ec2-user\")\n# The ssh user name when connecting to the cluster.\nssh_user = ...\n\n# (optional; default = 2)\n# The number of worker nodes to create in the cluster.\nnumber_of_workers = ...\n\n# (optional; default = \"m7g.medium\")\n# The instance type to use for the head and worker nodes.\ninstance_type = ...\n\n# (optional; default = \"ami-01c3c55948a949a52\")\n# The AMI ID to use for the head and worker nodes.\nimage_id = ...\n\n# (optional; default = [])\n# A list of dependencies to install on the head and worker nodes.\n# These will be installed using UV (https://docs.astral.sh/uv/).\ndependencies = [...]\n\n[run]\n\n# (optional; default = ['echo \"Hello, World!\"'])\n# Any post-setup commands that you want to invoke manually.\n# This is a good location to install any custom dependencies or run some arbitrary script.\nsetup_commands = [...]\n</code></pre>"},{"location":"distributed/#spin-up-a-cluster","title":"Spin Up a Cluster","text":"<p><code>daft up</code> will spin up a cluster given the configuration file you initialized earlier. The configuration file contains all required information necessary for Daft launcher to know how to spin up a cluster.</p> <pre><code># Spin up a cluster using the default .daft.toml configuration file created earlier\ndaft up\n\n# Alternatively spin up a cluster using a custom configuration file created earlier\ndaft up -c my-custom-config.toml\n</code></pre> <p>This command will do a couple of things:</p> <ol> <li> <p>First, it will reach into your cloud provider and spin up the necessary resources. This includes things such as the worker nodes, security groups, permissions, etc.</p> </li> <li> <p>When the nodes are spun up, the ray and daft dependencies will be downloaded into a python virtual environment.</p> </li> <li> <p>Next, any other custom dependencies that you've specified in the configuration file will then be downloaded.</p> </li> <li> <p>Finally, the setup commands that you've specified in the configuration file will be run on the head node.</p> </li> </ol> <p>Note</p> <p><code>daft up</code> will only return successfully when the head node is fully set up. Even though the command will request the worker nodes to also spin up, it will not wait for them to be spun up before returning. Therefore, when the command completes and you type <code>daft list</code>, the worker nodes may be in a \u201cpending\u201d state immediately after. Give it a few seconds and they should be fully running.</p>"},{"location":"distributed/#submit-a-job","title":"Submit a Job","text":"<p><code>daft submit</code> enables you to submit a working directory and command or a \u201cjob\u201d to the remote cluster to be run.</p> <pre><code># Submit a job using the default .daft.toml configuration file\ndaft submit -i my-keypair.pem -w my-working-director\n\n# Alternatively submit a job using a custom configuration file\ndaft submit -c my-custom-config.toml -i my-keypair.pem -w my-working-director\n</code></pre>"},{"location":"distributed/#run-a-sql-query","title":"Run a SQL Query","text":"<p>Daft supports SQL API so you can use <code>daft sql</code> to run raw SQL queries against your data. The SQL dialect is the postgres standard.</p> <pre><code># Run a sql query using the default .daft.toml configuration file\ndaft sql -- \"\\\"SELECT * FROM my_table\\\"\"\n\n# Alternatively you can run a sql query using a custom configuration file\ndaft sql -c my-custom-config.toml -- \"\\\"SELECT * FROM my_table\\\"\"\n</code></pre>"},{"location":"distributed/#view-ray-dashboard","title":"View Ray Dashboard","text":"<p>You can view the Ray dashboard of your running cluster with <code>daft connect</code> which establishes a port-forward over SSH from your local machine to the head node of the cluster (connecting\u00a0<code>localhost:8265</code>\u00a0to the remote head's\u00a0<code>8265</code>).</p> <pre><code># Establish the port-forward using the default .daft.toml configuration file\ndaft connect -i my-keypair.pem\n\n# Alternatively establish the port-forward using a custom configuration file\ndaft connect -c my-custom-config.toml -i my-keypair.pem\n</code></pre> <p>Note</p> <p><code>daft connect</code> will require you to have the appropriate SSH keypair to authenticate against the remote head\u2019s public SSH keypair. Make sure to pass this SSH keypair as an argument to the command.</p>"},{"location":"distributed/#spin-down-a-cluster","title":"Spin Down a Cluster","text":"<p><code>daft down</code> will spin down all instances of the cluster specified in the configuration file, not just the head node.</p> <pre><code># Spin down a cluster using the default .daft.toml configuration file\ndaft down\n\n# Alternatively spin down a cluster using a custom configuration file\ndaft down -c my-custom-config.toml\n</code></pre>"},{"location":"distributed/#list-running-and-terminated-clusters","title":"List Running and Terminated Clusters","text":"<p><code>daft list</code> allows you to view the current state of all clusters, running and terminated, and includes each instance name and their given IPs (assuming the cluster is running). Here\u2019s an example output after running <code>daft list</code>:</p> <pre><code>Running:\n  - daft-demo, head, i-053f9d4856d92ea3d, 35.94.91.91\n  - daft-demo, worker, i-00c340dc39d54772d\n  - daft-demo, worker, i-042a96ce1413c1dd6\n</code></pre> <p>Say we spun up another cluster <code>new-cluster</code> and then terminated it, here\u2019s what the output of <code>daft list</code> would look like immediately after:</p> <pre><code>Running:\n  - daft-demo, head, i-053f9d4856d92ea3d, 35.94.91.91\n  - daft-demo, worker, i-00c340dc39d54772d, 44.234.112.173\n  - daft-demo, worker, i-042a96ce1413c1dd6, 35.94.206.130\nShutting-down:\n  - new-cluster, head, i-0be0db9803bd06652, 35.86.200.101\n  - new-cluster, worker, i-056f46bd69e1dd3f1, 44.242.166.108\n  - new-cluster, worker, i-09ff0e1d8e67b8451, 35.87.221.180\n</code></pre> <p>In a few seconds later, the state of <code>new-cluster</code> will be finalized to \u201cTerminated\u201d.</p>"},{"location":"install/","title":"Installation","text":"<p>To install Daft, run this from your terminal:</p> <pre><code>pip install -U getdaft\n</code></pre>"},{"location":"install/#extra-dependencies","title":"Extra Dependencies","text":"<p>Some Daft functionality may also require other dependencies, which are specified as \"extras\":</p> <p>To install Daft with the extra dependencies required for interacting with AWS services, such as AWS S3, run:</p> <pre><code>pip install -U getdaft[aws]\n</code></pre> <p>To install Daft with the extra dependencies required for running distributed Daft on top of a Ray cluster, run:</p> <pre><code>pip install -U getdaft[ray]\n</code></pre> <p>To install Daft with all extras, run:</p> <pre><code>pip install -U getdaft[all]\n</code></pre>"},{"location":"install/#advanced-installation","title":"Advanced Installation","text":""},{"location":"install/#installing-nightlies","title":"Installing Nightlies","text":"<p>If you wish to use Daft at the bleeding edge of development, you may also install the nightly build of Daft which is built every night against the <code>main</code> branch:</p> <pre><code>pip install -U getdaft --pre --extra-index-url https://pypi.anaconda.org/daft-nightly/simple\n</code></pre>"},{"location":"install/#installing-daft-from-source","title":"Installing Daft from source","text":"<pre><code>pip install -U https://github.com/Eventual-Inc/Daft/archive/refs/heads/main.zip\n</code></pre> <p>Please note that Daft requires the Rust toolchain in order to build from source.</p>"},{"location":"quickstart/","title":"Quickstart","text":"In\u00a0[\u00a0]: Copied! <pre>pip install getdaft\n</pre> pip install getdaft In\u00a0[18]: Copied! <pre>import daft\n\ndf = daft.from_pydict(\n    {\n        \"A\": [1, 2, 3, 4],\n        \"B\": [1.5, 2.5, 3.5, 4.5],\n        \"C\": [True, True, False, False],\n        \"D\": [None, None, None, None],\n    }\n)\n\ndf\n</pre> import daft  df = daft.from_pydict(     {         \"A\": [1, 2, 3, 4],         \"B\": [1.5, 2.5, 3.5, 4.5],         \"C\": [True, True, False, False],         \"D\": [None, None, None, None],     } )  df Out[18]: AInt64BFloat64CBooleanDNull 11.5trueNone 22.5trueNone 33.5falseNone 44.5falseNone (Showing first 4 of 4 rows) <p>You just created your first DataFrame!</p> In\u00a0[19]: Copied! <pre># Set IO Configurations to use anonymous data access mode\ndaft.set_planning_config(default_io_config=daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True)))\n\ndf = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf\n</pre> # Set IO Configurations to use anonymous data access mode daft.set_planning_config(default_io_config=daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True)))  df = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\") df Out[19]: first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean (No data to display: Dataframe not materialized) <p>Why does it say <code>(No data to display: Dataframe not materialized)</code> and where are the rows?</p> In\u00a0[20]: Copied! <pre>df.collect()\n</pre> df.collect() <pre>                                                              \r</pre> Out[20]: first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean WolfgangWinter232001-02-12GermanyNone ShandraShamas571967-01-02United Kingdomtrue ZayaZaphora401984-04-07United Kingdomtrue ErnestoEvergreen341990-04-03Canadatrue JamesJale621962-03-24Canadatrue (Showing first 5 of 5 rows) <p>To view just the first few rows, you can use the <code>df.show()</code> method:</p> In\u00a0[21]: Copied! <pre>df.show(3)\n</pre> df.show(3) first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean WolfgangWinter232001-02-12GermanyNone ShandraShamas571967-01-02United Kingdomtrue ZayaZaphora401984-04-07United Kingdomtrue (Showing first 3 of 5 rows) <p>Now let's take a look at some common DataFrame operations.</p> In\u00a0[22]: Copied! <pre>df.select(\"first_name\", \"has_dog\").show()\n</pre> df.select(\"first_name\", \"has_dog\").show() first_nameUtf8has_dogBoolean WolfgangNone Shandratrue Zayatrue Ernestotrue Jamestrue (Showing first 5 of 5 rows) In\u00a0[23]: Copied! <pre>df.where(daft.col(\"age\") &gt;= 40).show()\n</pre> df.where(daft.col(\"age\") &gt;= 40).show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean ShandraShamas571967-01-02United Kingdomtrue ZayaZaphora401984-04-07United Kingdomtrue JamesJale621962-03-24Canadatrue (Showing first 3 of 3 rows) <p>Filtering can give you powerful optimization when you are working with partitioned files or tables. Daft will use the predicate to read only the necessary partitions, skipping any data that is not relevant.</p> <p>Note</p> <p>         As mentioned earlier that our Parquet file is partitioned on the <code>country</code> column, this means that queries with a <code>country</code> predicate will benefit from query optimization.     </p> In\u00a0[24]: Copied! <pre>df.limit(1).show()\n</pre> df.limit(1).show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBoolean WolfgangWinter232001-02-12GermanyNone (Showing first 1 of 1 rows) <p>To drop columns from the DataFrame, use the <code>df.exclude()</code> method.</p> In\u00a0[25]: Copied! <pre>df.exclude(\"DoB\").show()\n</pre> df.exclude(\"DoB\").show() first_nameUtf8last_nameUtf8ageInt64countryUtf8has_dogBoolean WolfgangWinter23GermanyNone ShandraShamas57United Kingdomtrue ZayaZaphora40United Kingdomtrue ErnestoEvergreen34Canadatrue JamesJale62Canadatrue (Showing first 5 of 5 rows) In\u00a0[26]: Copied! <pre>df = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\"))\ndf.select(\"full_name\", \"age\", \"country\", \"has_dog\").show()\n</pre> df = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\")) df.select(\"full_name\", \"age\", \"country\", \"has_dog\").show() full_nameUtf8ageInt64countryUtf8has_dogBoolean Wolfgang Winter23GermanyNone Shandra Shamas57United Kingdomtrue Zaya Zaphora40United Kingdomtrue Ernesto Evergreen34Canadatrue James Jale62Canadatrue (Showing first 5 of 5 rows) <p>Alternatively, you can also run your column transformation using Expressions directly inside your <code>df.select()</code> method:</p> In\u00a0[27]: Copied! <pre>df.select((daft.col(\"first_name\").alias(\"full_name\") + \" \" + daft.col(\"last_name\")), \"age\", \"country\", \"has_dog\").show()\n</pre> df.select((daft.col(\"first_name\").alias(\"full_name\") + \" \" + daft.col(\"last_name\")), \"age\", \"country\", \"has_dog\").show() full_nameUtf8ageInt64countryUtf8has_dogBoolean Wolfgang Winter23GermanyNone Shandra Shamas57United Kingdomtrue Zaya Zaphora40United Kingdomtrue Ernesto Evergreen34Canadatrue James Jale62Canadatrue (Showing first 5 of 5 rows) In\u00a0[31]: Copied! <pre>df.sort(daft.col(\"age\"), desc=False).show()\n</pre> df.sort(daft.col(\"age\"), desc=False).show() first_nameUtf8last_nameUtf8ageInt64DoBDatecountryUtf8has_dogBooleanfull_nameUtf8 WolfgangWinter232001-02-12GermanyNoneWolfgang Winter ErnestoEvergreen341990-04-03CanadatrueErnesto Evergreen ZayaZaphora401984-04-07United KingdomtrueZaya Zaphora ShandraShamas571967-01-02United KingdomtrueShandra Shamas JamesJale621962-03-24CanadatrueJames Jale (Showing first 5 of 5 rows) In\u00a0[29]: Copied! <pre>grouped = df.groupby(\"country\").agg(daft.col(\"age\").mean().alias(\"avg_age\"), daft.col(\"has_dog\").count()).show()\n</pre> grouped = df.groupby(\"country\").agg(daft.col(\"age\").mean().alias(\"avg_age\"), daft.col(\"has_dog\").count()).show() countryUtf8avg_ageFloat64has_dogUInt64 Canada482 Germany230 United Kingdom48.52 (Showing first 3 of 3 rows) <p>Note</p> <p>     The <code>df.alias</code> method renames the given column.     </p>"},{"location":"quickstart/#quickstart","title":"Quickstart\u00b6","text":"<p>In this quickstart, you will learn the basics of Daft's DataFrame and SQL API and the features that set it apart from frameworks like Pandas, PySpark, Dask, and Ray.</p> <p>todo(docs): incorporate sql examples</p>"},{"location":"quickstart/#install-daft","title":"Install Daft\u00b6","text":"<p>You can install Daft using <code>pip</code>. Run the following command in your terminal or notebook:</p>"},{"location":"quickstart/#create-your-first-daft-dataframe","title":"Create Your First Daft DataFrame\u00b6","text":"<p>Let's create a DataFrame from a dictionary of columns:</p>"},{"location":"quickstart/#read-from-a-data-source","title":"Read From a Data Source\u00b6","text":"<p>Daft supports both local paths as well as paths to object storage such as AWS S3:</p> <ul> <li>CSV files: <code>daft.read_csv(\"s3://path/to/bucket/*.csv\")</code></li> <li>Parquet files: <code>daft.read_parquet(\"/path/*.parquet\")</code></li> <li>JSON line-delimited files: <code>daft.read_json(\"/path/*.json\")</code></li> <li>Files on disk: <code>daft.from_glob_path(\"/path/*.jpeg\")</code></li> </ul> <p>Note</p> <p>         See Integrations to learn more about working with other formats like Delta Lake and Iceberg.     </p> <p>Let\u2019s read in a Parquet file from a public S3 bucket. Note that this Parquet file is partitioned on the column <code>country</code>. This will be important later on.</p> <p>todo(docs): sql equivalent?</p>"},{"location":"quickstart/#execute-your-dataframe-and-view-data","title":"Execute Your DataFrame and View Data\u00b6","text":"<p>Daft DataFrames are lazy by default. This means that the contents will not be computed (\u201cmaterialized\u201d) unless you explicitly tell Daft to do so. This is best practice for working with larger-than-memory datasets and parallel/distributed architectures.</p> <p>The file we have just loaded only has 5 rows. You can materialize the whole DataFrame in memory easily using the <code>df.collect()</code> method:</p> <p>todo(docs): sql equivalent?</p>"},{"location":"quickstart/#selecting-columns","title":"Selecting Columns\u00b6","text":"<p>You can select specific columns from your DataFrame with the <code>df.select()</code> method.</p> <p>todo(docs): sql equivalent?</p>"},{"location":"quickstart/#selecting-rows","title":"Selecting Rows\u00b6","text":"<p>You can filter rows using the <code>df.where()</code> method that takes an Logical Expression predicate input. In this case, we call the <code>df.col()</code> method that refers to the column with the provided name <code>age</code>:</p>"},{"location":"quickstart/#excluding-data","title":"Excluding Data\u00b6","text":"<p>You can limit the number of rows in a DataFrame by calling the <code>df.limit()</code> method:</p>"},{"location":"quickstart/#transforming-columns-with-expressions","title":"Transforming Columns with Expressions\u00b6","text":"<p>Expressions are an API for defining computation that needs to happen over columns. For example, use the <code>daft.col()</code> expressions together with the <code>with_column</code> method to create a new column called <code>full_name</code>, joining the contents from the <code>last_name</code> column with the <code>first_name</code> column:</p>"},{"location":"quickstart/#sorting-data","title":"Sorting Data\u00b6","text":"<p>You can sort a DataFrame with the <code>df.sort()</code>, in this example we chose to sort in ascending order:</p>"},{"location":"quickstart/#grouping-and-aggregating-data","title":"Grouping and Aggregating Data\u00b6","text":"<p>You can group and aggregate your data using the <code>df.groupby()</code> and the <code>df.agg()</code> methods. A groupby aggregation operation over a dataset happens in 2 steps:</p> <ol> <li>Split the data into groups based on some criteria using <code>df.groupby()</code></li> <li>Specify how to aggregate the data for each group using <code>df.agg()</code></li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>todo(docs): Are there too many sections?</p> <p>todo(docs): Incorporate SQL examples</p> <p>todo(docs): Add link to notebook to DIY (notebook is in docs-v2 dir, but idk how to host on colab).</p> <p>todo(docs): What does the actual output look like for some of these examples?</p> <p>In this quickstart, you will learn the basics of Daft's DataFrame and SQL API and the features that set it apart from frameworks like Pandas, PySpark, Dask, and Ray.</p>"},{"location":"quickstart/#install-daft","title":"Install Daft","text":"<p>You can install Daft using <code>pip</code>. Run the following command in your terminal or notebook:</p> \ud83d\udc0d Python <pre><code>pip install getdaft\n</code></pre> <p>For more advanced installation options, please see Installation.</p>"},{"location":"quickstart/#create-your-first-daft-dataframe","title":"Create Your First Daft DataFrame","text":"<p>todo(docs): Simplify this example, take from \"dataframe\" section, but will they be too similar now?</p> <p>See also DataFrame Creation. Let's create a DataFrame from a dictionary of columns:</p> \ud83d\udc0d Python <pre><code>import daft\n\ndf = daft.from_pydict({\n    \"A\": [1, 2, 3, 4],\n    \"B\": [1.5, 2.5, 3.5, 4.5],\n    \"C\": [True, True, False, False],\n    \"D\": [None, None, None, None],\n})\n\ndf\n</code></pre> Output<pre><code>+-------+---------+---------+------+\n| A     | B       | C       | D    |\n| Int64 | Float64 | Boolean | Null |\n+=======+=========+=========+======+\n| 1     | 1.5     | true    | None |\n+-------+---------+---------+------+\n| 2     | 2.5     | true    | None |\n+-------+---------+---------+------+\n| 3     | 3.5     | false   | None |\n+-------+---------+---------+------+\n| 4     | 4.5     | false   | None |\n+-------+---------+---------+------+\n\n\n(Showing first 4 of 4 rows)\n</code></pre> <p>You just created your first DataFrame!</p>"},{"location":"quickstart/#read-from-a-data-source","title":"Read From a Data Source","text":"<p>Daft supports both local paths as well as paths to object storage such as AWS S3:</p> <ul> <li>CSV files: <code>daft.read_csv(\"s3://path/to/bucket/*.csv\")</code></li> <li>Parquet files: <code>daft.read_parquet(\"/path/*.parquet\")</code></li> <li>JSON line-delimited files: <code>daft.read_json(\"/path/*.json\")</code></li> <li>Files on disk: <code>daft.from_glob_path(\"/path/*.jpeg\")</code></li> </ul> <p>Note</p> <p>See Integrations to learn more about working with other formats like Delta Lake and Iceberg.</p> <p>Let\u2019s read in a Parquet file from a public S3 bucket. Note that this Parquet file is partitioned on the column <code>country</code>. This will be important later on.</p> <p>todo(docs): SQL equivalent?</p> \ud83d\udc0d Python <pre><code># Set IO Configurations to use anonymous data access mode\ndaft.set_planning_config(default_io_config=daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True)))\n\ndf = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf\n</code></pre> Output<pre><code>+------------+-----------+-------+------+---------+---------+\n| first_name | last_name | age   | DoB  | country | has_dog |\n| Utf8       | Utf8      | Int64 | Date | Utf8    | Boolean |\n+------------+-----------+-------+------+---------+---------+\n\n(No data to display: Dataframe not materialized)\n</code></pre> <p>Why does it say <code>(No data to display: Dataframe not materialized)</code> and where are the rows?</p>"},{"location":"quickstart/#execute-your-dataframe-and-view-data","title":"Execute Your DataFrame and View Data","text":"<p>Daft DataFrames are lazy by default. This means that the contents will not be computed (\u201cmaterialized\u201d) unless you explicitly tell Daft to do so. This is best practice for working with larger-than-memory datasets and parallel/distributed architectures.</p> <p>The file we have just loaded only has 5 rows. You can materialize the whole DataFrame in memory easily using the <code>df.collect()</code> method:</p> <p>todo(docs): How does SQL materialize the DataFrame?</p> \ud83d\udc0d Python <pre><code>df.collect()\n</code></pre> Output<pre><code>+------------+-----------+-------+------------+----------------+---------+\n| first_name | last_name | age   | DoB        | country        | has_dog |\n| Utf8       | Utf8      | Int64 | Date       | Utf8           | Boolean |\n+------------+-----------+-------+------------+----------------+---------+\n| Ernesto    | Evergreen | 34    | 1990-04-03 | Canada         | true    |\n| James      | Jale      | 62    | 1962-03-24 | Canada         | true    |\n| Wolfgang   | Winter    | 23    | 2001-02-12 | Germany        | None    |\n| Shandra    | Shamas    | 57    | 1967-01-02 | United Kingdom | true    |\n| Zaya       | Zaphora   | 40    | 1984-04-07 | United Kingdom | true    |\n+------------+-----------+-------+------------+----------------+---------+\n(Showing first 5 of 5 rows)\n</code></pre> <p>To view just the first few rows, you can use the <code>df.show()</code> method:</p> \ud83d\udc0d Python <pre><code>df.show(3)\n</code></pre> Output<pre><code>+------------+-----------+-------+------------+----------------+---------+\n| first_name | last_name | age   | DoB        | country        | has_dog |\n| Utf8       | Utf8      | Int64 | Date       | Utf8           | Boolean |\n+------------+-----------+-------+------------+----------------+---------+\n| Ernesto    | Evergreen | 34    | 1990-04-03 | Canada         | true    |\n| James      | Jale      | 62    | 1962-03-24 | Canada         | true    |\n| Wolfgang   | Winter    | 23    | 2001-02-12 | Germany        | None    |\n+------------+-----------+-------+------------+----------------+---------+\n(Showing first 3 of 5 rows)\n</code></pre> <p>Now let's take a look at some common DataFrame operations.</p>"},{"location":"quickstart/#select-columns","title":"Select Columns","text":"<p>todo(docs): SQL equivalent?</p> <p>You can select specific columns from your DataFrame with the <code>df.select()</code> method:</p> \ud83d\udc0d Python <pre><code>df.select(\"first_name\", \"has_dog\").show()\n</code></pre> Output<pre><code>+------------+---------+\n| first_name | has_dog |\n| Utf8       | Boolean |\n+------------+---------+\n| Ernesto    | true    |\n| James      | true    |\n| Wolfgang   | None    |\n| Shandra    | true    |\n| Zaya       | true    |\n+------------+---------+\n(Showing first 5 of 5 rows)\n</code></pre>"},{"location":"quickstart/#select-rows","title":"Select Rows","text":"<p>You can filter rows using the <code>df.where()</code> method that takes an Logical Expression predicate input. In this case, we call the <code>df.col()</code> method that refers to the column with the provided name <code>age</code>:</p> \ud83d\udc0d Python <pre><code>df.where(daft.col(\"age\") &gt;= 40).show()\n</code></pre> Output<pre><code>+------------+-----------+-------+------------+----------------+---------+\n| first_name | last_name | age   | DoB        | country        | has_dog |\n| Utf8       | Utf8      | Int64 | Date       | Utf8           | Boolean |\n+------------+-----------+-------+------------+----------------+---------+\n| James      | Jale      | 62    | 1962-03-24 | Canada         | true    |\n| Shandra    | Shamas    | 57    | 1967-01-02 | United Kingdom | true    |\n| Zaya       | Zaphora   | 40    | 1984-04-07 | United Kingdom | true    |\n+------------+-----------+-------+------------+----------------+---------+\n(Showing first 3 of 3 rows)\n</code></pre> <p>Filtering can give you powerful optimization when you are working with partitioned files or tables. Daft will use the predicate to read only the necessary partitions, skipping any data that is not relevant.</p> <p>Note</p> <p>As mentioned earlier that our Parquet file is partitioned on the <code>country</code> column, this means that queries with a <code>country</code> predicate will benefit from query optimization.</p>"},{"location":"quickstart/#exclude-data","title":"Exclude Data","text":"<p>You can limit the number of rows in a DataFrame by calling the <code>df.limit()</code> method:</p> \ud83d\udc0d Python <pre><code>df.limit(2).show()\n</code></pre> Output<pre><code>+------------+-----------+-------+------------+----------------+---------+\n| first_name | last_name | age   | DoB        | country        | has_dog |\n| Utf8       | Utf8      | Int64 | Date       | Utf8           | Boolean |\n+------------+-----------+-------+------------+----------------+---------+\n| Ernesto    | Evergreen | 34    | 1990-04-03 | Canada         | true    |\n+------------+-----------+-------+------------+----------------+---------+\n(Showing first 1 of 1 rows)\n</code></pre> <p>To drop columns from the DataFrame, use the <code>df.exclude()</code> method.</p> \ud83d\udc0d Python <pre><code>df.exclude(\"DoB\").show()\n</code></pre> Output<pre><code>+------------+-----------+-------+----------------+---------+\n| first_name | last_name | age   | country        | has_dog |\n| Utf8       | Utf8      | Int64 | Utf8           | Boolean |\n+------------+-----------+-------+----------------+---------+\n| Ernesto    | Evergreen | 34    | Canada         | true    |\n| James      | Jale      | 62    | Canada         | true    |\n| Wolfgang   | Winter    | 23    | Germany        | None    |\n| Shandra    | Shamas    | 57    | United Kingdom | true    |\n| Zaya       | Zaphora   | 40    | United Kingdom | true    |\n+------------+-----------+-------+----------------+---------+\n(Showing first 5 of 5 rows)\n</code></pre>"},{"location":"quickstart/#transform-columns-with-expressions","title":"Transform Columns with Expressions","text":"<p>Expressions are an API for defining computation that needs to happen over columns. For example, use the <code>daft.col()</code> expressions together with the <code>with_column</code> method to create a new column called <code>full_name</code>, joining the contents from the <code>last_name</code> column with the <code>first_name</code> column:</p> \ud83d\udc0d Python <pre><code>df = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\"))\ndf.select(\"full_name\", \"age\", \"country\", \"has_dog\").show()\n</code></pre> Output<pre><code>+-------------------+-------+----------------+---------+\n| full_name         | age   | country        | has_dog |\n| Utf8              | Int64 | Utf8           | Boolean |\n+-------------------+-------+----------------+---------+\n| Ernesto Evergreen | 34    | Canada         | true    |\n| James Jale        | 62    | Canada         | true    |\n| Wolfgang Winter   | 23    | Germany        | None    |\n| Shandra Shamas    | 57    | United Kingdom | true    |\n| Zaya Zaphora      | 40    | United Kingdom | true    |\n+-------------------+-------+----------------+---------+\n(Showing first 5 of 5 rows)\n</code></pre> <p>Alternatively, you can also run your column transformation using Expressions directly inside your <code>df.select()</code> method*:</p> \ud83d\udc0d Python <pre><code>df.select((daft.col(\"first_name\").alias(\"full_name\") + \" \" + daft.col(\"last_name\")), \"age\", \"country\", \"has_dog\").show()\n</code></pre> Output<pre><code>+-------------------+-------+----------------+---------+\n| full_name         | age   | country        | has_dog |\n| Utf8              | Int64 | Utf8           | Boolean |\n+-------------------+-------+----------------+---------+\n| Ernesto Evergreen | 34    | Canada         | true    |\n| James Jale        | 62    | Canada         | true    |\n| Wolfgang Winter   | 23    | Germany        | None    |\n| Shandra Shamas    | 57    | United Kingdom | true    |\n| Zaya Zaphora      | 40    | United Kingdom | true    |\n+-------------------+-------+----------------+---------+\n(Showing first 5 of 5 rows)\n</code></pre>"},{"location":"quickstart/#sort-data","title":"Sort Data","text":"<p>You can sort a DataFrame with the <code>df.sort()</code>, in this example we chose to sort in ascending order:</p> \ud83d\udc0d Python <pre><code>df.sort(daft.col(\"age\"), desc=False).show()\n</code></pre> Output<pre><code>+------------+-----------+-------+------------+----------------+---------+\n| first_name | last_name | age   | DoB        | country        | has_dog |\n| Utf8       | Utf8      | Int64 | Date       | Utf8           | Boolean |\n+------------+-----------+-------+------------+----------------+---------+\n| Wolfgang   | Winter    | 23    | 2001-02-12 | Germany        | None    |\n| Ernesto    | Evergreen | 34    | 1990-04-03 | Canada         | true    |\n| Zaya       | Zaphora   | 40    | 1984-04-07 | United Kingdom | true    |\n| Shandra    | Shamas    | 57    | 1967-01-02 | United Kingdom | true    |\n| James      | Jale      | 62    | 1962-03-24 | Canada         | true    |\n+------------+-----------+-------+------------+----------------+---------+\n(Showing first 5 of 5 rows)\n</code></pre>"},{"location":"quickstart/#group-and-aggregate-data","title":"Group and Aggregate Data","text":"<p>You can group and aggregate your data using the <code>df.groupby()</code> and the <code>df.agg()</code> methods. A groupby aggregation operation over a dataset happens in 2 steps:</p> <ol> <li>Split the data into groups based on some criteria using <code>df.groupby()</code></li> <li>Specify how to aggregate the data for each group using <code>df.agg()</code></li> </ol> \ud83d\udc0d Python <pre><code>grouped = df.groupby(\"country\").agg(\n    daft.col(\"age\").mean().alias(\"avg_age\"),\n    daft.col(\"has_dog\").count()\n).show()\n</code></pre> Output<pre><code>+----------------+---------+---------+\n| country        | avg_age | has_dog |\n| Utf8           | Float64 | UInt64  |\n+----------------+---------+---------+\n| Canada         | 48      | 2       |\n| Germany        | 23      | 0       |\n| United Kingdom | 48.5    | 2       |\n+----------------+---------+---------+\n(Showing first 3 of 3 rows)\n</code></pre> <p>Note</p> <p>The <code>df.alias()</code> method renames the given column.</p>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you have a basic sense of Daft\u2019s functionality and features, here are some more resources to help you get the most out of Daft:</p> <p>Check out the Core Concepts sections for more details about:</p> <ul> <li> DataFrame Operations</li> <li> Expressions</li> <li> Reading Data</li> <li> Writing Data</li> <li> DataTypes</li> <li> SQL</li> <li> Aggregations and Grouping</li> <li> User-Defined Functions (UDFs)</li> <li> Multimodal Data</li> </ul> <p>Work with your favorite tools:</p> <ul> <li> Ray</li> <li>Unity Catalog</li> <li>Apache Iceberg</li> <li>Delta Lake</li> <li> Microsoft Azure</li> <li> Amazon Web Services (AWS)</li> <li>SQL</li> <li> Hugging Face Datasets</li> </ul> <p>Coming from?</p> <ul> <li> Dask Migration Guide</li> </ul> <p>Try your hand at some Tutorials:</p> <ul> <li> MNIST Digit Classification</li> <li> Running LLMs on the Red Pajamas Dataset</li> <li> Querying Images with UDFs</li> <li> Image Generation on GPUs</li> </ul>"},{"location":"terms/","title":"Terminology","text":"<p>todo(docs): For each term, included a link to its respective section under \"Core Concepts\" (except Query Plan doesn't have a section)</p> <p>Daft is a distributed data engine. The main abstraction in Daft is the <code>DataFrame</code>, which conceptually can be thought of as a \"table\" of data with rows and columns.</p> <p>Daft also exposes a <code>SQL</code> interface which interoperates closely with the DataFrame interface, allowing you to express data transformations and queries on your tables as SQL strings.</p> <p></p>"},{"location":"terms/#dataframes","title":"DataFrames","text":"<p>The <code>DataFrame</code> is the core concept in Daft. Think of it as a table with rows and columns, similar to a spreadsheet or a database table. It's designed to handle large amounts of data efficiently.</p> <p>Daft DataFrames are lazy. This means that calling most methods on a DataFrame will not execute that operation immediately - instead, DataFrames expose explicit methods such as <code>daft.DataFrame.show</code> and <code>daft.DataFrame.write_parquet</code> which will actually trigger computation of the DataFrame.</p> <p>Learn more at DataFrame</p>"},{"location":"terms/#expressions","title":"Expressions","text":"<p>An <code>Expression</code> is a fundamental concept in Daft that allows you to define computations on DataFrame columns. They are the building blocks for transforming and manipulating data within your DataFrame and will be your best friend if you are working with Daft primarily using the Python API.</p> <p>Learn more at Expressions</p>"},{"location":"terms/#query-plan","title":"Query Plan","text":"<p>As mentioned earlier, Daft DataFrames are lazy. Under the hood, each DataFrame in Daft is represented by <code>LogicalPlan</code>, a plan of operations that describes how to compute that DataFrame. This plan is called the \"query plan\" and calling methods on the DataFrame actually adds steps to the query plan! When your DataFrame is executed, Daft will read this plan, optimize it to make it run faster and then execute it to compute the requested results.</p> <p>You can examine a logical plan using <code>df.explain()</code>, here's an example:</p> \ud83d\udc0d Python <pre><code>df2 = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\ndf2.where(df[\"country\"] == \"Canada\").explain(show_all=True)\n</code></pre> Output<pre><code>== Unoptimized Logical Plan ==\n\n* Filter: col(country) == lit(\"Canada\")\n|\n* GlobScanOperator\n|   Glob paths = [s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-\n|     partitioned.pq/**]\n|   Coerce int96 timestamp unit = Nanoseconds\n|   IO config = S3 config = { Max connections = 8, Retry initial backoff ms = 1000,\n|     Connect timeout ms = 30000, Read timeout ms = 30000, Max retries = 25, Retry\n|     mode = adaptive, Anonymous = false, Use SSL = true, Verify SSL = true, Check\n|     hostname SSL = true, Requester pays = false, Force Virtual Addressing = false },\n|     Azure config = { Anonymous = false, Use SSL = true }, GCS config = { Anonymous =\n|     false }, HTTP config = { user_agent = daft/0.0.1 }\n|   Use multithreading = true\n|   File schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n|   Partitioning keys = []\n|   Output schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n\n\n== Optimized Logical Plan ==\n\n* GlobScanOperator\n|   Glob paths = [s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-\n|     partitioned.pq/**]\n|   Coerce int96 timestamp unit = Nanoseconds\n|   IO config = S3 config = { Max connections = 8, Retry initial backoff ms = 1000,\n|     Connect timeout ms = 30000, Read timeout ms = 30000, Max retries = 25, Retry\n|     mode = adaptive, Anonymous = false, Use SSL = true, Verify SSL = true, Check\n|     hostname SSL = true, Requester pays = false, Force Virtual Addressing = false },\n|     Azure config = { Anonymous = false, Use SSL = true }, GCS config = { Anonymous =\n|     false }, HTTP config = { user_agent = daft/0.0.1 }\n|   Use multithreading = true\n|   File schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n|   Partitioning keys = []\n|   Filter pushdown = col(country) == lit(\"Canada\")\n|   Output schema = first_name#Utf8, last_name#Utf8, age#Int64, DoB#Date,\n|     country#Utf8, has_dog#Boolean\n\n\n== Physical Plan ==\n\n* TabularScan:\n|   Num Scan Tasks = 1\n|   Estimated Scan Bytes = 6336\n|   Clustering spec = { Num partitions = 1 }\n</code></pre> <p>Learn more at Planning</p>"},{"location":"terms/#structured-query-language-sql","title":"Structured Query Language (SQL)","text":"<p>SQL is a common query language for expressing queries over tables of data. Daft exposes a SQL API as an alternative (but often also complementary API) to the Python <code>DataFrame</code> and <code>Expression</code> APIs for building queries.</p> <p>You can use SQL in Daft via the <code>daft.sql()</code> function, and Daft will also convert many SQL-compatible strings into Expressions via <code>daft.sql_expr()</code> for easy interoperability with DataFrames.</p> <p>Learn more at SQL</p>"},{"location":"advanced/memory/","title":"Managing Memory Usage","text":"<p>Managing memory usage and avoiding out-of-memory (OOM) issues while still maintaining efficient throughput is one of the biggest challenges when building resilient big data processing system!</p> <p>This page is a walkthrough on how Daft handles such situations and possible remedies available to users when you encounter such situations.</p>"},{"location":"advanced/memory/#out-of-core-processing","title":"Out-of-core Processing","text":"<p>Daft supports out-of-core data processing when running on the Ray runner by leveraging Ray's object spilling capabilities.</p> <p>This means that when the total amount of data in Daft gets too large, Daft will spill data onto disk. This slows down the overall workload (because data now needs to be written to and read from disk) but frees up space in working memory for Daft to continue executing work without causing an OOM.</p> <p>You will be alerted when spilling is occurring by log messages that look like this:</p> <pre><code>(raylet, ip=xx.xx.xx.xx) Spilled 16920 MiB, 9 objects, write throughput 576 MiB/s.\n...\n</code></pre> <p>Troubleshooting</p> <p>Spilling to disk is a mechanism that Daft uses to ensure workload completion in an environment where there is insufficient memory, but in some cases this can cause issues.</p> <ol> <li> <p>If your cluster is extremely aggressive with spilling (e.g. spilling hundreds of gigabytes of data) it can be possible that your machine may eventually run out of disk space and be killed by your cloud provider</p> </li> <li> <p>Overly aggressive spilling can also cause your overall workload to be much slower</p> </li> </ol> <p>There are some things you can do that will help with this.</p> <ol> <li> <p>Use machines with more available memory per-CPU to increase each Ray worker's available memory (e.g. AWS EC2 r5 instances</p> </li> <li> <p>Use more machines in your cluster to increase overall cluster memory size</p> </li> <li> <p>Use machines with attached local nvme SSD drives for higher throughput when spilling (e.g. AWS EC2 r5d instances)</p> </li> </ol> <p>For more troubleshooting, you may also wish to consult the Ray documentation's recommendations for object spilling.</p>"},{"location":"advanced/memory/#dealing-with-out-of-memory-oom-errors","title":"Dealing with out-of-memory (OOM) errors","text":"<p>While Daft is built to be extremely memory-efficient, there will inevitably be situations in which it has poorly estimated the amount of memory that it will require for a certain operation, or simply cannot do so (for example when running arbitrary user-defined Python functions).</p> <p>Even with object spilling enabled, you may still sometimes see errors indicating OOMKill behavior on various levels such as your operating system, Ray or a higher-level cluster orchestrator such as Kubernetes:</p> <ol> <li> <p>On the local PyRunner, you may see that your operating system kills the process with an error message <code>OOMKilled</code>.</p> </li> <li> <p>On the RayRunner, you may notice Ray logs indicating that workers are aggressively being killed by the Raylet with log messages such as: <code>Workers (tasks / actors) killed due to memory pressure (OOM)</code></p> </li> <li> <p>If you are running in an environment such as Kubernetes, you may notice that your pods are being killed or restarted with an <code>OOMKill</code> reason</p> </li> </ol> <p>These OOMKills are often recoverable (Daft-on-Ray will take care of retrying work after reviving the workers), however they may often significantly affect the runtime of your workload or if we simply cannot recover, fail the workload entirely.</p> <p>Troubleshooting</p> <p>There are some options available to you.</p> <ol> <li> <p>Use machines with more available memory per-CPU to increase each Ray worker's available memory (e.g. AWS EC2 r5 instances)</p> </li> <li> <p>Use more machines in your cluster to increase overall cluster memory size</p> </li> <li> <p>Aggressively filter your data so that Daft can avoid reading data that it does not have to (e.g. <code>df.where(...)</code>)</p> </li> <li> <p>Request more memory for your UDFs (see Resource Requests if your UDFs are memory intensive (e.g. decompression of data, running large matrix computations etc)</p> </li> <li> <p>Increase the number of partitions in your dataframe (hence making each partition smaller) using something like: <code>df.into_partitions(df.num_partitions() * 2)</code></p> </li> </ol> <p>If your workload continues to experience OOM issues, perhaps Daft could be better estimating the required memory to run certain steps in your workload. Please contact Daft developers on our forums!</p>"},{"location":"advanced/partitioning/","title":"Partitioning","text":"<p>Daft is a distributed dataframe. This means internally, data is represented as partitions which are then spread out across your system.</p>"},{"location":"advanced/partitioning/#why-do-we-need-partitions","title":"Why do we need partitions?","text":"<p>When running in a distributed settings (a cluster of machines), Daft spreads your dataframe's data across these machines. This means that your workload is able to efficiently utilize all the resources in your cluster because each machine is able to work on its assigned partition(s) independently.</p> <p>Additionally, certain global operations in a distributed setting requires data to be partitioned in a specific way for the operation to be correct, because all the data matching a certain criteria needs to be on the same machine and in the same partition. For example, in a groupby-aggregation Daft needs to bring together all the data for a given key into the same partition before it can perform a definitive local groupby-aggregation which is then globally correct. Daft refers to this as a \"clustering specification\", and you are able to see this in the plans that it constructs as well.</p> <p>Note</p> <p>When running locally on just a single machine, Daft is currently still using partitioning as well. This is still useful for controlling parallelism and how much data is being materialized at a time.</p> <p>However, Daft's new experimental execution engine will remove the concept of partitioning entirely for local execution. You may enable it with <code>DAFT_RUNNER=native</code>. Instead of using partitioning to control parallelism, this new execution engine performs a streaming-based execution on small \"morsels\" of data, which provides much more stable memory utilization while improving the user experience with not having to worry about partitioning.</p> <p>This user guide helps you think about how to correctly partition your data to improve performance as well as memory stability in Daft.</p> <p>General rule of thumb:</p> <ol> <li> <p>Have Enough Partitions: our general recommendation for high throughput and maximal resource utilization is to have at least <code>2 x TOTAL_NUM_CPUS</code> partitions, which allows Daft to fully saturate your CPUs.</p> </li> <li> <p>More Partitions: if you are observing memory issues (excessive spilling or out-of-memory (OOM) issues) then you may choose to increase the number of partitions. This increases the amount of overhead in your system, but improves overall memory stability (since each partition will be smaller).</p> </li> <li> <p>Fewer Partitions: if you are observing a large amount of overhead (e.g. if you observe that shuffle operations such as joins and sorts are taking too much time), then you may choose to decrease the number of partitions. This decreases the amount of overhead in the system, at the cost of using more memory (since each partition will be larger).</p> </li> </ol> <p>See Also</p> <p>Managing Memory Usage - a guide for dealing with memory issues when using Daft</p>"},{"location":"advanced/partitioning/#how-is-my-data-partitioned","title":"How is my data partitioned?","text":"<p>Daft will automatically use certain heuristics to determine the number of partitions for you when you create a DataFrame. When reading data from files (e.g. Parquet, CSV or JSON), Daft will group small files/split large files appropriately into nicely-sized partitions based on their estimated in-memory data sizes.</p> <p>To interrogate the partitioning of your current DataFrame, you may use the <code>df.explain(show_all=True)</code> method. Here is an example output from a simple <code>df = daft.read_parquet(...)</code> call on a fairly large number of Parquet files.</p> <p><code>df.explain(show_all=True)</code></p> \ud83d\udc0d Python <pre><code>df = daft.read_parquet(\"s3://bucket/path_to_100_parquet_files/**\")\ndf.explain(show_all=True)\n</code></pre> Output<pre><code>    == Unoptimized Logical Plan ==\n\n    * GlobScanOperator\n    |   Glob paths = [s3://bucket/path_to_100_parquet_files/**]\n    |   ...\n\n\n    ...\n\n\n    == Physical Plan ==\n\n    * TabularScan:\n    |   Num Scan Tasks = 3\n    |   Estimated Scan Bytes = 72000000\n    |   Clustering spec = { Num partitions = 3 }\n    |   ...\n</code></pre> <p>In the above example, the call to <code>df.read_parquet</code> read 100 Parquet files, but the Physical Plan indicates that Daft will only create 3 partitions. This is because these files are quite small (in this example, totalling about 72MB of data) and Daft recognizes that it should be able to read them as just 3 partitions, each with about 33 files each!</p>"},{"location":"advanced/partitioning/#how-can-i-change-the-way-my-data-is-partitioned","title":"How can I change the way my data is partitioned?","text":"<p>You can change the way your data is partitioned by leveraging certain DataFrame methods:</p> <ol> <li> <p><code>daft.DataFrame.repartition</code>: repartitions your data into <code>N</code> partitions by performing a hash-bucketing that ensure that all data with the same values for the specified columns ends up in the same partition. Expensive, requires data movement between partitions and machines.</p> </li> <li> <p><code>daft.DataFrame.into_partitions</code>: splits or coalesces adjacent partitions to meet the specified target number of total partitions. This is less expensive than a call to <code>df.repartition</code> because it does not require shuffling or moving data between partitions.</p> </li> <li> <p>Many global dataframe operations such as <code>daft.DataFrame.join</code>, <code>daft.DataFrame.sort</code> and <code>daft.GroupedDataframe.agg</code> will change the partitioning of your data. This is because they require shuffling data between partitions to be globally correct.</p> </li> </ol> <p>Note that many of these methods will change both the number of partitions as well as the clustering specification of the new partitioning. For example, when calling <code>df.repartition(8, col(\"x\"))</code>, the resultant dataframe will now have 8 partitions in total with the additional guarantee that all rows with the same value of <code>col(\"x\")</code> are in the same partition! This is called \"hash partitioning\".</p> \ud83d\udc0d Python <pre><code>df = df.repartition(8, daft.col(\"x\"))\ndf.explain(show_all=True)\n</code></pre> Output<pre><code>    == Unoptimized Logical Plan ==\n\n    * Repartition: Scheme = Hash\n    |   Num partitions = Some(8)\n    |   By = col(x)\n    |\n    * GlobScanOperator\n    |   Glob paths = [s3://bucket/path_to_1000_parquet_files/**]\n    |   ...\n\n    ...\n\n    == Physical Plan ==\n\n    * ReduceMerge\n    |\n    * FanoutByHash: 8\n    |   Partition by = col(x)\n    |\n    * TabularScan:\n    |   Num Scan Tasks = 3\n    |   Estimated Scan Bytes = 72000000\n    |   Clustering spec = { Num partitions = 3 }\n    |   ...\n</code></pre>"},{"location":"integrations/aws/","title":"Amazon Web Services","text":"<p>Daft is able to read/write data to/from AWS S3, and understands natively the URL protocol <code>s3://</code> as referring to data that resides in S3.</p>"},{"location":"integrations/aws/#authorizationauthentication","title":"Authorization/Authentication","text":"<p>In AWS S3, data is stored under the hierarchy of:</p> <ol> <li>Bucket: The container for data storage, which is the top-level namespace for data storage in S3.</li> <li>Object Key: The unique identifier for a piece of data within a bucket.</li> </ol> <p>URLs to data in S3 come in the form: <code>s3://{BUCKET}/{OBJECT_KEY}</code>.</p>"},{"location":"integrations/aws/#rely-on-environment","title":"Rely on Environment","text":"<p>You can configure the AWS CLI to have Daft automatically discover credentials. Alternatively, you may specify your credentials in environment variables: <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and <code>AWS_SESSION_TOKEN</code>.</p> <p>Please be aware that when doing so in a distributed environment such as Ray, Daft will pick these credentials up from worker machines and thus each worker machine needs to be appropriately provisioned.</p> <p>If instead you wish to have Daft use credentials from the \"driver\", you may wish to manually specify your credentials.</p>"},{"location":"integrations/aws/#manually-specify-credentials","title":"Manually specify credentials","text":"<p>You may also choose to pass these values into your Daft I/O function calls using an <code>daft.io.S3Config</code> config object.</p> <p>todo(docs): add SQL S3Config https://www.getdaft.io/projects/docs/en/stable/api_docs/doc_gen/sql_funcs/daft.sql._sql_funcs.S3Config.html</p> <p><code>daft.set_planning_config</code> is a convenient way to set your <code>daft.io.IOConfig</code> as the default config to use on any subsequent Daft method calls.</p> \ud83d\udc0d Python <pre><code>from daft.io import IOConfig, S3Config\n\n# Supply actual values for the se\nio_config = IOConfig(s3=S3Config(key_id=\"key_id\", session_token=\"session_token\", secret_key=\"secret_key\"))\n\n# Globally set the default IOConfig for any subsequent I/O calls\ndaft.set_planning_config(default_io_config=io_config)\n\n# Perform some I/O operation\ndf = daft.read_parquet(\"s3://my_bucket/my_path/**/*\")\n</code></pre> <p>Alternatively, Daft supports overriding the default IOConfig per-operation by passing it into the <code>io_config=</code> keyword argument. This is extremely flexible as you can pass a different <code>daft.io.S3Config</code> per function call if you wish!</p> \ud83d\udc0d Python <pre><code># Perform some I/O operation but override the IOConfig\ndf2 = daft.read_csv(\"s3://my_bucket/my_other_path/**/*\", io_config=io_config)\n</code></pre>"},{"location":"integrations/azure/","title":"Microsoft Azure","text":"<p>Daft is able to read/write data to/from Azure Blob Store, and understands natively the URL protocols <code>az://</code> and <code>abfs://</code> as referring to data that resides in Azure Blob Store.</p> <p>Warning</p> <p>Daft currently only supports globbing and listing files in storage accounts with hierarchical namespaces enabled. Hierarchical namespaces enable Daft to use its embarrassingly parallel globbing algorithm to improve performance of listing large nested directories of data. Please file an issue if you need support for non-hierarchical namespace buckets! We'd love to support your use-case.</p>"},{"location":"integrations/azure/#authorizationauthentication","title":"Authorization/Authentication","text":"<p>In Azure Blob Service, data is stored under the hierarchy of:</p> <ol> <li>Storage Account</li> <li>Container (sometimes referred to as \"bucket\" in S3-based services)</li> <li>Object Key</li> </ol> <p>URLs to data in Azure Blob Store come in the form: <code>az://{CONTAINER_NAME}/{OBJECT_KEY}</code>.</p> <p>Given that the Storage Account is not a part of the URL, you must provide this separately.</p>"},{"location":"integrations/azure/#rely-on-environment","title":"Rely on Environment","text":"<p>You can rely on Azure's environment variables to have Daft automatically discover credentials.</p> <p>Please be aware that when doing so in a distributed environment such as Ray, Daft will pick these credentials up from worker machines and thus each worker machine needs to be appropriately provisioned.</p> <p>If instead you wish to have Daft use credentials from the \"driver\", you may wish to manually specify your credentials.</p>"},{"location":"integrations/azure/#manually-specify-credentials","title":"Manually specify credentials","text":"<p>You may also choose to pass these values into your Daft I/O function calls using an <code>daft.io.AzureConfig</code> config object.</p> <p><code>daft.set_planning_config</code> is a convenient way to set your <code>daft.io.IOConfig</code> as the default config to use on any subsequent Daft method calls.</p> \ud83d\udc0d Python <pre><code>from daft.io import IOConfig, AzureConfig\n\n# Supply actual values for the storage_account and access key here\nio_config = IOConfig(azure=AzureConfig(storage_account=\"***\", access_key=\"***\"))\n\n# Globally set the default IOConfig for any subsequent I/O calls\ndaft.set_planning_config(default_io_config=io_config)\n\n# Perform some I/O operation\ndf = daft.read_parquet(\"az://my_container/my_path/**/*\")\n</code></pre> <p>Alternatively, Daft supports overriding the default IOConfig per-operation by passing it into the <code>io_config=</code> keyword argument. This is extremely flexible as you can pass a different <code>daft.io.AzureConfig</code> per function call if you wish!</p> \ud83d\udc0d Python <pre><code># Perform some I/O operation but override the IOConfig\ndf2 = daft.read_csv(\"az://my_container/my_other_path/**/*\", io_config=io_config)\n</code></pre>"},{"location":"integrations/azure/#connect-to-microsoft-fabriconelake","title":"Connect to Microsoft Fabric/OneLake","text":"<p>If you are connecting to storage in OneLake or another Microsoft Fabric service, set the <code>use_fabric_endpoint</code> parameter to <code>True</code> in the <code>daft.io.AzureConfig</code> object.</p> \ud83d\udc0d Python <pre><code>from daft.io import IOConfig, AzureConfig\n\nio_config = IOConfig(\n    azure=AzureConfig(\n        storage_account=\"onelake\",\n        use_fabric_endpoint=True,\n\n        # Set credentials as needed\n    )\n)\n\ndf = daft.read_deltalake('abfss://[WORKSPACE]@onelake.dfs.fabric.microsoft.com/[LAKEHOUSE].Lakehouse/Tables/[TABLE]', io_config=io_config)\n</code></pre>"},{"location":"integrations/delta_lake/","title":"Delta Lake","text":"<p>Delta Lake is an open-source storage framework for data analytics on data lakes. It provides ACID transactions, scalable metadata handling, and a unification of streaming and batch data processing, all on top of Parquet files in cloud storage.</p> <p>Daft currently supports:</p> <ol> <li> <p>Parallel + Distributed Reads: Daft parallelizes Delta Lake table reads over all cores of your machine, if using the default multithreading runner, or all cores + machines of your Ray cluster, if using the distributed Ray runner.</p> </li> <li> <p>Skipping Filtered Data: Daft ensures that only data that matches your <code>df.where(...)</code> filter will be read, often skipping entire files/partitions.</p> </li> <li> <p>Multi-cloud Support: Daft supports reading Delta Lake tables from AWS S3, Azure Blob Store, and GCS, as well as local files.</p> </li> </ol>"},{"location":"integrations/delta_lake/#installing-daft-with-delta-lake-support","title":"Installing Daft with Delta Lake Support","text":"<p>Daft internally uses the deltalake Python package to fetch metadata about the Delta Lake table, such as paths to the underlying Parquet files and table statistics. The <code>deltalake</code> package therefore must be installed to read Delta Lake tables with Daft, either manually or with the below <code>getdaft[deltalake]</code> extras install of Daft.</p> <pre><code>pip install -U \"getdaft[deltalake]\"\n</code></pre>"},{"location":"integrations/delta_lake/#reading-a-table","title":"Reading a Table","text":"<p>A Delta Lake table can be read by providing <code>daft.read_deltalake</code> with the URI for your table.</p> <p>The below example uses the deltalake Python package to create a local Delta Lake table for Daft to read, but Daft can also read Delta Lake tables from all of the major cloud stores.</p> \ud83d\udc0d Python <pre><code># Create a local Delta Lake table.\nfrom deltalake import write_deltalake\nimport pandas as pd\n\ndf = pd.DataFrame({\n    \"group\": [1, 1, 2, 2, 3, 3, 4, 4],\n    \"num\": list(range(8)),\n    \"letter\": [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"],\n})\n\n# This will write out separate partitions for group=1, group=2, group=3, group=4.\nwrite_deltalake(\"some-table\", df, partition_by=\"group\")\n</code></pre> <p>After writing this local example table, we can easily read it into a Daft DataFrame.</p> \ud83d\udc0d Python <pre><code># Read Delta Lake table into a Daft DataFrame.\nimport daft\n\ndf = daft.read_deltalake(\"some-table\")\n</code></pre>"},{"location":"integrations/delta_lake/#data-skipping-optimizations","title":"Data Skipping Optimizations","text":"<p>Subsequent filters on the partition column <code>group</code> will efficiently skip data that doesn't match the predicate. In the below example, the <code>group != 2</code> partitions (files) will be pruned, i.e. they will never be read into memory.</p> \ud83d\udc0d Python <pre><code># Filter on partition columns will result in efficient partition pruning; non-matching partitions will be skipped.\ndf2 = df.where(df[\"group\"] == 2)\ndf2.show()\n</code></pre> <p>Filters on non-partition columns will still benefit from automatic file pruning via file-level statistics. In the below example, the <code>group=2</code> partition (file) will have <code>2 &lt;= df[\"num\"] &lt;= 3</code> lower/upper bounds for the <code>num</code> column, and since the filter predicate is <code>df[\"num\"] &lt; 2</code>, Daft will prune the file from the read. Similar is true for <code>group=3</code> and <code>group=4</code> partitions, with none of the data from those files being read into memory.</p> \ud83d\udc0d Python <pre><code># Filter on non-partition column, relying on file-level column stats to efficiently prune unnecessary file reads.\ndf3 = df.where(df[\"num\"] &lt; 2)\ndf3.show()\n</code></pre>"},{"location":"integrations/delta_lake/#write-to-delta-lake","title":"Write to Delta Lake","text":"<p>You can use <code>df.write_deltalake</code> to write a Daft DataFrame to a Delta table:</p> \ud83d\udc0d Python <pre><code>df.write_deltalake(\"tmp/daft-table\", mode=\"overwrite\")\n</code></pre> <p>Daft supports multiple write modes. See the API docs for <code>daft.DataFrame.write_deltalake</code> for more details.</p>"},{"location":"integrations/delta_lake/#type-system","title":"Type System","text":"<p>Daft and Delta Lake have compatible type systems. Here are how types are converted across the two systems.</p> <p>When reading from a Delta Lake table into Daft:</p> Delta Lake Daft Primitive Types <code>boolean</code> <code>daft.DataType.bool()</code> <code>byte</code> <code>daft.DataType.int8()</code> <code>short</code> <code>daft.DataType.int16()</code> <code>int</code> <code>daft.DataType.int32()</code> <code>long</code> <code>daft.DataType.int64()</code> <code>float</code> <code>daft.DataType.float32()</code> <code>double</code> <code>daft.DataType.float64()</code> <code>decimal(precision, scale)</code> <code>daft.DataType.decimal128(precision, scale)</code> <code>date</code> <code>daft.DataType.date()</code> <code>timestamp</code> <code>daft.DataType.timestamp(timeunit=\"us\", timezone=None)</code> <code>timestampz</code> <code>daft.DataType.timestamp(timeunit=\"us\", timezone=\"UTC\")</code> <code>string</code> <code>daft.DataType.string()</code> <code>binary</code> <code>daft.DataType.binary()</code> Nested Types <code>struct(fields)</code> <code>daft.DataType.struct(fields)</code> <code>list(child_type)</code> <code>daft.DataType.list(child_type)</code> <code>map(K, V)</code> <code>daft.DataType.struct({\"key\": K, \"value\": V})</code>"},{"location":"integrations/delta_lake/#roadmap","title":"Roadmap","text":"<p>Here are Delta Lake features that are on our roadmap. Please let us know if you would like to see support for any of these features!</p> <ol> <li> <p>Read support for deletion vectors (issue).</p> </li> <li> <p>Read support for column mappings (issue).</p> </li> <li> <p>Writing new Delta Lake tables (issue).</p> </li> </ol> <p>todo(docs): ^ this needs to be updated, issue is already closed</p> <ol> <li>Writing back to an existing table with appends, overwrites, upserts, or deletes (issue).</li> </ol>"},{"location":"integrations/hudi/","title":"Apache Hudi","text":"<p>Apache Hudi is an open-sourced transactional data lake platform that brings database and data warehouse capabilities to data lakes. Hudi supports transactions, efficient upserts/deletes, advanced indexes, streaming ingestion services, data clustering/compaction optimizations, and concurrency all while keeping your data in open source file formats.</p> <p>Daft currently supports:</p> <ol> <li> <p>Parallel + Distributed Reads: Daft parallelizes Hudi table reads over all cores of your machine, if using the default multithreading runner, or all cores + machines of your Ray cluster, if using the distributed Ray runner.</p> </li> <li> <p>Skipping Filtered Data: Daft ensures that only data that matches your <code>df.where(...)</code> filter will be read, often skipping entire files/partitions.</p> </li> <li> <p>Multi-cloud Support: Daft supports reading Hudi tables from AWS S3, Azure Blob Store, and GCS, as well as local files.</p> </li> </ol>"},{"location":"integrations/hudi/#installing-daft-with-apache-hudi-support","title":"Installing Daft with Apache Hudi Support","text":"<p>Daft supports installing Hudi through optional dependency.</p> <pre><code>pip install -U \"getdaft[hudi]\"\n</code></pre>"},{"location":"integrations/hudi/#reading-a-table","title":"Reading a Table","text":"<p>To read from an Apache Hudi table, use the <code>daft.read_hudi</code> function. The following is an example snippet of loading an example table:</p> \ud83d\udc0d Python <pre><code># Read Apache Hudi table into a Daft DataFrame.\nimport daft\n\ndf = daft.read_hudi(\"some-table-uri\")\ndf = df.where(df[\"foo\"] &gt; 5)\ndf.show()\n</code></pre>"},{"location":"integrations/hudi/#type-system","title":"Type System","text":"<p>Daft and Hudi have compatible type systems. Here are how types are converted across the two systems.</p> <p>When reading from a Hudi table into Daft:</p> Apachi Hudi Daft Primitive Types <code>boolean</code> <code>daft.DataType.bool()</code> <code>byte</code> <code>daft.DataType.int8()</code> <code>short</code> <code>daft.DataType.int16()</code> <code>int</code> <code>daft.DataType.int32()</code> <code>long</code> <code>daft.DataType.int64()</code> <code>float</code> <code>daft.DataType.float32()</code> <code>double</code> <code>daft.DataType.float64()</code> <code>decimal(precision, scale)</code> <code>daft.DataType.decimal128(precision, scale)</code> <code>date</code> <code>daft.DataType.date()</code> <code>timestamp</code> <code>daft.DataType.timestamp(timeunit=\"us\", timezone=None)</code> <code>timestampz</code> <code>daft.DataType.timestamp(timeunit=\"us\", timezone=\"UTC\")</code> <code>string</code> <code>daft.DataType.string()</code> <code>binary</code> <code>daft.DataType.binary()</code> Nested Types <code>struct(fields)</code> <code>daft.DataType.struct(fields)</code> <code>list(child_type)</code> <code>daft.DataType.list(child_type)</code> <code>map(K, V)</code> <code>daft.DataType.struct({\"key\": K, \"value\": V})</code>"},{"location":"integrations/hudi/#roadmap","title":"Roadmap","text":"<p>Currently there are limitations of reading Hudi tables</p> <ul> <li>Only support snapshot read of Copy-on-Write tables</li> <li>Only support reading table version 5 &amp; 6 (tables created using release 0.12.x - 0.15.x)</li> <li>Table must not have <code>hoodie.datasource.write.drop.partition.columns=true</code></li> </ul> <p>Support for more Hudi features are tracked as below:</p> <ol> <li>Support incremental query for Copy-on-Write tables issue).</li> <li>Read support for 1.0 table format (issue).</li> <li>Read support (snapshot) for Merge-on-Read tables (issue).</li> <li>Write support (issue).</li> </ol>"},{"location":"integrations/huggingface/","title":"Hugging Face Datasets","text":"<p>Daft is able to read datasets directly from Hugging Face via the <code>hf://datasets/</code> protocol.</p> <p>Since Hugging Face will automatically convert all public datasets to parquet format, we can read these datasets using the <code>daft.read_parquet()</code> method.</p> <p>Warning</p> <p>This is limited to either public datasets, or PRO/ENTERPRISE datasets.</p> <p>For other file formats, you will need to manually specify the path or glob pattern to the files you want to read, similar to how you would read from a local file system.</p>"},{"location":"integrations/huggingface/#reading-public-datasets","title":"Reading Public Datasets","text":"\ud83d\udc0d Python <pre><code>import daft\n\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name\")\n</code></pre> <p>This will read the entire dataset into a daft DataFrame.</p> <p>Not only can you read entire datasets, but you can also read individual files from a dataset.</p> \ud83d\udc0d Python <pre><code>import daft\n\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name/file_name.parquet\")\n# or a csv file\ndf = daft.read_csv(\"hf://datasets/username/dataset_name/file_name.csv\")\n\n# or a glob pattern\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name/**/*.parquet\")\n</code></pre>"},{"location":"integrations/huggingface/#authorization","title":"Authorization","text":"<p>For authenticated datasets:</p> \ud83d\udc0d Python <pre><code>from daft.io import IOConfig, HTTPConfig\n\nio_config = IoConfig(http=HTTPConfig(bearer_token=\"your_token\"))\ndf = daft.read_parquet(\"hf://datasets/username/dataset_name\", io_config=io_config)\n</code></pre> <p>It's important to note that this will not work with standard tier private datasets. Hugging Face does not auto convert private datasets to parquet format, so you will need to specify the path to the files you want to read.</p> \ud83d\udc0d Python <pre><code>df = daft.read_parquet(\"hf://datasets/username/my_private_dataset\", io_config=io_config) # Errors\n</code></pre> <p>to get around this, you can read all files using a glob pattern (assuming they are in parquet format)</p> \ud83d\udc0d Python <pre><code>df = daft.read_parquet(\"hf://datasets/username/my_private_dataset/**/*.parquet\", io_config=io_config) # Works\n</code></pre>"},{"location":"integrations/iceberg/","title":"Apache Iceberg","text":"<p>Apache Iceberg is an open-sourced table format originally developed at Netflix for large-scale analytical datasets.</p> <p>Daft currently natively supports:</p> <ol> <li>Distributed Reads: Daft will fully distribute the I/O of reads over your compute resources (whether Ray or on multithreading on the local PyRunner)</li> <li>Skipping Filtered Data: Daft uses <code>df.where(...)</code> filter calls to only read data that matches your predicates</li> <li>All Catalogs From PyIceberg: Daft is natively integrated with PyIceberg, and supports all the catalogs that PyIceberg does</li> </ol>"},{"location":"integrations/iceberg/#reading-a-table","title":"Reading a Table","text":"<p>To read from the Apache Iceberg table format, use the <code>daft.read_iceberg</code> function.</p> <p>We integrate closely with PyIceberg (the official Python implementation for Apache Iceberg) and allow the reading of Daft dataframes easily from PyIceberg's Table objects. The following is an example snippet of loading an example table, but for more information please consult the PyIceberg Table loading documentation.</p> \ud83d\udc0d Python <pre><code># Access a PyIceberg table as per normal\nfrom pyiceberg.catalog import load_catalog\n\ncatalog = load_catalog(\"my_iceberg_catalog\")\ntable = catalog.load_table(\"my_namespace.my_table\")\n</code></pre> <p>After a table is loaded as the <code>table</code> object, reading it into a DataFrame is extremely easy.</p> \ud83d\udc0d Python <pre><code># Create a Daft Dataframe\nimport daft\n\ndf = daft.read_iceberg(table)\n</code></pre> <p>Any subsequent filter operations on the Daft <code>df</code> DataFrame object will be correctly optimized to take advantage of Iceberg features such as hidden partitioning and file-level statistics for efficient reads.</p> \ud83d\udc0d Python <pre><code># Filter which takes advantage of partition pruning capabilities of Iceberg\ndf = df.where(df[\"partition_key\"] &lt; 1000)\ndf.show()\n</code></pre>"},{"location":"integrations/iceberg/#writing-to-a-table","title":"Writing to a Table","text":"<p>To write to an Apache Iceberg table, use the <code>daft.DataFrame.write_iceberg</code> method.</p> <p>The following is an example of appending data to an Iceberg table:</p> \ud83d\udc0d Python <pre><code>written_df = df.write_iceberg(table, mode=\"append\")\nwritten_df.show()\n</code></pre> <p>This call will then return a DataFrame containing the operations that were performed on the Iceberg table, like so:</p> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 operation \u2506 rows  \u2506 file_size \u2506 file_name                      \u2502\n\u2502 ---       \u2506 ---   \u2506 ---       \u2506 ---                            \u2502\n\u2502 Utf8      \u2506 Int64 \u2506 Int64     \u2506 Utf8                           \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 ADD       \u2506 5     \u2506 707       \u2506 2f1a2bb1-3e64-49da-accd-1074e\u2026 \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"integrations/iceberg/#type-system","title":"Type System","text":"<p>Daft and Iceberg have compatible type systems. Here are how types are converted across the two systems.</p> <p>When reading from an Iceberg table into Daft:</p> Iceberg Daft Primitive Types <code>boolean</code> <code>daft.DataType.bool()</code> <code>int</code> <code>daft.DataType.int32()</code> <code>long</code> <code>daft.DataType.int64()</code> <code>float</code> <code>daft.DataType.float32()</code> <code>double</code> <code>daft.DataType.float64()</code> <code>decimal(precision, scale)</code> <code>daft.DataType.decimal128(precision, scale)</code> <code>date</code> <code>daft.DataType.date()</code> <code>time</code> <code>daft.DataType.int64()</code> <code>timestamp</code> <code>daft.DataType.timestamp(timeunit=\"us\", timezone=None)</code> <code>timestampz</code> <code>daft.DataType.timestamp(timeunit=\"us\", timezone=\"UTC\")</code> <code>string</code> <code>daft.DataType.string()</code> <code>uuid</code> <code>daft.DataType.binary()</code> <code>fixed(L)</code> <code>daft.DataType.binary()</code> <code>binary</code> <code>daft.DataType.binary()</code> Nested Types <code>struct(fields)</code> <code>daft.DataType.struct(fields)</code> <code>list(child_type)</code> <code>daft.DataType.list(child_type)</code> <code>map(K, V)</code> <code>daft.DataType.struct({\"key\": K, \"value\": V})</code>"},{"location":"integrations/iceberg/#roadmap","title":"Roadmap","text":"<p>Here are some features of Iceberg that are works-in-progress:</p> <ol> <li>Reading Iceberg V2 equality deletes</li> <li>More extensive usage of Iceberg-provided statistics to further optimize queries</li> <li>Copy-on-write and merge-on-read writes</li> </ol> <p>A more detailed Iceberg roadmap for Daft can be found on our Github Issues page.</p>"},{"location":"integrations/ray/","title":"Ray","text":"<p>todo(docs): add reference to daft launcher</p> <p>Ray is an open-source framework for distributed computing. Daft's native support for Ray enables you to run distributed DataFrame workloads at scale.</p>"},{"location":"integrations/ray/#usage","title":"Usage","text":"<p>You can run Daft on Ray in two ways: by using the Ray Client or by submitting a Ray job.</p>"},{"location":"integrations/ray/#ray-client","title":"Ray Client","text":"<p>The Ray client is a quick way to get started with running tasks and retrieving their results on Ray using Python.</p> <p>Warning</p> <p>To run tasks using the Ray client, the version of Daft and the minor version (eg. 3.9, 3.10) of Python must match between client and server.</p> <p>Here's an example of how you can use the Ray client with Daft:</p> \ud83d\udc0d Python <pre><code>import daft\nimport ray\n\n# Refer to the note under \"Ray Job\" for details on \"runtime_env\"\nray.init(\"ray://&lt;head_node_host&gt;:10001\", runtime_env={\"pip\": [\"getdaft\"]})\n\n# Starts the Ray client and tells Daft to use Ray to execute queries\n# If ray.init() has already been called, it uses the existing client\ndaft.context.set_runner_ray(\"ray://&lt;head_node_host&gt;:10001\")\n\ndf = daft.from_pydict({\n    \"a\": [3, 2, 5, 6, 1, 4],\n    \"b\": [True, False, False, True, True, False]\n})\ndf = df.where(df[\"b\"]).sort(df[\"a\"])\n\n# Daft executes the query remotely and returns a preview to the client\ndf.collect()\n</code></pre> Output<pre><code>\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 a     \u2506 b       \u2502\n\u2502 ---   \u2506 ---     \u2502\n\u2502 Int64 \u2506 Boolean \u2502\n\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n\u2502 1     \u2506 true    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 3     \u2506 true    \u2502\n\u251c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u253c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u254c\u2524\n\u2502 6     \u2506 true    \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n(Showing first 3 of 3 rows)\n</code></pre>"},{"location":"integrations/ray/#ray-job","title":"Ray Job","text":"<p>Ray jobs allow for more control and observability over using the Ray client. In addition, your entire code runs on Ray, which means it is not constrained by the compute, network, library versions, or availability of your local machine.</p> \ud83d\udc0d Python <pre><code># wd/job.py\n\nimport daft\n\ndef main():\n    # call without any arguments to connect to Ray from the head node\n    daft.context.set_runner_ray()\n\n    # ... Run Daft commands here ...\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>To submit this script as a job, use the Ray CLI, which can be installed with <code>pip install \"ray[default]\"</code>.</p> <pre><code>ray job submit \\\n    --working-dir wd \\\n    --address \"http://&lt;head_node_host&gt;:8265\" \\\n    --runtime-env-json '{\"pip\": [\"getdaft\"]}' \\\n    -- python job.py\n</code></pre> <p>Note</p> <p>The runtime env parameter specifies that Daft should be installed on the Ray workers. Alternative methods of including Daft in the worker dependencies can be found here.</p> <p>For more information about Ray jobs, see Ray docs -&gt; Ray Jobs Overview.</p>"},{"location":"integrations/sql/","title":"SQL","text":"<p>You can read the results of SQL queries from databases, data warehouses, and query engines, into a Daft DataFrame via the <code>daft.read_sql()</code> function.</p> <p>Daft currently supports:</p> <ol> <li> <p>20+ SQL Dialects: Daft supports over 20 databases, data warehouses, and query engines by using SQLGlot to convert SQL queries across dialects. See the full list of supported dialects here.</p> </li> <li> <p>Parallel + Distributed Reads: Daft parallelizes SQL reads by using all local machine cores with its default multithreading runner, or all cores across multiple machines if using the distributed Ray runner.</p> </li> <li> <p>Skipping Filtered Data: Daft ensures that only data that matches your <code>df.select(...)</code>, <code>df.limit(...)</code>, and <code>df.where(...)</code> expressions will be read, often skipping entire partitions/columns.</p> </li> </ol>"},{"location":"integrations/sql/#installing-daft-with-sql-support","title":"Installing Daft with SQL Support","text":"<p>Install Daft with the <code>getdaft[sql]</code> extra, or manually install the required packages: ConnectorX, SQLAlchemy and SQLGlot.</p> <pre><code>pip install -U \"getdaft[sql]\"\n</code></pre>"},{"location":"integrations/sql/#reading-a-sql-query","title":"Reading a SQL query","text":"<p>To read a SQL query, provide <code>daft.read_sql()</code> with the SQL query and a URL for the data source.</p> <p>The example below creates a local SQLite table for Daft to read.</p> \ud83d\udc0d Python <pre><code>import sqlite3\n\nconnection = sqlite3.connect(\"example.db\")\nconnection.execute(\n    \"CREATE TABLE IF NOT EXISTS books (title TEXT, author TEXT, year INTEGER)\"\n)\nconnection.execute(\n    \"\"\"\nINSERT INTO books (title, author, year)\nVALUES\n    ('The Great Gatsby', 'F. Scott Fitzgerald', 1925),\n    ('To Kill a Mockingbird', 'Harper Lee', 1960),\n    ('1984', 'George Orwell', 1949),\n    ('The Catcher in the Rye', 'J.D. Salinger', 1951)\n\"\"\"\n)\nconnection.commit()\nconnection.close()\n</code></pre> <p>After writing this local example table, we can easily read it into a Daft DataFrame.</p> \ud83d\udc0d Python <pre><code># Read SQL query into Daft DataFrame\nimport daft\n\ndf = daft.read_sql(\n    \"SELECT * FROM books\",\n    \"sqlite://example.db\",\n)\n</code></pre> <p>Daft uses ConnectorX under the hood to read SQL data. ConnectorX is a fast, Rust based SQL connector that reads directly into Arrow Tables, enabling zero-copy transfer into Daft dataframes. If the database is not supported by ConnectorX (list of supported databases here), Daft will fall back to using SQLAlchemy.</p> <p>You can also directly provide a SQL alchemy connection via a connection factory. This way, you have the flexibility to provide additional parameters to the engine.</p> \ud83d\udc0d Python <pre><code># Read SQL query into Daft DataFrame using a connection factory\nimport daft\nfrom sqlalchemy import create_engine\n\ndef create_connection():\n    return sqlalchemy.create_engine(\"sqlite:///example.db\", echo=True).connect()\n\ndf = daft.read_sql(\"SELECT * FROM books\", create_connection)\n</code></pre>"},{"location":"integrations/sql/#parallel-distributed-reads","title":"Parallel + Distributed Reads","text":"<p>For large datasets, Daft can parallelize SQL reads by using all local machine cores with its default multithreading runner, or all cores across multiple machines if using the distributed Ray runner.</p> <p>Supply the <code>daft.read_sql()</code> function with a partition column and optionally the number of partitions to enable parallel reads.</p> \ud83d\udc0d Python <pre><code># Read SQL query into Daft DataFrame with parallel reads\nimport daft\n\ndf = daft.read_sql(\n    \"SELECT * FROM table\",\n    \"sqlite:///big_table.db\",\n    partition_on=\"col\",\n    num_partitions=3,\n)\n</code></pre> <p>Behind the scenes, Daft will partition the data by appending a <code>WHERE col &gt; ... AND col &lt;= ...</code> clause to the SQL query, and then reading each partition in parallel.</p> <p></p>"},{"location":"integrations/sql/#data-skipping-optimizations","title":"Data Skipping Optimizations","text":"<p>Filter, projection, and limit pushdown optimizations can be used to reduce the amount of data read from the database.</p> <p>In the example below, Daft reads the top ranked terms from the BigQuery Google Trends dataset. The <code>where</code> and <code>select</code> expressions in this example will be pushed down into the SQL query itself, we can see this by calling the <code>df.explain()</code> method.</p> \ud83d\udc0d Python <pre><code>import daft, sqlalchemy, datetime\n\ndef create_conn():\n    engine = sqlalchemy.create_engine(\n        \"bigquery://\", credentials_path=\"path/to/service_account_credentials.json\"\n    )\n    return engine.connect()\n\n\ndf = daft.read_sql(\"SELECT * FROM `bigquery-public-data.google_trends.top_terms`\", create_conn)\n\ndf = df.where((df[\"refresh_date\"] &gt;= datetime.date(2024, 4, 1)) &amp; (df[\"refresh_date\"] &lt; datetime.date(2024, 4, 8)))\ndf = df.where(df[\"rank\"] == 1)\ndf = df.select(df[\"refresh_date\"].alias(\"Day\"), df[\"term\"].alias(\"Top Search Term\"), df[\"rank\"])\ndf = df.distinct()\ndf = df.sort(df[\"Day\"], desc=True)\n\ndf.explain(show_all=True)\n</code></pre> Output<pre><code>..\n== Physical Plan ==\n..\n|   SQL Query = SELECT refresh_date, term, rank FROM\n (SELECT * FROM `bigquery-public-data.google_trends.top_terms`)\n AS subquery WHERE rank = 1 AND refresh_date &gt;= CAST('2024-04-01' AS DATE)\n AND refresh_date &lt; CAST('2024-04-08' AS DATE)\n</code></pre> <p>The second last line labeled 'SQL Query =' shows the query that Daft executed. Filters such as <code>rank = 1</code> and projections such as <code>SELECT refresh_date, term, rank</code> have been injected into the query.</p> <p>Without these pushdowns, Daft would execute the unmodified <code>SELECT * FROM 'bigquery-public-data.google_trends.top_terms'</code> query and read in the entire dataset/table. We tested the code above on Google Colab (12GB RAM):</p> <ul> <li>With pushdowns, the code ran in 8.87s with a peak memory of 315.97 MiB</li> <li>Without pushdowns, the code took over 2 mins before crashing with an out of memory error.</li> </ul> <p>You could modify the SQL query to add the filters and projections yourself, but this may become lengthy and error-prone, particularly with many expressions. That's why Daft automatically handles it for you.</p>"},{"location":"integrations/sql/#roadmap","title":"Roadmap","text":"<p>Here are the SQL features that are on our roadmap. Please let us know if you would like to see support for any of these features!</p> <ol> <li>Write support into SQL databases.</li> <li>Reads via ADBC (Arrow Database Connectivity).</li> </ol>"},{"location":"integrations/unity_catalog/","title":"Unity Catalog","text":"<p>Unity Catalog is an open-sourced catalog developed by Databricks. Users of Unity Catalog are able to work with data assets such as tables (Parquet, CSV, Iceberg, Delta), volumes (storing raw files), functions and models.</p> <p>To use Daft with the Unity Catalog, you will need to install Daft with the <code>unity</code> option specified like so:</p> <pre><code>pip install getdaft[unity]\n</code></pre> <p>Warning</p> <p>These APIs are in beta and may be subject to change as the Unity Catalog continues to be developed.</p>"},{"location":"integrations/unity_catalog/#connecting-to-the-unity-catalog","title":"Connecting to the Unity Catalog","text":"<p>Daft includes an abstraction for the Unity Catalog.</p> \ud83d\udc0d Python <pre><code>from daft.unity_catalog import UnityCatalog\n\nunity = UnityCatalog(\n    endpoint=\"https://&lt;databricks_workspace_id&gt;.cloud.databricks.com\",\n    # Authentication can be retrieved from your provider of Unity Catalog\n    token=\"my-token\",\n)\n\n# See all available catalogs\nprint(unity.list_catalogs())\n\n# See available schemas in a given catalog\nprint(unity.list_schemas(\"my_catalog_name\"))\n\n# See available tables in a given schema\nprint(unity.list_tables(\"my_catalog_name.my_schema_name\"))\n</code></pre>"},{"location":"integrations/unity_catalog/#loading-a-daft-dataframe-from-a-delta-lake-table-in-unity-catalog","title":"Loading a Daft Dataframe from a Delta Lake table in Unity Catalog","text":"\ud83d\udc0d Python <pre><code>unity_table = unity.load_table(\"my_catalog_name.my_schema_name.my_table_name\")\n\ndf = daft.read_deltalake(unity_table)\ndf.show()\n</code></pre> <p>Any subsequent filter operations on the Daft <code>df</code> DataFrame object will be correctly optimized to take advantage of DeltaLake features:</p> \ud83d\udc0d Python <pre><code># Filter which takes advantage of partition pruning capabilities of Delta Lake\ndf = df.where(df[\"partition_key\"] &lt; 1000)\ndf.show()\n</code></pre> <p>See also Delta Lake for more information about how to work with the Delta Lake tables provided by the Unity Catalog.</p>"},{"location":"integrations/unity_catalog/#roadmap","title":"Roadmap","text":"<ol> <li> <p>Volumes integration for reading objects from volumes (e.g. images and documents)</p> </li> <li> <p>Unity Iceberg integration for reading tables using the Iceberg interface instead of the Delta Lake interface</p> </li> </ol> <p>Please make issues on the Daft repository if you have any use-cases that Daft does not currently cover!</p>"},{"location":"migration/dask_migration/","title":"Coming from Dask","text":"<p>This migration guide explains the most important points that anyone familiar with Dask should know when trying out Daft or migrating Dask workloads to Daft.</p> <p>The guide includes an overview of technical, conceptual and syntax differences between the two libraries that you should be aware of. Understanding these differences will help you evaluate your choice of tooling and ease your migration from Dask to Daft.</p>"},{"location":"migration/dask_migration/#when-should-i-use-daft","title":"When should I use Daft?","text":"<p>Dask and Daft are DataFrame frameworks built for distributed computing. Both libraries enable you to process large, tabular datasets in parallel, either locally or on remote instances on-prem or in the cloud.</p> <p>If you are currently using Dask, you may want to consider migrating to Daft if you:</p> <ul> <li>Are working with multimodal data types, such as nested JSON, tensors, Images, URLs, etc.,</li> <li>Need faster computations through query planning and optimization,</li> <li>Are executing machine learning workloads at scale,</li> <li>Need deep support for data catalogs, predicate pushdowns and metadata pruning from Iceberg, Delta, and Hudi</li> <li>Want to benefit from native Rust concurrency</li> </ul> <p>You may want to stick with using Dask if you:</p> <ul> <li>Want to only write pandas-like syntax,</li> <li>Need to parallelize array-based workloads or arbitrary Python code that does not involve DataFrames (with Dask Array, Dask Delayed and/or Dask Futures)</li> </ul> <p>The following sections explain conceptual and technical differences between Dask and Daft. Whenever relevant, code snippets are provided to illustrate differences in syntax.</p>"},{"location":"migration/dask_migration/#daft-does-not-use-an-index","title":"Daft does not use an index","text":"<p>Dask aims for as much feature-parity with pandas as possible, including maintaining the presence of an Index in the DataFrame. But keeping an Index is difficult when moving to a distributed computing environment. Dask doesn\u2019t support row-based positional indexing (with .iloc) because it does not track the length of its partitions. It also does not support pandas MultiIndex. The argument for keeping the Index is that it makes some operations against the sorted index column very fast. In reality, resetting the Index forces a data shuffle and is an expensive operation.</p> <p>Daft drops the need for an Index to make queries more readable and consistent. How you write a query should not change because of the state of an index or a reset_index call. In our opinion, eliminating the index makes things simpler, more explicit, more readable and therefore less error-prone. Daft achieves this by using the Expressions API.</p> <p>In Dask you would index your DataFrame to return row <code>b</code> as follows:</p> <pre><code>ddf.loc[[\u201cb\u201d]]\n</code></pre> <p>In Daft, you would accomplish the same by using a <code>col</code> Expression to refer to the column that contains <code>b</code>:</p> <pre><code>df.where(daft.col(\u201calpha\u201d)==\u201db\u201d)\n</code></pre> <p>More about Expressions in the sections below.</p>"},{"location":"migration/dask_migration/#daft-does-not-try-to-copy-the-pandas-syntax","title":"Daft does not try to copy the pandas syntax","text":"<p>Dask is built as a parallelizable version of pandas and Dask partitions are in fact pandas DataFrames. When you call a Dask function you are often applying a pandas function on each partition. This makes Dask relatively easy to learn for people familiar with pandas, but it also causes complications when pandas logic (built for sequential processing) does not translate well to a distributed context. When reading the documentation, Dask users will often encounter this phrase <code>\u201cThis docstring was copied from pandas.core\u2026 Some inconsistencies with the Dask version may exist.\u201d</code> It is often unclear what these inconsistencies are and how they might affect performance.</p> <p>Daft does not try to copy the pandas syntax. Instead, we believe that efficiency is best achieved by defining logic specifically for the unique challenges of distributed computing. This means that we trade a slightly higher learning curve for pandas users against improved performance and more clarity for the developer experience.</p>"},{"location":"migration/dask_migration/#daft-eliminates-manual-repartitioning-of-data","title":"Daft eliminates manual repartitioning of data","text":"<p>In distributed settings, your data will always be partitioned for efficient parallel processing. How to partition this data is not straightforward and depends on factors like data types, query construction, and available cluster resources. While Dask often requires manual repartitioning for optimal performance, Daft abstracts this away from users so you don\u2019t have to worry about it.</p> <p>Dask leaves repartitioning up to the user with guidelines on having partitions that are \u201cnot too large and not too many\u201d. This is hard to interpret, especially given that the optimal partitioning strategy may be different for every query. Instead, Daft automatically controls your partitions in order to execute queries faster and more efficiently. As a side-effect, this means that Daft does not support partition indexing the way Dask does (i.e. \u201cget me partition X\u201d). If things are working well, you shouldn't need to index partitions like this.</p>"},{"location":"migration/dask_migration/#daft-performs-query-optimization-for-optimal-performance","title":"Daft performs Query Optimization for optimal performance","text":"<p>Daft is built with logical query optimization by default. This means that Daft will optimize your queries and skip any files or partitions that are not required for your query. This can give you significant performance gains, especially when working with file formats that support these kinds of optimized queries.</p> <p>Dask currently does not support full-featured query optimization.</p> <p>Note</p> <p>As of version 2024.3.0 Dask is slowly implementing query optimization as well. As far as we can tell this is still in early development and has some rough edges. For context see the discussion in the Dask repo.</p>"},{"location":"migration/dask_migration/#daft-uses-expressions-and-udfs-to-perform-computations-in-parallel","title":"Daft uses Expressions and UDFs to perform computations in parallel","text":"<p>Dask provides a <code>map_partitions</code> method to map computations over the partitions in your DataFrame. Since Dask partitions are pandas DataFrames, you can pass pandas functions to <code>map_partitions</code>. You can also map arbitrary Python functions over Dask partitions using <code>map_partitions</code>.</p> <p>For example:</p> \ud83d\udc0d Python <pre><code>def my_function(**kwargs):\n    return ...\n\nres = ddf.map_partitions(my_function, **kwargs)\n</code></pre> <p>Daft implements two APIs for mapping computations over the data in your DataFrame in parallel: Expressions and User-Defined Functions (UDFs). Expressions are most useful when you need to define computation over your columns.</p> \ud83d\udc0d Python <pre><code># Add 1 to each element in column \"A\"\ndf = df.with_column(\"A_add_one\", daft.col(\"A\") + 1)\n</code></pre> <p>You can use User-Defined Functions (UDFs) to run computations over multiple rows or columns:</p> \ud83d\udc0d Python <pre><code># apply a custom function \u201ccrop_image\u201d to the image column\n@daft.udf(...)\ndef crop_image(**kwargs):\n    return ...\n\ndf = df.with_column(\n    \"cropped\",\n    crop_image(daft.col(\"image\"), **kwargs),\n)\n</code></pre>"},{"location":"migration/dask_migration/#daft-is-built-for-machine-learning-workloads","title":"Daft is built for Machine Learning Workloads","text":"<p>Dask offers some distributed Machine Learning functionality through the dask-ml library. This library provides parallel implementations of a few common scikit-learn algorithms. Note that <code>dask-ml</code> is not a core Dask library and is not as actively maintained. It also does not offer support for deep-learning algorithms or neural networks.</p> <p>Daft is built as a DataFrame API for distributed Machine learning. You can use Daft UDFs to apply Machine Learning tasks to the data stored in your Daft DataFrame, including deep learning algorithms from libraries like PyTorch. See our Quickstart for a toy example.</p>"},{"location":"migration/dask_migration/#daft-supports-multimodal-data-types","title":"Daft supports Multimodal Data Types","text":"<p>Dask supports the same data types as pandas. Daft is built to support many more data types, including Images, nested JSON, tensors, etc. See the documentation for a list of all supported data types.</p>"},{"location":"migration/dask_migration/#distributed-computing-and-remote-clusters","title":"Distributed Computing and Remote Clusters","text":"<p>Both Dask and Daft support distributed computing on remote clusters. In Dask, you create a Dask cluster either locally or remotely and perform computations in parallel there. Currently, Daft supports distributed cluster computing with Ray. Support for running Daft computations on Dask clusters is on the roadmap.</p> <p>Cloud support for both Dask and Daft is the same.</p>"},{"location":"migration/dask_migration/#sql-support","title":"SQL Support","text":"<p>Dask does not natively provide full support for running SQL queries. You can use pandas-like code to write SQL-equivalent queries, or use the external dask-sql library.</p> <p>Daft provides a <code>read_sql()</code> method to read SQL queries into a DataFrame. Daft uses SQLGlot to build SQL queries, so it supports all databases that SQLGlot supports. Daft pushes down operations such as filtering, projections, and limits into the SQL query when possible. Full-featured support for SQL queries (as opposed to a DataFrame API) is in progress.</p>"},{"location":"migration/dask_migration/#daft-combines-python-with-rust-and-pyarrow-for-optimal-performance","title":"Daft combines Python with Rust and Pyarrow for optimal performance","text":"<p>Daft combines Python with Rust and Pyarrow for optimal performance (see Benchmarks). Under the hood, Table and Series are implemented in Rust on top of the Apache Arrow specification (using the Rust arrow2 library). This architecture means that all the computationally expensive operations on Table and Series are performed in Rust, and can be heavily optimized for raw speed. Python is most useful as a user-facing API layer for ease of use and an interactive data science user experience (see Architecture).</p>"},{"location":"resources/architecture/","title":"Architecture","text":"<p>todo(docs): Add information about where Daft fits into the ecosystem or architecture of a system</p>"},{"location":"resources/architecture/#high-level-overview","title":"High Level Overview","text":""},{"location":"resources/architecture/#1-user-api","title":"1. User API","text":"<p>The user-facing API of Daft</p> <p>a. DataFrame: A tabular (rows and columns) Python interface to a distributed table of data. It supports common tabular operations such as filters, joins, column projections and (grouped) aggregations.</p> <p>b. Expressions: A tree data-structure expressing the computation that produces a column of data in a DataFrame. Expressions are built in Rust, but expose a Python API for users to access them from Python.</p>"},{"location":"resources/architecture/#2-planning","title":"2. Planning","text":"<p>Users\u2019 function calls on the User API layer are collected into the Planning layer, which is responsible for optimizing the plan and serializing it into a PhysicalPlan for the Scheduling layer.</p> <p>a. LogicalPlan: When a user calls methods on a DataFrame, these operations are enqueued in a LogicalPlan for delayed execution.</p> <p>b. Optimizer: The Optimizer performs optimizations on LogicalPlans such as predicate pushdowns, column pruning, limit pushdowns and more</p> <p>c. PhysicalPlan: The optimized LogicalPlan is then translated into a PhysicalPlan, which can be polled for tasks to be executed along with other metadata such as resource requirements</p>"},{"location":"resources/architecture/#3-scheduling","title":"3. Scheduling","text":"<p>The scheduling layer is where Daft schedules tasks produced by the PhysicalPlan to be run on the requested backend</p> <p>a. Runner: The Runner consumes tasks produced by the PhysicalPlan. It is responsible for scheduling work on its backend (e.g. local threads or on Ray) and maintaining global distributed state.</p>"},{"location":"resources/architecture/#4-execution","title":"4. Execution","text":"<p>The Execution layer comprises the data-structures that are the actual in-memory representation of the data, and all of the kernels that run on these data-structures.</p> <p>a. Table: Tables are local data-structures with rows/columns built in Rust. It is a high-performance tabular abstraction for fast local execution of work on each partition of data. Tables expose a Python API that is used in the PhysicalPlans.</p> <p>b. Series: Each column in a Table is a Series. Series expose methods which invoke high-performance kernels for manipulation of a column of data. Series are implemented in Rust and expose a Python API only for testing purposes.</p>"},{"location":"resources/architecture/#execution-model","title":"Execution Model","text":"<p>Daft DataFrames are lazy. When operations are called on the DataFrame, their actual execution is delayed. These operations are \u201cenqueued\u201d for execution in a LogicalPlan, which is a tree datastructure which describes the operations that will need to be performed to produce the requested DataFrame.</p> \ud83d\udc0d Python <pre><code>df = daft.read_csv(\"s3://foo/*.csv\")\ndf = df.where(df[\"baz\"] &gt; 0)\n</code></pre> <p>When the Dataframe is executed, a few things will happen:</p> <ol> <li>The LogicalPlan is optimized by a query optimizer</li> <li>The optimized LogicalPlan is translated into a PhysicalPlan</li> <li>The Runner executes the PhysicalPlan by pulling tasks from it</li> </ol> <p></p> <p>These modules can also be understood as:</p> <ol> <li>LogicalPlan: what to run</li> <li>PhysicalPlan: how to run it</li> <li>Runner: when and where to run it</li> </ol> <p>By default, Daft runs on the PyRunner which uses Python multithreading as its backend. Daft also includes other runners including the RayRunner which can run the PhysicalPlan on a distributed Ray cluster.</p>"},{"location":"resources/architecture/#dataframe-partitioning","title":"DataFrame Partitioning","text":"<p>Daft DataFrames are Partitioned - meaning that under the hood they are split row-wise into Partitions of data.</p> <p>This is useful for a few reasons:</p> <ol> <li>Parallelization: each partition of data can be processed independently of other partitions, allowing parallelization of work across all available compute resources.</li> <li>Distributed Computing: each partition can reside in a different machine, unlocking DataFrames that can span terabytes of data</li> <li>Pipelining: different operations may require different resources (some operations can be I/O-bound, while others may be compute-bound). By chunking up the data into Partitions, Daft can effectively pipeline these operations during scheduling to maximize resource utilization.</li> <li>Memory pressure: by processing one partition at a time, Daft can limit the amount of memory it needs to execute and possibly spill result partitions to disk if necessary, freeing up memory that it needs for execution.</li> <li>Optimizations: by understanding the PartitionSpec (invariants around the data inside each partition), Daft can make intelligent decisions to avoid unnecessary data movement for certain operations that may otherwise require a global shuffle of data.</li> </ol> <p>Partitioning is most often inherited from the data source that Daft is reading from. For example, if read from a directory of files, each file naturally is read as a single partition. If reading from a data catalog service such as Apache Iceberg or Delta Lake, Daft will inherit the partitioning scheme as informed by these services.</p> <p>When querying a DataFrame, global operations will also require a repartitioning of the data, depending on the operation. For instance, sorting a DataFrame on <code>col(x)</code> will require repartitioning by range on <code>col(x)</code>, so that a local sort on each partition will provide a globally sorted DataFrame.</p>"},{"location":"resources/architecture/#in-memory-data-representation","title":"In-Memory Data Representation","text":"<p>Each Partition of a DataFrame is represented as a Table object, which is in turn composed of Columns which are Series objects.</p> <p>Under the hood, Table and Series are implemented in Rust on top of the Apache Arrow specification (using the Rust arrow2 library). We expose Python API bindings for Table using PyO3, which allows our PhysicalPlan to define operations that should be run on each Table.</p> <p>This architecture means that all the computationally expensive operations on Table and Series are performed in Rust, and can be heavily optimized for raw speed. Python is most useful as a user-facing API layer for ease of use and an interactive data science user experience.</p>"},{"location":"resources/dataframe_comparison/","title":"Dataframe Comparison","text":"<p>A Dataframe can be thought of conceptually as a \"table of data\", with rows and columns. If you are familiar with Pandas or Spark Dataframes, you will be right at home with Daft! Dataframes are used for:</p> <ul> <li>Interactive Data Science: Performing interactive and ad-hoc exploration of data in a Notebook environment</li> <li>Extract/Transform/Load (ETL): Defining data pipelines that clean and process data for consumption by other users</li> <li>Data Analytics: Analyzing data by producing summaries and reports</li> </ul> <p>Daft Dataframe focuses on Machine Learning/Deep Learning workloads that often involve Complex media data (images, video, audio, text documents and more).</p> <p>Below we discuss some other Dataframe libraries and compare them to Daft.</p> Dataframe Query Optimizer Multimodal Distributed Arrow Backed Vectorized Execution Engine Out-of-Core Daft \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 Pandas \u274c Python object \u274c optional &gt;= 2.0 some (Numpy) \u274c Polars \u2705 Python object \u274c \u2705 \u2705 \u2705 Modin Eagar Python object \u2705 \u274c some (Pandas) \u2705 PySpark \u2705 \u274c \u2705 Pandas UDF/IO Pandas UDF \u2705 Dask \u274c Python object \u2705 \u274c some (Pandas) \u2705"},{"location":"resources/dataframe_comparison/#pandasmodin","title":"Pandas/Modin","text":"<p>The main drawback of using Pandas is scalability. Pandas is single-threaded and not built for distributed computing. While this is not as much of a problem for purely tabular datasets, when dealing with data such as images/video your data can get very large and expensive to compute very quickly.</p> <p>Modin is a project that provides \"distributed Pandas\". If the use-case is tabular, has code that is already written in Pandas but just needs to be scaled up to larger data, Modin may be a good choice. Modin aims to be 100% Pandas API compatible which means that certain operations that are important for performance in the world of multimodal data such as requesting for certain amount of resources (e.g. GPUs) is not yet possible.</p>"},{"location":"resources/dataframe_comparison/#spark-dataframes","title":"Spark Dataframes","text":"<p>Spark Dataframes are the modern enterprise de-facto solution for many ETL (Extract-Load-Transform) jobs. Originally written for Scala, a Python wrapper library called PySpark exists for Python compatibility which allows for some flexibility in leveraging the Python ecosystem for data processing.</p> <p>Spark excels at large scale tabular analytics, with support for running Python code using Pandas UDFs, but suffer from a few key issues.</p> <ul> <li>Serialization overhead: Spark itself is run on the JVM, and the PySpark wrapper runs a Python subprocess to execute Python code. This means means that running Python code always involves copying data back and forth between the Python and the Spark process. This is somewhat alleviated with Arrow as the intermediate serialization format but generally without the correct configurations and expert tuning this can be very slow.</li> <li>Development experience: Spark is not written for Python as a first-class citizen, which means that development velocity is often very slow when users need to run Python code for machine learning, image processing and more.</li> <li> <p>Typing: Python is dynamically typed, but programming in Spark requires jumping through various hoops to be compatible with Spark's strongly typed data model. For example to pass a 2D Numpy array between Spark functions, users have to:</p> <ol> <li>Store the shape and flatten the array</li> <li>Tell Spark exactly what type the array is</li> <li>Unravel the flattened array again on the other end</li> </ol> </li> <li> <p>Debugging: Key features such as exposing print statements or breakpoints from user-defined functions to the user are missing, which make PySpark extremely difficult to develop on.</p> </li> <li>Lack of granular execution control: with heavy processing of multimodal data, users often need more control around the execution and scheduling of their work. For example, users may need to ensure that Spark runs a single executor per GPU, but Spark's programming model makes this very difficult.</li> <li>Compatibility with downstream Machine Learning tasks: Spark itself is not well suited for performing distributed ML training which is increasingly becoming the domain of frameworks such as Ray and Horovod. Integrating with such a solution is difficult and requires expert tuning of intermediate storage and data engineering solutions.</li> </ul>"},{"location":"resources/dataframe_comparison/#dask-dataframes","title":"Dask Dataframes","text":"<p>Dask and Daft are both DataFrame frameworks built for distributed computing. Both libraries enable you to process large, tabular datasets in parallel, either locally or on remote instances on-prem or in the cloud.</p> <p>If you are currently using Dask, you may want to consider migrating to Daft if you:</p> <ul> <li>Are working with multimodal data types, such as nested JSON, tensors, Images, URLs, etc.,</li> <li>Need faster computations through query planning and optimization,</li> <li>Are executing machine learning workloads at scale,</li> <li>Need deep support for data catalogs, predicate pushdowns and metadata pruning from Iceberg, Delta, and Hudi</li> <li>Want to benefit from native Rust concurrency</li> </ul> <p>You may want to stick with using Dask if you:</p> <ul> <li>Want to only write pandas-like syntax,</li> <li>Need to parallelize array-based workloads or arbitrary Python code that does not involve DataFrames (with Dask Array, Dask Delayed and/or Dask Futures)</li> </ul> <p>Read more detailed comparisons in the Dask Migration Guide.</p>"},{"location":"resources/dataframe_comparison/#ray-datasets","title":"Ray Datasets","text":"<p>Ray Datasets make it easy to feed data really efficiently into Ray's model training and inference ecosystem. Datasets also provide basic functionality for data preprocessing such as mapping a function over each data item, filtering data etc.</p> <p>However, Ray Datasets are not a fully-fledged Dataframe abstraction (and it is explicit in not being an ETL framework for data science) which means that it lacks key features in data querying, visualization and aggregations.</p> <p>Instead, Ray Data is a perfect destination for processed data from DaFt Dataframes to be sent to with a simple <code>df.to_ray_dataset()</code> call. This is useful as an entrypoint into your model training and inference ecosystem!</p>"},{"location":"resources/telemetry/","title":"Telemetry","text":"<p>To help core developers improve Daft, we collect non-identifiable statistics on Daft usage in order to better understand how Daft is used, common bugs and performance bottlenecks.</p> <p>We take the privacy of our users extremely seriously, and telemetry in Daft is built to be:</p> <ol> <li>Easy to opt-out: to disable telemetry, set the following environment variable: <code>DAFT_ANALYTICS_ENABLED=0</code></li> <li>Non-identifiable: events are keyed by a session ID which is generated on import of Daft</li> <li>Metadata-only: we do not collect any of our users' proprietary code or data</li> </ol> <p>We do not sell or buy any of the data that is collected in telemetry.</p> <p>Daft telemetry is enabled in versions &gt;= v0.0.21</p>"},{"location":"resources/telemetry/#what-data-do-we-collect","title":"What data do we collect?","text":"<p>To audit what data is collected, please see the implementation of <code>AnalyticsClient</code> in the <code>daft.analytics</code> module.</p> <p>In short, we collect the following:</p> <ol> <li>On import, we track system information such as the runner being used, version of Daft, OS, Python version, etc.</li> <li>On calls of public methods on the DataFrame object, we track metadata about the execution: the name of the method, the walltime for execution and the class of error raised (if any). Function parameters and stacktraces are not logged, ensuring that user data remains private.</li> </ol>"},{"location":"resources/tutorials/","title":"Tutorials","text":""},{"location":"resources/tutorials/#mnist-digit-classification","title":"MNIST Digit Classification","text":"<p>Load the MNIST image dataset and use a simple deep learning model to run classification on each image. Evaluate the model's performance with simple aggregations.</p> <p>Run this tutorial on Google Colab</p>"},{"location":"resources/tutorials/#running-llms-on-the-red-pajamas-dataset","title":"Running LLMs on the Red Pajamas Dataset","text":"<p>Load the Red Pajamas dataset and perform similarity search on Stack Exchange questions using language models and embeddings.</p> <p>Run this tutorial on Google Colab</p>"},{"location":"resources/tutorials/#querying-images-with-udfs","title":"Querying Images with UDFs","text":"<p>Query the Open Images dataset to retrieve the top N \"reddest\" images. This tutorial uses common open-source tools such as numpy and Pillow inside Daft UDFs to execute this query.</p> <p>Run this tutorial on Google Colab</p>"},{"location":"resources/tutorials/#image-generation-on-gpus","title":"Image Generation on GPUs","text":"<p>Generate images from text prompts using a deep learning model (Mini DALL-E) and Daft UDFs. Run Daft UDFs on GPUs for more efficient resource allocation.</p> <p>Run this tutorial on Google Colab</p>"},{"location":"resources/benchmarks/tpch/","title":"TPC-H Benchmarks","text":"<p>Here we compare Daft against some popular Distributed Dataframes such as Spark, Modin, and Dask on the TPC-H benchmark. Our goal for this benchmark is to demonstrate that Daft is able to meet the following development goals:</p> <ol> <li>Solid out of the box performance: great performance without having to tune esoteric flags or configurations specific to this workload</li> <li>Reliable out-of-core execution: highly performant and reliable processing on larger-than-memory datasets, without developer intervention and Out-Of-Memory (OOM) errors</li> <li>Ease of use: getting up and running should be easy on cloud infrastructure for an individual developer or in an enterprise cloud setting</li> </ol> <p>A great stress test for Daft is the TPC-H benchmark, which is a standard benchmark for analytical query engines. This benchmark helps ensure that while Daft makes it very easy to work with multimodal data, it can also do a great job at larger scales (terabytes) of more traditional tabular analytical workloads.</p>"},{"location":"resources/benchmarks/tpch/#setup","title":"Setup","text":"<p>The basic setup for our benchmarks are as follows:</p> <ol> <li>We run questions 1 to 10 of the TPC-H benchmarks using Daft and other commonly used Python Distributed Dataframes.</li> <li>The data for the queries are stored and retrieved from AWS S3 as partitioned Apache Parquet files, which is typical of enterprise workloads. No on disk/in-memory caching was performed.</li> <li>We run each framework on a cluster of AWS i3.2xlarge instances that each have:<ul> <li>8 vCPUs</li> <li>61G of memory</li> <li>1900G of NVMe SSD space</li> </ul> </li> </ol> <p>The frameworks that we benchmark against are Spark, Modin, and Dask. We chose these comparable Dataframes as they are the most commonly referenced frameworks for running large scale distributed analytical queries in Python.</p> <p>For benchmarking against Spark, we use AWS EMR which is a hosted Spark service. For other benchmarks, we host our own Ray and Dask clusters on Kubernetes. Please refer to the section on our Detailed Benchmarking Setup for additional information.</p>"},{"location":"resources/benchmarks/tpch/#results","title":"Results","text":"<p>Highlights</p> <ol> <li>Out of all the benchmarked frameworks, only Daft and EMR Spark are able to run terabyte scale queries reliably on out-of-the-box configurations.</li> <li>Daft is consistently much faster (3.3x faster than EMR Spark, 7.7x faster than Dask Dataframes, and 44.4x faster than Modin).</li> </ol> <p>Note</p> <p>We were unable to obtain full results for Modin due to cluster OOMs, errors and timeouts (one hour limit per question attempt). Similarly, Dask was unable to provide comparable results for the terabyte scale benchmark. It is possible that these frameworks may perform and function better with additional tuning and configuration. Logs for all the runs are provided in a public AWS S3 bucket.</p>"},{"location":"resources/benchmarks/tpch/#100-scale-factor","title":"100 Scale Factor","text":"<p>First we run TPC-H 100 Scale Factor (around 100GB) benchmark  on 4 i3.2xlarge worker instances. In total, these instances add up to 244GB of cluster memory which will require the Dataframe library to perform disk spilling and out-of-core processing for certain questions that have a large join or sort.</p> Dataframe Questions Completed Total Time (seconds) Relative to Daft Daft 10/10 785 1.0x Spark 10/10 2648 3.3x Dask 10/10 6010 7.7x Modin 5/10 Did not finish 44.4x* <p>* Only for queries that completed.</p> <p>From the results we see that Daft, Spark, and Dask are able to complete all the questions and Modin completes less than half. We also see that Daft is 3.3x faster than Spark and 7.7x faster than Dask including S3 IO. We expect these speed-ups to be much larger if the data is loaded in memory instead of cloud storage, which we will show in future benchmarks.</p>"},{"location":"resources/benchmarks/tpch/#1000-scale-factor","title":"1000 Scale Factor","text":"<p>Next we scale up the data size by 10x while keeping the cluster size the same. Since we only have 244GB of memory and 1TB+ of tabular data, the DataFrame library will be required to perform disk spilling and out-of-core processing for all questions at nearly all stages of the query.</p> Dataframe Questions Completed Total Time (seconds) Relative to Daft Daft 10/10 7774 1.0x Spark 10/10 27161 3.5x Dask 3/10 Did not finish 5.8x* Modin 0/10 Did not finish No data <p>* Only for queries that completed.</p> <p>From the results we see that only Daft and Spark are able to complete all the questions. Dask completes less than a third and Modin is unable to complete any due to OOMs and cluster crashes. Since we can only compare to Spark here, we see that Daft is 3.5x faster including S3 IO. This shows that Daft and Spark are the only Dataframes in this comparison capable of processing data larger than memory, with Daft standing out as the significantly faster option.</p>"},{"location":"resources/benchmarks/tpch/#1000-scale-factor-node-count-ablation","title":"1000 Scale Factor - Node Count Ablation","text":"<p>Finally, we compare how Daft performs on varying size clusters on the terabyte scale dataset. We run the same Daft TPC-H questions on the same dataset as the previous section but sweep the worker node count.</p> <p>We note two interesting results here:</p> <ol> <li>Daft can process 1TB+ of analytical data on a single 61GB instance without being distributed (16x more data than memory).</li> <li>Daft query times scale linearly with the number of nodes (e.g. 4 nodes being 4 times faster than a single node). This allows for faster queries while maintaining the same compute cost!</li> </ol>"},{"location":"resources/benchmarks/tpch/#detailed-benchmarking-setup","title":"Detailed Benchmarking Setup","text":""},{"location":"resources/benchmarks/tpch/#benchmarking-code","title":"Benchmarking Code","text":"<p>Our benchmarking scripts and code can be found in the distributed-query-benchmarks GitHub repository.</p> <ul> <li>TPC-H queries for Daft were written by us.</li> <li>TPC-H queries for SparkSQL was adapted from this repository.</li> <li>TPC-H queries for Dask and Modin were adapted from these repositories for questions Q1-7 and Q8-10.</li> </ul>"},{"location":"resources/benchmarks/tpch/#infrastructure","title":"Infrastructure","text":"<p>Our infrastructure runs on an EKS Kubernetes cluster.</p> <ul> <li>Driver Instance: i3.2xlarge</li> <li>Worker Instance: i3.2xlarge</li> <li>Number of Workers: 1/4/8</li> <li>Networking: All instances colocated in the same Availability Zone in the AWS us-west-2 region</li> </ul>"},{"location":"resources/benchmarks/tpch/#data","title":"Data","text":"<p>Data for the benchmark was stored in AWS S3. No node-level caching was performed, and data is read directly from AWS S3 on every attempt to simulate realistic workloads.</p> <ul> <li>Storage: AWS S3 Bucket</li> <li>Format: Parquet</li> <li>Region: us-west-2</li> <li>File Layout: Each table is split into 32 (for the 100SF benchmark) or 512 (for the 1000SF benchmark) separate Parquet files. Parquet files for a given table have their paths prefixed with that table\u2019s name, and are laid out in a flat folder structure under that prefix. Frameworks are instructed to read Parquet files from that prefix.</li> <li>Data Generation: TPC-H data was generated using the utilities found in the open-sourced Daft repository. This data is also available on request if you wish to reproduce any results!</li> </ul>"},{"location":"resources/benchmarks/tpch/#cluster-setup","title":"Cluster Setup","text":""},{"location":"resources/benchmarks/tpch/#dask-and-ray","title":"Dask and Ray","text":"<p>To help us run the Distributed Dataframe libraries, we used Kubernetes for deploying Dask and Ray clusters. The configuration files for these setups can be found in our open source benchmarking repository.</p> <p>Our benchmarks for Daft and Modin were run on a KubeRay cluster, and our benchmarks for Dask was run on a Dask-on-Kubernetes cluster. Both projects are owned and maintained officially by the creators of these libraries as one of the main methods of deploying.</p>"},{"location":"resources/benchmarks/tpch/#spark","title":"Spark","text":"<p>For benchmarking Spark we used AWS EMR, the official managed Spark solution provided by AWS. For more details on our setup and approach, please consult our Spark benchmarks README.</p>"},{"location":"resources/benchmarks/tpch/#logs","title":"Logs","text":"Dataframe Scale Factor Nodes Links Daft 1000 8 1. s3://daft-public-data/benchmarking/logs/daft.0_1_3.1tb.8-i32xlarge.log Daft 1000 4 1. s3://daft-public-data/benchmarking/logs/daft.0_1_3.1tb.4-i32xlarge.log Daft 1000 1 1. s3://daft-public-data/benchmarking/logs/daft.1tb.1.i3-2xlarge.part1.log  2. s3://daft-public-data/benchmarking/logs/daft.1tb.1.i3-2xlarge.part2.log Daft 100 4 1. s3://daft-public-data/benchmarking/logs/daft.0_1_3.100gb.4-i32xlarge.log Spark 1000 4 1. s3://daft-public-data/benchmarking/logs/emr-spark.6_10_0.1tb.4-i32xlarge.log Spark 100 4 1. s3://daft-public-data/benchmarking/logs/emr-spark.6_10_0.100gb.4-i32xlarge.log.gz Dask (failed, multiple retries) 1000 16 1. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.1tb.16-i32xlarge.0.log  2. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.1tb.16-i32xlarge.1.log  3. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.1tb.16-i32xlarge.2.log  4. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.1tb.16-i32xlarge.3.log Dask (failed, multiple retries) 1000 4 1. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.1tb.4-i32xlarge.q126.log Dask (multiple retries) 100 4 1. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.100gb.4-i32xlarge.0.log  2. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.100gb.4-i32xlarge.0.log  3. s3://daft-public-data/benchmarking/logs/dask.2023_5_0.100gb.4-i32xlarge.1.log Modin (failed, multiple retries) 1000 16 1. s3://daft-public-data/benchmarking/logs/modin.0_20_1.1tb.16-i32xlarge.0.log  2. s3://daft-public-data/benchmarking/logs/modin.0_20_1.1tb.16-i32xlarge.1.log Modin (failed, multiple retries) 100 4 1. s3://daft-public-data/benchmarking/logs/modin.0_20_1.100gb.4-i32xlarge.log"}]}