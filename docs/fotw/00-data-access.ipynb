{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e823b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install daft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4804cd5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "CI = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75f3566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this notebook execution in CI because it hits data in a relative path\n",
    "if CI:\n",
    "    import sys\n",
    "\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c8e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install daft\n",
    "!uv pip install validators matplotlib Pillow torch torchvision\n",
    "\n",
    "\n",
    "import daft\n",
    "\n",
    "# Read parquet file containing sample dog owners\n",
    "df = daft.read_parquet(\"s3://daft-public-data/tutorials/10-min/sample-data-dog-owners-partitioned.pq/**\")\n",
    "\n",
    "# Combine \"first_name\" and \"last_name\" to create new column \"full_name\"\n",
    "df = df.with_column(\"full_name\", daft.col(\"first_name\") + \" \" + daft.col(\"last_name\"))\n",
    "df.select(\"full_name\", \"age\", \"country\", \"has_dog\").show()\n",
    "\n",
    "# Create dataframe of dogs\n",
    "df_dogs = daft.from_pydict(\n",
    "    {\n",
    "        \"urls\": [\n",
    "            \"https://live.staticflickr.com/65535/53671838774_03ba68d203_o.jpg\",\n",
    "            \"https://live.staticflickr.com/65535/53671700073_2c9441422e_o.jpg\",\n",
    "            \"https://live.staticflickr.com/65535/53670606332_1ea5f2ce68_o.jpg\",\n",
    "            \"https://live.staticflickr.com/65535/53671838039_b97411a441_o.jpg\",\n",
    "            \"https://live.staticflickr.com/65535/53671698613_0230f8af3c_o.jpg\",\n",
    "        ],\n",
    "        \"full_name\": [\n",
    "            \"Ernesto Evergreen\",\n",
    "            \"James Jale\",\n",
    "            \"Wolfgang Winter\",\n",
    "            \"Shandra Shamas\",\n",
    "            \"Zaya Zaphora\",\n",
    "        ],\n",
    "        \"dog_name\": [\"Ernie\", \"Jackie\", \"Wolfie\", \"Shaggie\", \"Zadie\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "# Join owners with dogs\n",
    "df_family = df.join(df_dogs, on=\"full_name\").exclude(\"first_name\", \"last_name\", \"DoB\", \"country\", \"age\")\n",
    "\n",
    "df_family = df_family.with_column(\"image_bytes\", df_dogs[\"urls\"].url.download(on_error=\"null\"))\n",
    "\n",
    "df_family_decpde = df_family.with_column(\"image\", df_family[\"image_bytes\"].image.decode())\n",
    "\n",
    "\n",
    "df_family.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f0369-4304-416d-9db9-d2e191be96c1",
   "metadata": {},
   "source": [
    "# #00 - Data Access\n",
    "\n",
    "This Feature-of-the-Week tutorial shows the canonical way of accessing data with Daft. \n",
    "\n",
    "Daft reads from 3 main data sources:\n",
    "1. Files (local and remote)\n",
    "2. SQL Databases\n",
    "3. Data Catalogs\n",
    "\n",
    "Let's dive into each type of data access in more detail ðŸª‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ccf99f-de05-4ec2-b4d0-ce2e6be57b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5dea95-1da5-401a-a6c9-9444b065e82a",
   "metadata": {},
   "source": [
    "## Files\n",
    "\n",
    "You can read many different file types with Daft.\n",
    "\n",
    "The most common file formats are:\n",
    "- CSV\n",
    "- JSON\n",
    "- Parquet\n",
    "\n",
    "You can read these file types from local and remote filesystems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550855c-99c2-4f9f-a01b-2886523f4751",
   "metadata": {},
   "source": [
    "### CSV\n",
    "\n",
    "Use the `read_csv` method to read CSV files from your local filesystem.\n",
    "\n",
    "Here, we'll read in some synthetic US Census data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2eb317-bbf2-4b78-b888-7659fc9f637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a single CSV file from your local filesystem\n",
    "df = daft.read_csv(\"data/census001.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f70ee8c-34e1-4976-a09d-48cbcb40c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a075b75-d01f-40be-8531-ff30937803e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dcca7a-1a3d-479d-9c09-aa127c2e48ed",
   "metadata": {},
   "source": [
    "You can also read folders of CSV files or include wildcards to select for patterns of file paths. \n",
    "\n",
    "These files will have to follow the same schema.\n",
    "\n",
    "Here, we'll read in multiple CSV files containing synthetic US Census data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99181e94-f514-4244-a7a8-3c4356229f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read multiple CSV files into one DataFrame\n",
    "df = daft.read_csv(\"data/census*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35607fda-9491-40af-9cd8-13241a99a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006bdbd-5ff7-4652-84ff-5ac29131ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a7fdb-3388-4ab1-affd-d05bc0b0cc3e",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "You can read line-delimited JSON using the `read_json` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fce57346-8396-4b2a-809f-1c16247335b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read a JSON file from your local filesystem\n",
    "df = daft.read_json(\"data/sampled-tpch.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf1888e-8a30-4c1f-bfed-4107f4aa83f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005fa7d-972f-4ab9-a37c-ae9a3c5a5df1",
   "metadata": {},
   "source": [
    "### Parquet\n",
    "\n",
    "Use the `read_parquet` method to read Parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0bf3a29f-b85a-4f92-8fd6-26be142809c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a Parquet file from your local filesystem\n",
    "df = daft.read_parquet(\"data/sample_taxi.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20ff085-6434-464c-b5b0-2d43a53a637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a643e97-ab65-4922-962a-e6e0892eba99",
   "metadata": {},
   "source": [
    "### Remote Reads, e.g. S3\n",
    "\n",
    "You can read files from remote filesystems such as AWS S3:\n",
    "\n",
    "```\n",
    "# Read multiple Parquet files from s3\n",
    "df = daft.read_parquet(\"s3://mybucket/path/to/*.parquet\")\n",
    "```\n",
    "\n",
    "These reads can be specified with their corresponding protocols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7644b2-66c4-46c3-b165-2b8cf2154ed5",
   "metadata": {},
   "source": [
    "#### Reading from Public Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f92d2e4-9d70-4a94-ae14-15cc1ae8a6fd",
   "metadata": {},
   "source": [
    "You can read from public buckets using an \"anonymous\" IO Config.\n",
    "\n",
    "An anonymous IOConfig will access storage **without credentials**, and can only access fully public data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63bbec5d-d1bc-4144-9b48-aa0762e85532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create anonymous config\n",
    "MY_ANONYMOUS_IO_CONFIG = daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True))\n",
    "\n",
    "# Read this file using `MY_ANONYMOUS_IO_CONFIG`\n",
    "df = daft.read_csv(\"s3://daft-public-data/melbourne-airbnb/melbourne_airbnb.csv\", io_config=MY_ANONYMOUS_IO_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3924275-4035-47d9-a5eb-d0e6e2ed0716",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.select(\"id\", \"text\", \"price\", \"review_scores_rating\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7651f5f-b6b9-46fe-9167-9fbe58682c0e",
   "metadata": {},
   "source": [
    "#### Remote IO Configuration\n",
    "Use the `IOConfig` module to configure access to remote data.\n",
    "\n",
    "Daft will automatically detect S3 credentials from your local environment. If your current session is authenticated with access credentials to your private bucket, then you can access the bucket  without explicitly passing credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf5c3ab-7422-4fe1-b323-6d55cdb63139",
   "metadata": {},
   "source": [
    "Substitute the path below with a path to a private bucket you have access to with your credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2613a3d2-7317-4c78-bd2b-711f0eadd71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"s3://avriiil/yellow_tripdata_2023-12.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c707d-cfb8-49ca-9e02-43279c04bfab",
   "metadata": {},
   "source": [
    "Now configure your IOConfig object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a44ba849-3ab8-4ba2-9d83-e825b236eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft.io import IOConfig, S3Config\n",
    "\n",
    "io_config = IOConfig(\n",
    "    s3=S3Config(\n",
    "        region_name=\"eu-north-1\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de767705-2155-4f3b-95ca-18459d3bdd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = daft.read_parquet(bucket, io_config=io_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bf1385-a0c5-4386-a495-97e4fda0be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd209fd5-5108-42fc-ae15-9e9d3287ea39",
   "metadata": {},
   "source": [
    "The IOConfig object has many more configuration options. You can use this object to configure access to:\n",
    "* [AWS S3](https://www.getdaft.io/projects/docs/en/latest/api_docs/doc_gen/io_configs/daft.io.S3Config.html)\n",
    "* [GCP](https://www.getdaft.io/projects/docs/en/latest/api_docs/doc_gen/io_configs/daft.io.GCSConfig.html)\n",
    "* [Azure](https://www.getdaft.io/projects/docs/en/latest/api_docs/doc_gen/io_configs/daft.io.AzureConfig.html)\n",
    "\n",
    "All the cloud-specific Config options follow the standard protocols of the respective cloud platforms. See the documentation links above for more information.\n",
    "\n",
    "There is a dedicated section for the `IOConfig` object at the end of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc38745-5631-4aed-bf04-bd59edf9ce0b",
   "metadata": {},
   "source": [
    "## SQL Databases\n",
    "You can use Daft to read the results of SQL queries from databases, data warehouses, and query engines, into a Daft DataFrame via the `daft.read_sql()` function.\n",
    "\n",
    "```python\n",
    "# Read from a PostgreSQL database\n",
    "uri = \"postgresql://user:password@host:port/database\"\n",
    "df = daft.read_sql(\"SELECT * FROM my_table\", uri)\n",
    "```\n",
    "In order to partition the data, you can specify a partition column:\n",
    "\n",
    "```python\n",
    "# Read with a partition column\n",
    "df = daft.read_sql(\"SELECT * FROM my_table\", partition_col=\"date\", uri)\n",
    "```\n",
    "\n",
    "Partitioning your data will allow Daft to read the data in parallel. This will make your queries faster.\n",
    "\n",
    "### ConnectorX vs SQLAlchemy\n",
    "\n",
    "Daft uses [ConnectorX](https://sfu-db.github.io/connector-x/databases.html) under the hood to read SQL data. ConnectorX is a fast, Rust based SQL connector that reads directly into Arrow Tables, enabling zero-copy transfer into Daft dataframes. If the database is [not supported](https://sfu-db.github.io/connector-x/intro.html#supported-sources-destinations) by ConnectorX, Daft will fall back to using [SQLAlchemy](https://docs.sqlalchemy.org/en/20/orm/quickstart.html).\n",
    "\n",
    "You can also directly provide a SQL alchemy connection via a connection factory. This way, you have the flexibility to provide additional parameters to the engine.\n",
    "\n",
    "### Example\n",
    "\n",
    "Let's look at an example with a simple local database.\n",
    "\n",
    "You will need some extra dependencies installed. The easiest way to do so is using the `[sql]` extras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9986979-938d-4a55-bccd-a6b2191f986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"getdaft[sql]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebdcdb0-ca54-48df-bd1c-19100322b2e4",
   "metadata": {},
   "source": [
    "### create local SQL database from CSV file\n",
    "\n",
    "Let's start by creating a local SQLite database from a CSV file using ConnectorX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aef3716-6f26-466c-8c16-f811ec4cdd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "connection = sqlite3.connect(\"example.db\")\n",
    "connection.execute(\"CREATE TABLE IF NOT EXISTS books (title TEXT, author TEXT, year INTEGER)\")\n",
    "connection.execute(\n",
    "    \"\"\"\n",
    "INSERT INTO books (title, author, year)\n",
    "VALUES\n",
    "    ('The Great Gatsby', 'F. Scott Fitzgerald', 1925),\n",
    "    ('To Kill a Mockingbird', 'Harper Lee', 1960),\n",
    "    ('1984', 'George Orwell', 1949),\n",
    "    ('The Catcher in the Rye', 'J.D. Salinger', 1951)\n",
    "\"\"\"\n",
    ")\n",
    "connection.commit()\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e0215a-4142-4588-b4b2-97a72f146318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read SQL query into Daft DataFrame\n",
    "df = daft.read_sql(\n",
    "    \"SELECT * FROM books\",\n",
    "    \"sqlite://example.db\",\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eebd2c3-2a7d-4ff6-a1c7-48029b21162d",
   "metadata": {},
   "source": [
    "You can also directly provide a SQL alchemy connection via a connection factory. This way, you have the flexibility to provide additional parameters to the engine.\n",
    "\n",
    "Let's use a SQL alchemy connection to read a CSV file into a local SQLite database and then query it with Daft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cfcb220-18b0-4aa1-891a-239a76259964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "# substitue the uri below with the engine path on your local machine\n",
    "engine_uri = \"sqlite:////Users/rpelgrim/daft_sql\"\n",
    "engine = create_engine(engine_uri, echo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ea42b-5cab-4f6b-9fac-18b2ae284b18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file_path = \"data/census-01.csv\"\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "sql_df = df.to_sql(name=\"censustable\", con=engine, index=False, index_label=\"id\", if_exists=\"replace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cc8ed0-868b-43ab-9c09-0baffe261dff",
   "metadata": {},
   "source": [
    "### Access SQL Database with Daft"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bab1ca-026c-4414-833a-ca27b41d1056",
   "metadata": {},
   "source": [
    "Great, now let's see how we can access this data with Daft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e82dcdfb-b8d0-4b06-b130-838507e6a8e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read from local SQLite database\n",
    "uri = \"sqlite:////Users/rpelgrim/daft_sql\"  # replace with your local uri\n",
    "\n",
    "df = daft.read_sql(\"SELECT * FROM censustable\", uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57731b4c-dbfb-478b-97c5-23e27d27cf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bbba7d-1592-42a3-abfa-d95f568d1259",
   "metadata": {},
   "source": [
    "### Parallel and Distributed Reads\n",
    "Supply a partition column and optionally the number of partitions to enable parallel reads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d54b5f-d2e2-4e37-ba63-5980554c6d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = daft.read_sql(\n",
    "    \"SELECT * FROM censustable\",\n",
    "    uri,\n",
    "    partition_col=\"education\",\n",
    "    #    num_partitions=12\n",
    ")\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b7875a-8ac5-4105-b1e1-90586d5f9851",
   "metadata": {},
   "source": [
    "### Data Skipping Optimizations\n",
    "Filter, projection, and limit pushdown optimizations can be used to reduce the amount of data read from the database.\n",
    "\n",
    "In the example below, Daft reads the top ranked terms from the BigQuery Google Trends dataset. The where and select expressions in this example will be pushed down into the SQL query itself, we can see this by calling the `df.explain()` method:\n",
    "\n",
    "```python\n",
    "import daft, sqlalchemy, datetime\n",
    "\n",
    "def create_conn():\n",
    "    engine = sqlalchemy.create_engine(\n",
    "        \"bigquery://\", credentials_path=\"path/to/service_account_credentials.json\"\n",
    "    )\n",
    "    return engine.connect()\n",
    "\n",
    "\n",
    "df = daft.read_sql(\"SELECT * FROM `bigquery-public-data.google_trends.top_terms`\", create_conn)\n",
    "\n",
    "df = df.where((df[\"refresh_date\"] >= datetime.date(2024, 4, 1)) & (df[\"refresh_date\"] < datetime.date(2024, 4, 8)))\n",
    "df = df.where(df[\"rank\"] == 1)\n",
    "df = df.select(df[\"refresh_date\"].alias(\"Day\"), df[\"term\"].alias(\"Top Search Term\"), df[\"rank\"])\n",
    "df = df.distinct()\n",
    "df = df.sort(df[\"Day\"], desc=True)\n",
    "\n",
    "df.explain(show_all=True)\n",
    "\n",
    "# Output\n",
    "# ..\n",
    "# == Physical Plan ==\n",
    "# ..\n",
    "# |   SQL Query = SELECT refresh_date, term, rank FROM\n",
    "#  (SELECT * FROM `bigquery-public-data.google_trends.top_terms`)\n",
    "#  AS subquery WHERE rank = 1 AND refresh_date >= CAST('2024-04-01' AS DATE)\n",
    "#  AND refresh_date < CAST('2024-04-08' AS DATE)\n",
    "```\n",
    "\n",
    "You could code the SQL query to add the filters and projections yourself, but this may become lengthy and error-prone, particularly with many expressions. Daft automatically handles these performance optimizations for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91efa0a-5d3c-45fd-ba7d-f3ee924dea2c",
   "metadata": {},
   "source": [
    "## Data Catalogs\n",
    "\n",
    "Daft is built for efficient data access from Data Catalogs using open table formats like Delta Lake, Iceberg and Hudi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f91db0f-9a39-4dc5-b796-d890d18eafb2",
   "metadata": {},
   "source": [
    "### Delta Lake\n",
    "\n",
    "You can easily read Delta Lake tables using the `read_deltalake()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df82f7-916d-48b9-b1e4-bcc9c19703f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = daft.read_deltalake(\"data/delta_table\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dad9e0-c3f3-4d67-82da-99ccb8162857",
   "metadata": {},
   "source": [
    "To access Delta tables on S3 you will have to pass some more config options:\n",
    "- the AWS Region name\n",
    "- your access credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f8626e-b1f1-493f-8675-19a2f5f530bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "creds = session.get_credentials()\n",
    "\n",
    "# set io configs\n",
    "io_config = daft.io.IOConfig(\n",
    "    s3=daft.io.S3Config(\n",
    "        region_name=\"eu-north-1\",\n",
    "        key_id=creds.access_key,\n",
    "        access_key=creds.secret_key,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Read Delta Lake table in S3 into a Daft DataFrame.\n",
    "table_uri = \"s3://avriiil/delta-test-daft/\"\n",
    "\n",
    "df = daft.read_deltalake(table_uri, io_config=io_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc056467-ed7b-447a-874b-634bb46961fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a916e0e-6d2e-4ae0-8a64-de8883a627af",
   "metadata": {},
   "source": [
    "### Iceberg\n",
    "Daft is integrated with [PyIceberg](https://py.iceberg.apache.org/), the official Python implementation for Apache Iceberg.\n",
    "\n",
    "This means you can easily read Iceberg tables into Daft DataFrames in 2 steps:\n",
    "1. Load your Iceberg table from your Iceberg catalog using PyIceberg\n",
    "2. Read your Iceberg table into Daft\n",
    "\n",
    "We'll use a simple local SQLite Catalog implementation for this toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e814b81-3996-4869-984f-d618c86a4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize your catalog\n",
    "\n",
    "from pyiceberg.catalog.sql import SqlCatalog\n",
    "\n",
    "warehouse_path = \"data/iceberg-warehouse/\"\n",
    "catalog = SqlCatalog(\n",
    "    \"default\",\n",
    "    **{\n",
    "        \"uri\": f\"sqlite:///{warehouse_path}/pyiceberg_catalog.db\",\n",
    "        \"warehouse\": f\"file://{warehouse_path}\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5746d73-0fdd-4c85-9b9b-22c9bf57b6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your table\n",
    "table = catalog.load_table(\"default.taxi_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3dc8f-9c9f-4a5f-9e6d-b4531325811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into Daft\n",
    "df = daft.read_iceberg(table)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49d8bf6-c053-4a37-98c9-bc2cfd0142cc",
   "metadata": {},
   "source": [
    "Any subsequent filter operations on the Daft `df` DataFrame object will be correctly optimized to take advantage of Iceberg features such as hidden partitioning and file-level statistics for efficient reads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121354a2-00b8-4daa-8a1b-a15cfa1d5631",
   "metadata": {},
   "source": [
    "### Hudi\n",
    "To read from an Apache Hudi table, use the `daft.read_hudi()` function. \n",
    "\n",
    "The following is an example snippet of loading an example table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca307b8-c4bc-4fe7-8b74-06a78691b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Apache Hudi table into a Daft DataFrame.\n",
    "import daft\n",
    "\n",
    "df = daft.read_hudi(\"data/hudi-data\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59d022-83c6-4c58-9016-73af00825335",
   "metadata": {},
   "source": [
    "Currently there are limitations of reading Hudi tables:\n",
    "- Only support snapshot read of Copy-on-Write tables\n",
    "- Only support reading table version 5 & 6 (tables created using release 0.12.x - 0.15.x)\n",
    "- Table must not have hoodie.datasource.write.drop.partition.columns=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6333ea-99e5-4467-9992-18213dc63900",
   "metadata": {},
   "source": [
    "## IOConfig Deep Dive\n",
    "\n",
    "Let's dive a little deeper into the IOConfig options for tweaking your remote data access.\n",
    "\n",
    "`IOConfig` is Daft's mechanism for controlling the behavior of data input/output from storage. It is useful for:\n",
    "\n",
    "1. **Providing credentials** for authenticating with cloud storage services\n",
    "2. **Tuning performance** or reducing load on storage services\n",
    "\n",
    "### Default IOConfig Behavior\n",
    "\n",
    "The default behavior for IOConfig is to automatically detect credentials on your machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad729cde-1d82-4550-938b-424285343221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "# By default, calls to AWS S3 will use credentials retrieved from the machine(s) that they are called from\n",
    "#\n",
    "# For AWS S3 services, the default mechanism is to look through a chain of possible \"providers\":\n",
    "# https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html#configuring-credentials\n",
    "df = daft.read_csv(\"s3://daft-public-data/file.csv\")\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59065702-64a9-4d52-b2a1-f3b1ab74dc6f",
   "metadata": {},
   "source": [
    "### Overriding the IOConfig\n",
    "#### Setting a Global Override\n",
    "\n",
    "Often you may want Daft to just use a certain configuration by default whenever it has to access storage such as S3, GCS or Azure Blob Store.\n",
    "\n",
    "> **Example:**\n",
    ">\n",
    "> An extremely common use-case is to create a set of temporary credentials once, and share that across all calls to data access happening in Daft.\n",
    ">\n",
    "> The example below demonstrates this with AWS S3's `boto3` Python SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b349ed-474f-4b00-89ef-4ccaad2ac05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the boto3 library to generate temporary credentials which can be used for S3 access\n",
    "import boto3\n",
    "\n",
    "session = boto3.session.Session()\n",
    "creds = session.get_credentials()\n",
    "\n",
    "# Attach temporary credentials to a Daft IOConfig object\n",
    "MY_IO_CONFIG = daft.io.IOConfig(\n",
    "    s3=daft.io.S3Config(\n",
    "        key_id=creds.access_key,\n",
    "        access_key=creds.secret_key,\n",
    "        session_token=creds.token,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Set the default config to `MY_IO_CONFIG` so that it is used in the absence of any overrides\n",
    "daft.set_planning_config(default_io_config=MY_IO_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ba6eb-44a7-41b2-834b-8e427d868b67",
   "metadata": {},
   "source": [
    "#### Overriding IOConfigs per-API call\n",
    "\n",
    "Daft also allows for more granular per-call overrides through the use of keyword arguments.\n",
    "\n",
    "This is extremely flexible, allowing you to use a different set of credentials to read from two different locations!\n",
    "\n",
    "Here we use `daft.read_csv` as an example, but the same `io_config=...` keyword arg also exists for other I/O related functionality such as:\n",
    "\n",
    "1. `daft.read_parquet`\n",
    "2. `daft.read_json`\n",
    "3. `Expression.url.download()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae90c07-3a74-4c4c-bdfa-fded87f81006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An \"Anonymous\" IOConfig will access storage **without credentials**, and can only access fully public data\n",
    "MY_ANONYMOUS_IO_CONFIG = daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True))\n",
    "\n",
    "# Read this file using `MY_ANONYMOUS_IO_CONFIG` instead of the overridden global config `MY_IO_CONFIG`\n",
    "df1 = daft.read_csv(\"s3://daft-public-data/melbourne-airbnb/melbourne_airbnb.csv\", io_config=MY_ANONYMOUS_IO_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed83cac4-9043-4290-8b8e-8b36811f3a06",
   "metadata": {},
   "source": [
    "For more see: [IOConfig Documentation](https://www.getdaft.io/projects/docs/en/latest/api_docs/doc_gen/io_configs/daft.io.IOConfig.html?highlight=IOConfig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f0b828-5afb-4438-8359-8be8fb648a15",
   "metadata": {},
   "source": [
    "## Data Access with Daft\n",
    "In this tutorial you have seen the canonical ways of accessing data with Daft.\n",
    "\n",
    "We've seen how to access:\n",
    "* local files, incl. JSON, CSV and Parquet\n",
    "* data in SQL databases\n",
    "* data in Data Catalogs, incl. Delta Lake, Iceberg and Hudi\n",
    "\n",
    "Take a look at our hands-on [Use Case tutorials](https://www.getdaft.io/projects/docs/en/latest/user_guide/tutorials.html) if you feel ready to start building workflows with Daft."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
