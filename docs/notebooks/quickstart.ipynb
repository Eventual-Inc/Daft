{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eventual-Inc/Daft/blob/tools%2Fdocs-to-notebook-converter/docs/notebooks/quickstart.ipynb)\n",
    "\n",
    "Daft is the best multimodal data processing engine that allows you to load data from anywhere, transform it with a powerful DataFrame API and AI functions, and store it in your destination of choice. In this quickstart, you'll see what this looks like in practice with a realistic e-commerce data workflow.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "Daft requires **Python 3.10 or higher**.\n",
    "\n",
    "### Install Daft\n",
    "\n",
    "You can install Daft using `pip`. Run the following command in your terminal or notebook:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install -U \"daft[openai]\"  # Includes OpenAI extras needed for this quickstart"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, install these packages for image processing (used later in this quickstart):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install numpy pillow"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Your Data\n",
    "\n",
    "Let's start by loading an e-commerce dataset from Hugging Face. [This dataset](https://huggingface.co/datasets/calmgoose/amazon-product-data-2020) contains 10,000+ Amazon products from diverse categories including electronics, toys, home goods, and more. Each product includes details like names, prices, descriptions, technical specifications, and product images."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import daft\n",
    "\n",
    "df_original = daft.read_huggingface(\"calmgoose/amazon-product-data-2020\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #448aff22; border-left: 4px solid #448aff; padding: 12px; margin: 16px 0;\">\n",
    "<strong style=\"color: #448aff;\">Load from anywhere</strong><br/>\n",
    "Daft can load data from many sources including <a href=\"https://docs.daft.ai/en/stable/connectors/aws/\">S3</a>, <a href=\"https://docs.daft.ai/en/stable/connectors/iceberg/\">Iceberg</a>, <a href=\"https://docs.daft.ai/en/stable/connectors/delta_lake/\">Delta Lake</a>, <a href=\"https://docs.daft.ai/en/stable/connectors/hudi/\">Hudi</a>, and <a href=\"https://docs.daft.ai/en/stable/connectors/\">more</a>. We're using Hugging Face here as a demonstration.\n",
    "</div>\n",
    "\n",
    "### Inspect Your Data\n",
    "\n",
    "Now let's take a look at what we loaded. You can inspect the DataFrame by simply printing it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_original"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the above output because **Daft is lazy by default** - it displays the schema (column names and types) but doesn't actually load or process your data until you explicitly tell it to. This allows Daft to optimize your entire workflow before executing anything.\n",
    "\n",
    "To actually view your data, you have two options:\n",
    "\n",
    "**Option 1: Preview with `.show()`** - View the first few rows:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "df_original.show(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This materializes and displays just the first 2 rows, which is perfect for quickly inspecting your data without loading the entire dataset.\n",
    "\n",
    "**Option 2: Materialize with `.collect()`** - Load the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# df_original.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would materialize the entire DataFrame (all 10,000+ rows in this case) into memory. Use `.collect()` when you need to work with the full dataset in memory.\n",
    "\n",
    "### Working with a Smaller Dataset\n",
    "\n",
    "For quick experimentation, let's create a smaller, simplified version of the dataframe with just the essential columns:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select only the columns we need and limit to 5 rows for faster iteration\n",
    "df = df_original.select(\"Product Name\", \"About Product\", \"Image\").limit(5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a manageable dataset of 5 products with just the product name, description, and image URLs. This simplified dataset lets us explore Daft's features without the overhead of unnecessary columns.\n",
    "\n",
    "### Downloading Images\n",
    "\n",
    "Let's extract and download product images. The `Image` column contains pipe-separated URLs. We'll extract the first URL and download it:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract the first image URL from the pipe-separated list\n",
    "# The pattern captures everything before the first pipe or the entire string if no pipe\n",
    "df = df.with_column(\n",
    "    \"first_image_url\",\n",
    "    daft.functions.regexp_extract(\n",
    "        df[\"Image\"],\n",
    "        r\"^([^|]+)\",  # Extract everything before the first pipe\n",
    "        1  # Get the first capture group\n",
    "    )\n",
    ")\n",
    "\n",
    "# Download the image data\n",
    "df = df.with_column(\n",
    "    \"image_data\",\n",
    "    daft.functions.download(df[\"first_image_url\"], on_error=\"null\")\n",
    ")\n",
    "\n",
    "# Decode images for visual display (in Jupyter notebooks, this shows actual images!)\n",
    "df = df.with_column(\n",
    "    \"image\",\n",
    "    daft.functions.decode_image(df[\"image_data\"], on_error=\"null\")\n",
    ")\n",
    "\n",
    "# Check what we have - in Jupyter notebooks, the 'image' column shows actual images!\n",
    "df.select(\"Product Name\", \"first_image_url\", \"image_data\", \"image\").show(3)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #448aff22; border-left: 4px solid #448aff; padding: 12px; margin: 16px 0;\">\n",
    "<strong style=\"color: #448aff;\">Visual Display in Notebooks</strong><br/>\n",
    "In Jupyter notebooks, the `image` column will display actual thumbnail images instead of `<Image>` text.\n",
    "</div>\n",
    "\n",
    "This demonstrates Daft's multimodal capabilities:\n",
    "\n",
    "- **Native regex support**: Use `regexp_extract()` to parse structured text with Rust-powered regex\n",
    "- **URL handling**: Download content directly with`daft.functions.download()`\n",
    "- **Image decoding**: Convert binary data to images with `decode_image()` for visual display\n",
    "\n",
    "The decoded images are now ready for further processing.\n",
    "\n",
    "### Batch AI Inference on Images\n",
    "\n",
    "Let's use AI to analyze product materials at scale. Daft automatically parallelizes AI operations across your local machine's cores, making it efficient to process multiple images concurrently.\n",
    "\n",
    "Let's suppose you want to create a new column that shows if each product is made of wood or not. This might be useful for, for example, a filtering feature on your website."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from daft.functions import prompt\n",
    "\n",
    "# Define a simple structured output model\n",
    "class WoodAnalysis(BaseModel):\n",
    "    is_wooden: bool = Field(description=\"Whether the product appears to be made of wood\")\n",
    "\n",
    "# Run AI inference on each image - Daft automatically batches and parallelizes this\n",
    "# Note: You can pass api_key explicitly here, or set the OPENAI_API_KEY environment variable\n",
    "df = df.with_column(\n",
    "    \"wood_analysis\",\n",
    "    prompt(\n",
    "        [\"Is this product made of wood? Look at the material.\", df[\"image\"]],\n",
    "        return_format=WoodAnalysis,\n",
    "        model=\"gpt-4o-mini\",  # Using mini for cost-efficiency\n",
    "        provider=\"openai\",\n",
    "        api_key=\"your-openai-api-key-here\"  # Or omit this to use OPENAI_API_KEY env var\n",
    "    )\n",
    ")\n",
    "\n",
    "# Extract the boolean value from the structured output\n",
    "# The result is a struct, so we extract the 'is_wooden' field\n",
    "df = df.with_column(\n",
    "    \"is_wooden\",\n",
    "    df[\"wood_analysis\"][\"is_wooden\"]\n",
    ")\n",
    "\n",
    "# Materialize the dataframe to compute all transformations\n",
    "df = df.collect()\n",
    "\n",
    "# View results\n",
    "df.select(\"Product Name\", \"image\", \"is_wooden\").show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AI analyzes each product image to determine if it's made of wood. Notice that the longboard is identified as wooden (true), while the electronic circuits, design studio, puzzle, and 3D printing filament are identified as not wooden (false).\n",
    "\n",
    "<div style=\"background-color: #448aff22; border-left: 4px solid #448aff; padding: 12px; margin: 16px 0;\">\n",
    "<strong style=\"color: #448aff;\">Improving Accuracy</strong><br/>\n",
    "Looking at the actual product data, the longboard is made of bamboo and fiberglass, not wood. However, this is exactly what a human might categorize from the image alone! To improve accuracy, you could feed additional context to the AI like the product name, category, and description alongside the image. This example demonstrates how to get started with image-based analysis.\n",
    "</div>\n",
    "\n",
    "### Expanding the Analysis\n",
    "\n",
    "Now, suppose you're satisfied with the results from your small subset and want to scale up. Instead of analyzing just 5 products, let's run the same analysis on 100 products to get more meaningful insights:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from daft.functions import prompt\n",
    "\n",
    "# Define a simple structured output model (same as before)\n",
    "class WoodAnalysis(BaseModel):\n",
    "    is_wooden: bool = Field(description=\"Whether the product appears to be made of wood\")\n",
    "\n",
    "# Start fresh with the first 100 products\n",
    "df_large = df_original.select(\"Product Name\", \"About Product\", \"Image\").limit(100)\n",
    "\n",
    "# Apply the same image processing pipeline\n",
    "# 1. Extract first image URL\n",
    "df_large = df_large.with_column(\n",
    "    \"first_image_url\",\n",
    "    daft.functions.regexp_extract(\n",
    "        df_large[\"Image\"],\n",
    "        r\"^([^|]+)\",\n",
    "        1\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Download images\n",
    "df_large = df_large.with_column(\n",
    "    \"image_data\",\n",
    "    daft.functions.download(df_large[\"first_image_url\"], on_error=\"null\")\n",
    ")\n",
    "\n",
    "# 3. Decode images\n",
    "df_large = df_large.with_column(\n",
    "    \"image\",\n",
    "    daft.functions.decode_image(df_large[\"image_data\"], on_error=\"null\")\n",
    ")\n",
    "\n",
    "# 4. Run AI analysis on all 100 products\n",
    "# Note: You can pass api_key explicitly here, or set the OPENAI_API_KEY environment variable\n",
    "df_large = df_large.with_column(\n",
    "    \"wood_analysis\",\n",
    "    prompt(\n",
    "        [\"Is this product made of wood? Look at the material.\", df_large[\"image\"]],\n",
    "        return_format=WoodAnalysis,\n",
    "        model=\"gpt-4o-mini\",  # Using mini for cost-efficiency\n",
    "        provider=\"openai\",\n",
    "        api_key=\"your-openai-api-key-here\"  # Or omit this to use OPENAI_API_KEY env var\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Extract the boolean value\n",
    "df_large = df_large.with_column(\n",
    "    \"is_wooden\",\n",
    "    df_large[\"wood_analysis\"][\"is_wooden\"]\n",
    ")\n",
    "\n",
    "# Materialize the dataframe to compute all transformations\n",
    "df_large = df_large.collect()\n",
    "\n",
    "# Count wooden products\n",
    "wooden_count = df_large.where(df_large[\"is_wooden\"] == True).count_rows()\n",
    "total_count = df_large.count_rows()\n",
    "\n",
    "print(f\"Out of {total_count} products analyzed:\")\n",
    "print(f\"  - {wooden_count} are made of wood\")\n",
    "print(f\"  - {total_count - wooden_count} are not made of wood\")\n",
    "print(f\"  - Percentage of wooden products: {(wooden_count / total_count * 100):.1f}%\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #448aff22; border-left: 4px solid #448aff; padding: 12px; margin: 16px 0;\">\n",
    "<strong style=\"color: #448aff;\">Results May Vary</strong><br/>\n",
    "AI models are non-deterministic, so you may see slightly different numbers when running this analysis.\n",
    "</div>\n",
    "\n",
    "### Storing Your Results\n",
    "\n",
    "After processing your data, you'll often want to save it for later use. Let's store our analyzed dataset as Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Write the analyzed data to local Parquet files\n",
    "df_large.write_parquet(\"product_analysis\", write_mode=\"overwrite\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This writes your data to the `product_analysis/` directory. Daft automatically handles file naming using UUIDs to prevent conflicts. The `write_mode=\"overwrite\"` parameter ensures that any existing data in the directory is replaced.\n",
    "\n",
    "<div style=\"background-color: #448aff22; border-left: 4px solid #448aff; padding: 12px; margin: 16px 0;\">\n",
    "<strong style=\"color: #448aff;\">Write anywhere</strong><br/>\n",
    "Just like reading, Daft can write data to many destinations including <a href=\"https://docs.daft.ai/en/stable/connectors/aws/\">S3</a>, <a href=\"https://docs.daft.ai/en/stable/connectors/iceberg/\">Iceberg</a>, <a href=\"https://docs.daft.ai/en/stable/connectors/delta_lake/\">Delta Lake</a>, and <a href=\"https://docs.daft.ai/en/stable/connectors/\">more</a>.\n",
    "</div>\n",
    "\n",
    "### Loading Your Stored Data\n",
    "\n",
    "Let's verify the stored data by loading it back from those Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Read the data back from Parquet files\n",
    "df_loaded = daft.read_parquet(\"product_analysis/*.parquet\")\n",
    "\n",
    "# Verify the data loaded correctly\n",
    "df_loaded.show(5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's Next?\n",
    "\n",
    "Now that you have a basic sense of Daft's functionality and features, here are some more resources to help you get the most out of Daft:\n",
    "\n",
    "<div style=\"background-color: #00c85322; border-left: 4px solid #00c853; padding: 12px; margin: 16px 0;\">\n",
    "<strong style=\"color: #00c853;\">Scaling Further</strong><br/>\n",
    "This same pipeline can process thousands or millions of products by leveraging Daft's distributed computing capabilities. Check out our <a href=\"https://docs.daft.ai/en/stable/distributed/\">distributed computing guide</a> to run this analysis at scale on Ray or Kubernetes clusters. Alternatively, <a href=\"https://www.daft.ai/cloud\">Daft Cloud</a> provides a fully managed serverless experience.\n",
    "</div>\n",
    "\n",
    "**Work with your favorite table and catalog formats:**\n",
    "\n",
    "- [Apache Hudi](https://docs.daft.ai/en/stable/connectors/hudi/)\n",
    "- [Apache Iceberg](https://docs.daft.ai/en/stable/connectors/iceberg/)\n",
    "- [AWS Glue](https://docs.daft.ai/en/stable/connectors/glue/)\n",
    "- [AWS S3 Tables](https://docs.daft.ai/en/stable/connectors/s3tables/)\n",
    "- [Delta Lake](https://docs.daft.ai/en/stable/connectors/delta_lake/)\n",
    "- [Hugging Face Datasets](https://docs.daft.ai/en/stable/connectors/huggingface/)\n",
    "- [Unity Catalog](https://docs.daft.ai/en/stable/connectors/unity_catalog/)\n",
    "\n",
    "**Explore our [Examples](https://docs.daft.ai/en/stable/examples/) to see Daft in action:**\n",
    "\n",
    "- [MNIST Digit Classification](https://docs.daft.ai/en/stable/examples/mnist/)\n",
    "- [Running LLMs on the Red Pajamas Dataset](https://docs.daft.ai/en/stable/examples/llms-red-pajamas/)\n",
    "- [Querying Images with UDFs](https://docs.daft.ai/en/stable/examples/querying-images/)\n",
    "- [Image Generation on GPUs](https://docs.daft.ai/en/stable/examples/image-generation/)\n",
    "- [Window Functions in Daft](https://docs.daft.ai/en/stable/examples/window-functions/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}