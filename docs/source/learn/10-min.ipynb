{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 minutes to Daft\n",
    "\n",
    "This is a short introduction to all the main functionality in Daft, geared towards new users.\n",
    "\n",
    "We import from daft as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import DataFrame, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame creation\n",
    "\n",
    "See also: [DataFrames User Guide: Loading Data](dataframe-loading-data)\n",
    "\n",
    "We can create a DataFrame from a dictionary of columns - this is a dictionary where the keys are strings representing the columns' names and the values are equal-length lists representing the columns' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "df = DataFrame.from_pydict({\n",
    "    \"A\": [1, 2, 3, 4],\n",
    "    \"B\": [1.5, 2.5, 3.5, 4.5],\n",
    "    \"C\": [True, True, False, False],\n",
    "    \"D\": [\"a\", \"b\", \"c\", \"d\"],\n",
    "    \"E\": [b\"a\", b\"b\", b\"c\", b\"d\"],\n",
    "    \"F\": [datetime.date(1994, 1, 1), datetime.date(1994, 1, 2), datetime.date(1994, 1, 3), datetime.date(1994, 1, 4)],\n",
    "    \"G\": [[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4]],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the schema of your dataframe by inspecting the `df` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load DataFrames from other sources, such as:\n",
    "\n",
    "1. CSV files: `DataFrame.from_csv(\"s3://bucket/*.csv\")`\n",
    "2. Parquet files: `DataFrame.from_csv(\"/path/*.parquet\")`\n",
    "3. JSON line-delimited files: `DataFrame.from_json(\"/path/*.parquet\")`\n",
    "4. Files on disk: `DataFrame.from_files(\"/path/*.jpeg\")`\n",
    "\n",
    "Daft automatically supports local paths as well as paths to object storage such as AWS S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Data\n",
    "\n",
    "Use `DataFrame.show(N)` to view N rows of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sort a dataframe with `DataFrame.sort` - let's do that here and retrieve 2 rows again but after sorting on A in descending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort(df[\"A\"], desc=True).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve all the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert a DataFrame to a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection\n",
    "\n",
    "You can limit the number of rows in a dataframe by calling `DataFrame.limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_limited = df.limit(1)\n",
    "df_limited.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select just a few columns, you can use `DataFrame.select`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df.select(df[\"A\"], df[\"B\"])\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column selection also allows you to rename columns using `Expression.alias`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_renamed = df.select(df[\"A\"].alias(\"A2\"), df[\"B\"])\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To drop columns from the dataframe, call `DataFrame.exclude`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excluded = df.exclude(\"A\")\n",
    "df_excluded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "See: [Expressions](user_guides/expressions.rst)\n",
    "\n",
    "Expressions are an API for defining computation that needs to happen over your columns. Daft is **lazy**, which means that it does not execute computation when you define it with expressions, but only when you [execute the DataFrame](user-guide-execution) with something such as a `DataFrame.show()`.\n",
    "\n",
    "For example, to create a new column that is just the column A incremented by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_A_plus1 = df.with_column(\"A_plus1\", df[\"A\"] + 1)  # does not run any computation\n",
    "df_A_plus1.show()  # runs all the computations defined on the dataframe, and displays it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Accessors\n",
    "\n",
    "Some Expression methods are only allowed on certain types and are accessible through \"method accessors\" such as the `Expression.str` accessor (see: [Expression Accessor Properties](expression-accessor-properties)).\n",
    "\n",
    "For example, the `.str.length()` expression is only valid when run on a STRING column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_E_length = df.with_column(\"D_length\", df[\"D\"].str.length())\n",
    "df_E_length.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a useful method accessor is the `.url` accessor. You can use `.url.download()` to download data from a column of URLs like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url_df = DataFrame.from_pydict({\n",
    "    \"urls\": [\n",
    "        \"http://farm9.staticflickr.com/8186/8119368305_4e622c8349_z.jpg\",\n",
    "        \"http://farm1.staticflickr.com/1/127244861_ab0c0381e7_z.jpg\",\n",
    "        \"http://farm3.staticflickr.com/2169/2118578392_1193aa04a0_z.jpg\",\n",
    "    ],\n",
    "})\n",
    "image_downloaded_df = image_url_df.with_column(\"image_bytes\", image_url_df[\"urls\"].url.download())\n",
    "image_downloaded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a full list of all Expression methods and operators, see: [Expressions API Docs](../api_docs/expressions.rst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on PY columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PY columns contain Python objects and operations called on these columns will be mapped on each object as well.\n",
    "\n",
    "To work with such columns, Daft provides a few useful Expression methods.\n",
    "\n",
    "For example, to repeat each list in column `G` 3 times, we can use the Python `list`'s native Python `*` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_extend_0 = df.with_column(\"G_repeat\", df[\"G\"] * 3)\n",
    "df_G_extend_0.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call a method on each list in column `G`, we can use the `.as_py` method. For example, here we use the Python `list`'s `.count()` method to count the number of occurences of the integer in column A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_count_A = df.with_column(\"G_count_A\", df[\"G\"].as_py(list).count(df[\"A\"]))\n",
    "df_G_count_A.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more complicated functions, you can use `.apply(f)` to call a function `f` on every object in the column. For example, here we construct a Numpy array for every list in column G.\n",
    "\n",
    "```{note}\n",
    "It is good practice to supply the `return_type=` keyword argument in `.apply`, as this lets Daft effectively optimize your data and operations under the hood. For example, in this case we specify `return_type=np.ndarray` which tells Daft that each row in this column contains a Numpy array object.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_G_to_numpy = df.with_column(\"G_to_numpy\", df[\"G\"].apply(lambda l: np.array(l), return_type=np.ndarray))\n",
    "df_G_to_numpy.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterable types such as a PY[list] column can be exploded with `DataFrame.explode`, splitting each list into a row of its own and repeating the other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G_exploded = df.explode(df[\"G\"])\n",
    "df_G_exploded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Defined Functions\n",
    "\n",
    "`.apply` makes it really easy to map a function on a single column, but is limited in 2 main ways:\n",
    "\n",
    "1. Only runs on a single column: some algorithms require multiple columns as inputs\n",
    "2. Only runs on a single row: some algorithms run much more efficiently when run on a batch of rows instead\n",
    "\n",
    "To overcome these limitations, you can use User-Defined Functions (UDFs).\n",
    "\n",
    "See Also: [UDF User Guide](https://getdaft.io/learn/user_guides/udf.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from daft import polars_udf\n",
    "import polars as pl\n",
    "\n",
    "@polars_udf(return_type=datetime.date)\n",
    "def add_days(f_date_data: pl.Series, a_days_data: pl.Series):\n",
    "    return f_date_data + pl.duration(days=a_days_data)\n",
    "\n",
    "df.with_column(\"F_add_A_days\", add_days(df[\"F\"], df[\"A\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple UDF demonstrated above is a \"stateless UDF\", and no state is maintained between invocations of the function. In certain use-cases, it can be important to maintain some state with a \"stateful UDF\", which you can write using a Class instead of a Function. For example, running machine learning models often requires downloading some trained weights and initializing the model in memory/on a GPU, which an expensive operation and should be cached between UDF invocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@polars_udf(return_type=float)\n",
    "class RunExpensiveModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize and cache an \"expensive\" model between invocations of the UDF\n",
    "        self.model = np.array([1.23, 4.56])\n",
    "        \n",
    "    def __call__(self, a_data: pl.Series, b_data: pl.Series):\n",
    "        return np.matmul(self.model, np.array([a_data.to_numpy(), b_data.to_numpy()]))\n",
    "\n",
    "df.with_column(\"expensive_model_results\", RunExpensiveModel(df[\"A\"], df[\"B\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "You can filter rows in dataframe using `DataFrame.where`, which accepts a LOGICAL type Expression as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only rows where values in column \"A\" are less than 3\n",
    "df_filtered = df.where(df[\"A\"] < 3)\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "All columns in Daft are \"nullable\" by default. Unlike other frameworks such as Pandas, Daft differentiates between \"null\" (missing) and \"nan\" (stands for not a number - a special value indicating an invalid float)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_df = DataFrame.from_pydict({\n",
    "    \"floats\": [1.5, None, float(\"nan\")],\n",
    "})\n",
    "missing_data_df = missing_data_df \\\n",
    "    .with_column(\"floats_is_null\", missing_data_df[\"floats\"].is_null()) \\\n",
    "    .with_column(\"floats_is_nan\", missing_data_df[\"floats\"].is_nan())\n",
    "\n",
    "# NOTE: there is an open issue with display of null vs NaNs, see: https://github.com/Eventual-Inc/Daft/issues/241\n",
    "missing_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fill in missing values, a useful Expression is the `.if_else` expression which can be used to fill in values if the value is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_df = missing_data_df.with_column(\"filled_in_floats\", (missing_data_df[\"floats\"].is_null()).if_else(0.0, missing_data_df[\"floats\"]))\n",
    "missing_data_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Dataframes\n",
    "\n",
    "DataFrames can be joined with `.join`. Here is a naive example of a self-join where we join `df` on itself with column \"A\" as the join key. Notice that we automatically assign a prefix `\"right.\"` to all the columns with conflicting names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df.join(df, on=\"A\")\n",
    "joined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "Grouping operations over a dataset happens in 2 phases:\n",
    "\n",
    "1. Splitting the data into groups based on some criteria using `DataFrame.groupby`\n",
    "2. Specifying how to aggregate the data for each group using `GroupedDataFrame.agg`\n",
    "\n",
    "Let's take a look at an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_df = DataFrame.from_pydict(\n",
    "    {\n",
    "        \"A\": [\"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"bar\", \"foo\", \"foo\"],\n",
    "        \"B\": [\"a\", \"a\", \"b\", \"c\", \"b\", \"b\", \"a\", \"c\"],\n",
    "        \"C\": [i for i in range(8)],\n",
    "        \"D\": [i for i in range(8)],\n",
    "    }\n",
    ")\n",
    "grouping_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we group by \"A\", so that we will evaluate rows with `A=foo` and `A=bar` separately in their respective groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = grouping_df.groupby(grouping_df[\"A\"])\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can specify the aggregations we want to compute over columns C and D. Here we compute the sum over column C, and the mean over column D for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_df = grouped_df.agg([\n",
    "    (col(\"C\").alias(\"C_sum\"), \"sum\"),\n",
    "    (col(\"D\").alias(\"D_mean\"), \"mean\"),\n",
    "])\n",
    "aggregated_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These operations work as well when run over multiple groupby columns, which will produce one row for each combination of columns that occur in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_df \\\n",
    "    .groupby(grouping_df[\"A\"], grouping_df[\"B\"]) \\\n",
    "    .agg([\n",
    "        (col(\"C\").alias(\"C_sum\"), \"sum\"),\n",
    "        (col(\"D\").alias(\"D_mean\"), \"mean\"),\n",
    "    ]) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "See: [Writing Data](dataframe-writing-data)\n",
    "\n",
    "Writing data will execute your DataFrame and write the results out to the specified backend. For example, to write data out to CSV:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Daft does not write PY columns at the moment.\n",
    "# This is a feature that is on the roadmap as various options for implementation are being designed.\n",
    "write_df = df.exclude(\"G\")\n",
    "\n",
    "written_df = write_df.write_csv(\"my-dataframe.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that writing your dataframe is a **blocking** operation that executes your DataFrame. It will return a new `DataFrame` that contains the filepaths to the written data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "written_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "abe0cbefa28213872cf2c91d6aa47443089aa2a6ddfc370b260793ec957ca67a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
