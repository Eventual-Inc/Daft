[build-dependencies]
rustc_version = "0.4.0"

[dependencies]
# Faster hashing
ahash = "0.8"
arrow-array = {version = "=51.0.0", optional = true}
# Support conversion to/from arrow-rs
arrow-buffer = {version = "=51.0.0", optional = true}
arrow-data = {version = "=51.0.0", optional = true}
arrow-format = {version = "0.8", optional = true, features = ["ipc"]}
arrow-schema = {version = "=51.0.0", optional = true}
# to read IPC as a stream
async-stream = {version = "0.3.2", optional = true}
# avro support
avro-schema = {version = "0.3", optional = true}
base64 = {version = "0.21.0", optional = true}
bytemuck = {version = "1", features = ["derive"]}
chrono = {version = "0.4.31", default_features = false, features = ["std"]}
# for timezone support
chrono-tz = {version = "0.8", optional = true}
# used to print columns in a nice columnar format
comfy-table = {version = "6.0", optional = true, default-features = false}
# for csv io
csv = {version = "^1.1", optional = true}
# for csv async io
csv-async = {version = "^1.1", optional = true}
csv-core = {version = "0.1", optional = true}
dyn-clone = "1"
either = "1.9"
# for decimal i256
ethnum = "1"
fallible-streaming-iterator = {version = "0.1", optional = true}
foreign_vec = "0.1.0"
# to write to parquet as a stream
futures = {version = "0.3", optional = true}
# We need to Hash values before sending them to an hasher. This
# crate provides HashMap that assumes pre-hashed values.
hash_hasher = "^2.0.3"
# A Rust port of SwissTable
hashbrown = {version = "0.14", default-features = false, features = ["ahash"]}
hex = {version = "^0.4", optional = true}
indexmap = {version = "^1.6", optional = true}
itertools = {version = "^0.10", optional = true}
json-deserializer = {version = "0.4.4", optional = true, features = [
  "preserve_order"
]}
# To efficiently cast numbers to strings
lexical-core = {version = "0.8", optional = true}
# for IPC compression
lz4 = {version = "1.24", optional = true}
# For `LIKE` matching "contains" fast-path
memchr = {version = "2.6", optional = true}
# For instruction multiversioning
multiversion = {version = "0.7.3", optional = true}
num-traits = "0.2"
# ORC support
orc-format = {version = "0.3.0", optional = true}
rand = {version = "0.8", optional = true}
regex = {version = "1.9", optional = true}
regex-syntax = {version = "0.7", optional = true}
# Arrow integration tests support
serde = {version = "^1.0", features = ["rc", "derive"]}
serde_derive = {version = "^1.0", optional = true}
serde_json = {version = "^1.0", features = [
  "preserve_order"
], optional = true}
# For SIMD utf8 validation
simdutf8 = "0.1.4"
streaming-iterator = {version = "0.1", optional = true}
# for division/remainder optimization at runtime
strength_reduce = {version = "0.2", optional = true}
zstd = {version = "0.12", optional = true}

# parquet support
[dependencies.parquet2]
default_features = false
optional = true
path = "../parquet2"

[dev-dependencies]
avro-rs = {version = "0.13", features = ["snappy"]}
criterion = "0.4"
crossbeam-channel = "0.5.1"
doc-comment = "0.3"
flate2 = "1"
# used to run formal property testing
proptest = {version = "1", default_features = false, features = ["std"]}
# use for flaky testing
rand = "0.8"
# use for generating and testing random data samples
sample-arrow2 = "0.1"
sample-std = "0.1"
sample-test = "0.1"
# used to test async readers
tokio = {version = "1", features = ["macros", "rt", "fs", "io-util"]}
tokio-util = {version = "0.7", features = ["compat"]}

[features]
arrow = ["arrow-buffer", "arrow-schema", "arrow-data", "arrow-array"]
benchmarks = ["rand"]
compute = [
  "compute_aggregate",
  "compute_arithmetics",
  "compute_bitwise",
  "compute_boolean",
  "compute_boolean_kleene",
  "compute_cast",
  "compute_comparison",
  "compute_concatenate",
  "compute_contains",
  "compute_filter",
  "compute_hash",
  "compute_if_then_else",
  "compute_length",
  "compute_like",
  "compute_limit",
  "compute_merge_sort",
  "compute_nullif",
  "compute_partition",
  "compute_regex_match",
  "compute_sort",
  "compute_substring",
  "compute_take",
  "compute_temporal",
  "compute_utf8",
  "compute_window"
]
# the compute kernels. Disabling this significantly reduces compile time.
compute_aggregate = ["multiversion"]
compute_arithmetics = ["strength_reduce", "compute_arithmetics_decimal"]
compute_arithmetics_decimal = ["strength_reduce"]
compute_bitwise = []
compute_boolean = []
compute_boolean_kleene = []
compute_cast = ["lexical-core", "compute_take"]
compute_comparison = ["compute_take", "compute_boolean"]
compute_concatenate = []
compute_contains = []
compute_filter = []
compute_hash = ["multiversion"]
compute_if_then_else = []
compute_length = []
compute_like = ["regex", "regex-syntax", "dep:memchr"]
compute_limit = []
compute_merge_sort = ["itertools", "compute_sort"]
compute_nullif = ["compute_comparison"]
compute_partition = ["compute_sort"]
compute_regex_match = ["regex"]
compute_sort = ["compute_take"]
compute_substring = []
compute_take = []
compute_temporal = []
compute_utf8 = []
compute_window = ["compute_concatenate"]
default = []
full = [
  "arrow",
  "io_csv",
  "io_csv_async",
  "io_json",
  "io_ipc",
  "io_ipc_write_async",
  "io_ipc_read_async",
  "io_ipc_compression",
  "io_parquet_async",
  "io_parquet_compression",
  "regex",
  "regex-syntax",
  "compute",
  # parses timezones used in timestamp conversions
  "chrono-tz"
]
io_avro = ["avro-schema", "streaming-iterator"]
io_avro_async = ["avro-schema/async"]
io_avro_compression = ["avro-schema/compression"]
io_csv = ["io_csv_read", "io_csv_write"]
io_csv_async = ["io_csv_read_async"]
io_csv_read = ["csv", "lexical-core"]
io_csv_read_async = ["csv-async", "lexical-core", "futures"]
io_csv_write = ["csv-core", "streaming-iterator", "lexical-core"]
io_ipc = ["arrow-format"]
io_ipc_compression = ["lz4", "zstd"]
io_ipc_read_async = ["io_ipc", "futures", "async-stream"]
io_ipc_write_async = ["io_ipc", "futures"]
io_json = ["io_json_read", "io_json_write"]
# serde+serde_json: its dependencies + error handling
# serde_derive: there is some derive around
io_json_integration = ["hex", "serde_derive", "serde_json", "io_ipc"]
io_json_read = ["json-deserializer", "indexmap", "lexical-core"]
io_json_write = [
  "streaming-iterator",
  "fallible-streaming-iterator",
  "lexical-core"
]
io_orc = ["orc-format"]
# base64 + io_ipc because arrow schemas are stored as base64-encoded ipc format.
io_parquet = [
  "parquet2",
  "io_ipc",
  "base64",
  "streaming-iterator",
  "fallible-streaming-iterator"
]
io_parquet_async = ["futures", "io_parquet", "parquet2/async"]
# parquet bloom filter functions
io_parquet_bloom_filter = ["parquet2/bloom_filter"]
io_parquet_brotli = ["parquet2/brotli"]
io_parquet_compression = [
  "io_parquet_zstd",
  "io_parquet_gzip",
  "io_parquet_snappy",
  "io_parquet_lz4",
  "io_parquet_brotli"
]
io_parquet_gzip = ["parquet2/gzip"]
io_parquet_lz4 = ["parquet2/lz4"]
io_parquet_lz4_flex = ["parquet2/lz4_flex"]
# sample testing of generated arrow data
io_parquet_sample_test = ["io_parquet_async"]
io_parquet_snappy = ["parquet2/snappy"]
# compression backends
io_parquet_zstd = ["parquet2/zstd"]
io_print = ["comfy-table"]
serde_types = ["serde_derive"]
simd = []

[lib]
bench = false
name = "arrow2"

[package]
authors = [
  "Jorge C. Leitao <jorgecarleitao@gmail.com>",
  "Apache Arrow <dev@arrow.apache.org>"
]
description = "Unofficial implementation of Apache Arrow spec in safe Rust"
edition = "2021"
exclude = ["testing/"]
homepage = "https://github.com/jorgecarleitao/arrow2"
keywords = ["arrow", "analytics"]
license = "Apache-2.0"
name = "arrow2"
repository = "https://github.com/jorgecarleitao/arrow2"
version = "0.17.4"

[package.metadata.cargo-all-features]
allowlist = ["compute", "compute_sort", "compute_hash", "compute_nullif"]

[package.metadata.cargo-machete]
ignored = ["arrow_array", "arrow_buffer", "avro_rs", "criterion", "crossbeam_channel", "flate2", "getrandom", "rustc_version", "sample_arrow2", "sample_std", "sample_test", "tokio", "tokio_util"]

[target.wasm32-unknown-unknown.dependencies]
getrandom = {version = "0.2", features = ["js"]}
