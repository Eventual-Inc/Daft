{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3dd45a8",
   "metadata": {
    "id": "e3dd45a8"
   },
   "source": [
    "# MinHash Deduplication on Common-Crawl Web Text \n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Eventual-Inc/Daft/blob/main/tutorials/minhash_dedupe/minhash_dedupe_common_crawl.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "In this notebook we will be performing the MinHash Deduplication algorithm over extracted text from html documents in the common crawl dataset. The Common Crawl corpus contains petabytes of data, with its oldest entries dating back to 2008. Each dataset includes raw web page data, metadata extracts, and text extracts. Deduplication is a helpful top-of-funnel strategy for improving dataset quality and is commonly used to improve generalization in LLM training, RAG, and Search.\n",
    "\n",
    "When implemented as a pipeline, this workload can process 100,000 web pages in under 4 minutes on a MacBook Air (M2) with 8 GB of RAM. That includes preprocessing pages into html blocks, minhash, lsh banding, connected components, and final dedupe!\n",
    "\n",
    "```text\n",
    "# of documents loaded:  100000  \n",
    "# of text rows before:  4944669\n",
    "# of text rows after:   1855814\n",
    "% of text rows kept:    37.53%\n",
    "Overall Time:           222.21s\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "- [Connected Components in MapReduce and Beyond](https://dl.acm.org/doi/abs/10.1145/2670979.2670997)\n",
    "- [On the resemblance and containment of documents](https://ieeexplore.ieee.org/document/666900)\n",
    "- [Finding Near Duplicates with Jaccard Similarity and MinHash by Nelson Elhage](https://blog.nelhage.com/post/fuzzy-dedup/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10310f",
   "metadata": {
    "id": "ae10310f"
   },
   "source": [
    "## Table of Contents\n",
    "- [Quickstart](#quickstart)\n",
    "- [Loading Common Crawl](#loading-html-documents-from-common-crawl)\n",
    "- [Preprocessing](#preprocessing)\n",
    "- [Text Normalization](#text-normalization)\n",
    "- [Minhash](#minhash)\n",
    "- [LSH Banding](#lsh-banding)\n",
    "- [Connected Components](#connected-components)\n",
    "- [Validation with igraph](#validation-with-igraph)\n",
    "- [Merge Results](#merge-results)\n",
    "- [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c79153",
   "metadata": {
    "id": "41c79153"
   },
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c99119",
   "metadata": {
    "id": "b9c99119"
   },
   "outputs": [],
   "source": [
    "# Install additional dependencies\n",
    "# For Google Colab or standalone environments:\n",
    "!pip install 'daft[aws,pandas]' selectolax scipy matplotlib igraph\n",
    "\n",
    "# For local development with uv (temporary installation):\n",
    "# uv pip install 'daft[aws,pandas]' selectolax scipy matplotlib igraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940915ec",
   "metadata": {
    "id": "940915ec"
   },
   "source": [
    "### Define Key Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4bb7d2",
   "metadata": {
    "id": "ff4bb7d2"
   },
   "outputs": [],
   "source": [
    "# Pipeline\n",
    "NUM_ROWS = 500\n",
    "index_col = \"block_id\"\n",
    "content_col = \"block\"\n",
    "\n",
    "# Minhash\n",
    "K = 64  # Number of Permutations\n",
    "SEED = 42  # Seed for the hash function\n",
    "NGRAM_SIZE = 5  # Size of the n-grams\n",
    "LSH_THRESHOLD = 0.7  # Jaccard Similarity Threshold for LSH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7920348",
   "metadata": {
    "id": "e7920348"
   },
   "source": [
    "---\n",
    "\n",
    "## Loading HTML Documents from Common Crawl\n",
    "\n",
    "We will be accessing Common Crawl through [WARC files](https://commoncrawl.org/blog/navigating-the-warc-file-format) since [daft supports the format natively](https://docs.getdaft.io/en/stable/api/io/#daft.read_warc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058dd5a5",
   "metadata": {
    "id": "058dd5a5"
   },
   "source": [
    "#### (Optional) AWS Authentication\n",
    "\n",
    "Crawl data is free to access by anyone from anywhere. The data is hosted by Amazon Web Services’ Open Data Sets Sponsorships program on the bucket s3://commoncrawl/, located in the US-East-1 (Northern Virginia) AWS Region. The most performative means of accessing Common crawl is through s3, so  if you plan to process a lot of data you'll want to authenticate with a `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.\n",
    "\n",
    "However, Common Crawl data can also be accessed without authentication, anonymously via it's http endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869bfbf",
   "metadata": {
    "id": "0869bfbf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import daft\n",
    "from daft.io import IOConfig, S3Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aFfITKAhbSTV",
   "metadata": {
    "id": "aFfITKAhbSTV"
   },
   "outputs": [],
   "source": [
    "if os.environ.get(\"AWS_ACCESS_KEY_ID\"):\n",
    "    # Make sure to define your AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in your environment variables or in a .env file\n",
    "    s3_config = S3Config(\n",
    "        region_name=\"us-east-1\",\n",
    "        requester_pays=True,\n",
    "        key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "        access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        anonymous=False,\n",
    "    )\n",
    "\n",
    "    IO_CONFIG = IOConfig(s3=s3_config)\n",
    "    daft.set_planning_config(default_io_config=IO_CONFIG)\n",
    "\n",
    "    # Multifile access over s3\n",
    "    uri = \"s3://commoncrawl/crawl-data/CC-MAIN-2025-33/segments/*/warc/*.warc.gz\"\n",
    "else:\n",
    "    # For un-authenticated access over http\n",
    "    uri = \"https://data.commoncrawl.org/crawl-data/CC-MAIN-2025-33/segments/1754151279521.11/warc/CC-MAIN-20250802220907-20250803010907-00000.warc.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4980779",
   "metadata": {
    "id": "e4980779"
   },
   "outputs": [],
   "source": [
    "# Read the WARC files from the Common Crawl S3 bucket or HTTP endpoint\n",
    "df_warc = daft.read_warc(uri).limit(NUM_ROWS).collect()  # Materialize to avoid re-reading from source\n",
    "df_warc.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf049ad",
   "metadata": {
    "id": "3cf049ad"
   },
   "outputs": [],
   "source": [
    "# Lets investigate the different types of payloads we have:\n",
    "df_warc.select(\"WARC-Identified-Payload-Type\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95340f4b",
   "metadata": {
    "id": "95340f4b"
   },
   "source": [
    "## Preprocessing\n",
    "Since we are primarily concerned with text, we will focus on `text/html` payloads, extracting text content from html body and normalizing the text itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba1f6e",
   "metadata": {
    "id": "bfba1f6e"
   },
   "outputs": [],
   "source": [
    "from daft import col\n",
    "\n",
    "\n",
    "# Define a UDF to remove http headers from the payload\n",
    "@daft.func()\n",
    "def remove_http_headers(x: str) -> str:\n",
    "    \"\"\"Remove HTTP headers from input string by splitting on double CRLF, returning the body or empty string.\"\"\"\n",
    "    if x is None:\n",
    "        return \"\"\n",
    "    if len(x.split(\"\\r\\n\\r\\n\")) > 1:\n",
    "        return x.split(\"\\r\\n\\r\\n\")[1]\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "# Filter the dataframe to only include text/html payloads\n",
    "df_html = df_warc.where(col(\"WARC-Identified-Payload-Type\") == \"text/html\")\n",
    "\n",
    "# Create a Daft Expression to decode WARC content from bytes to UTF-8 and strip HTTP headers\n",
    "# Learn more about Daft Expressions: https://docs.daft.ai/en/stable/api/expressions/\n",
    "warc_without_headers_expr = remove_http_headers(col(\"warc_content\").try_decode(\"utf-8\"))\n",
    "\n",
    "# Separate the http headers from the payloads and filter out empty content\n",
    "df_html = df_html.with_column(\"content_raw\", warc_without_headers_expr).where(col(\"content_raw\") != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ab8f95",
   "metadata": {
    "id": "20ab8f95"
   },
   "source": [
    "#### Extracting Text from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c84c2",
   "metadata": {
    "id": "8d9c84c2"
   },
   "outputs": [],
   "source": [
    "from selectolax.parser import HTMLParser\n",
    "\n",
    "from daft import DataType as dt\n",
    "\n",
    "\n",
    "# Define a UDF to extract text from HTML content\n",
    "@daft.func()\n",
    "def extract_blocks(html: str) -> dt.list(dt.struct({\"txt\": dt.string(), \"tag\": dt.string()})):\n",
    "    \"\"\"Parse HTML using Selectolax, remove scripts/styles/noscripts, extract text blocks from key tags.\"\"\"\n",
    "    tree = HTMLParser(html)\n",
    "    for n in tree.css(\"script,style,noscript\"):\n",
    "        n.decompose()\n",
    "\n",
    "    all_nodes = tree.css(\n",
    "        \"\"\"title, article, main, p, h1, h2, h3, h4, h5, h6, li, div, section, img[alt], figcaption, caption, blockquote, table th, table td, pre, code, summary, meta[name=\"description\"], meta[property=\"og:title\"], meta[property=\"og:description\"]\"\"\"\n",
    "    )\n",
    "    blocks = [\n",
    "        {\"txt\": node.text(separator=\" \", strip=True), \"tag\": node.tag}\n",
    "        for node in all_nodes\n",
    "        if node.text(separator=\" \", strip=True)\n",
    "    ]\n",
    "    return blocks\n",
    "\n",
    "\n",
    "df_text = (\n",
    "    df_html.with_column(\"blocks\", extract_blocks(col(\"content_raw\")))\n",
    "    .explode(\"blocks\")\n",
    "    .where(col(\"blocks\")[\"txt\"] != \"\")\n",
    "    .where(col(\"blocks\")[\"txt\"].not_null())\n",
    "    .with_column(\n",
    "        index_col, col(\"WARC-Record-ID\") + \"-\" + col(\"blocks\")[\"tag\"] + \"-\" + col(\"blocks\")[\"txt\"].hash()\n",
    "    )  # Record ID + Tag + Text Hash\n",
    "    .with_column(content_col, col(\"blocks\")[\"txt\"])\n",
    ")\n",
    "df_text = df_text.collect()\n",
    "df_text.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d70c6f",
   "metadata": {
    "id": "49d70c6f"
   },
   "outputs": [],
   "source": [
    "# Drop Un-needed Columns\n",
    "df_ready = df_text.select(index_col, content_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ccbe4a",
   "metadata": {
    "id": "67ccbe4a"
   },
   "source": [
    "### Text Normalization\n",
    "\n",
    "So far we have extracted the text out of each html document into blocks. Now we move to normalize the text blocks to prepare for the MinHash operation.\n",
    "\n",
    "*Note: It is recommended to run your preprocessing pipeline seperately from your minhash deduplication workload.*\n",
    "\n",
    "See docs: [normalize](https://docs.daft.ai/en/stable/api/expressions/#daft.expressions.expressions.ExpressionStringNamespace.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8704a961",
   "metadata": {
    "id": "8704a961"
   },
   "outputs": [],
   "source": [
    "# Normalize text\n",
    "df_norm = df_ready.with_column(\n",
    "    \"content_normalized\",\n",
    "    col(content_col).str.normalize(remove_punct=True, lowercase=True, nfd_unicode=True, white_space=True),\n",
    ")\n",
    "df_norm.select(index_col, content_col, \"content_normalized\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f42632b",
   "metadata": {
    "id": "1f42632b"
   },
   "source": [
    "### MinHash\n",
    "\n",
    "Normally when you perform a minhash on text data, you have to define the shingling strategy, hash functions, and permutation parameters manually.\n",
    "\n",
    "Luckily, daft has a built in [minhash expression](https://docs.daft.ai/en/stable/api/expressions/#daft.expressions.Expression.minhash)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e127c",
   "metadata": {
    "id": "545e127c"
   },
   "outputs": [],
   "source": [
    "# Calculate the minhash vectors\n",
    "df_minhash = df_norm.with_column(\n",
    "    \"min_hashes\",\n",
    "    col(\"content_normalized\").minhash(num_hashes=K, ngram_size=NGRAM_SIZE, seed=SEED, hash_function=\"xxhash\"),\n",
    ")\n",
    "df_minhash.select(index_col, content_col, \"min_hashes\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7760f",
   "metadata": {
    "id": "4bf7760f"
   },
   "source": [
    "## LSH Banding\n",
    "\n",
    "LSH Banding involves splitting each document's MinHash signature into bands and rows, where documents with identical bands are considered candidate pairs for similarity comparison. This technique dramatically reduces the number of comparisons needed by only comparing documents that share at least one identical band, making near-duplicate detection scalable for large datasets\n",
    "\n",
    "Next, we will:\n",
    "1. Use the optimal_param function to determine the best band (b) and row (r) parameters for our LSH bucketing\n",
    "2. Split each document's minhash vector into `B` bands of `R` rows each\n",
    "3. Create buckets by hashing each band's signature, grouping similar documents together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d43ce7",
   "metadata": {
    "id": "37d43ce7"
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import quad as integrate\n",
    "\n",
    "\n",
    "def optimal_param(\n",
    "    threshold: float,\n",
    "    num_perm: int,\n",
    "    false_positive_weight: float = 0.5,\n",
    "    false_negative_weight: float = 0.5,\n",
    "):\n",
    "    \"\"\"Compute the optimal `MinHashLSH` parameter that minimizes the weighted sum of probabilities of false positive and false negative, taken from datasketch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    threshold : float\n",
    "        The threshold for similarity.\n",
    "    num_perm : int\n",
    "        The number of permutations.\n",
    "    false_positive_weight : float\n",
    "        The weight of false positive.\n",
    "    false_negative_weight : float\n",
    "        The weight of false negative.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    Tuple[int, int]\n",
    "        The optimal `b` and `r` parameters.\n",
    "        The number of bands, and the number of rows per band respectively.\n",
    "\n",
    "    Examples:\n",
    "    --------\n",
    "    >>> optimal_param(0.7, 256)\n",
    "    (25, 10)\n",
    "    \"\"\"\n",
    "\n",
    "    def false_positive_area(threshold: float, b: int, r: int):\n",
    "        \"\"\"Source: `datasketch.lsh`.\"\"\"\n",
    "\n",
    "        def area(s):\n",
    "            return 1 - (1 - s ** float(r)) ** float(b)\n",
    "\n",
    "        a, _ = integrate(area, 0.0, threshold)\n",
    "        return a\n",
    "\n",
    "    def false_negative_area(threshold: float, b: int, r: int):\n",
    "        \"\"\"Source: `datasketch.lsh`.\"\"\"\n",
    "\n",
    "        def area(s):\n",
    "            return 1 - (1 - (1 - s ** float(r)) ** float(b))\n",
    "\n",
    "        a, _ = integrate(area, threshold, 1.0)\n",
    "        return a\n",
    "\n",
    "    min_error = float(\"inf\")\n",
    "    opt = (0, 0)\n",
    "    for b in range(1, num_perm + 1):\n",
    "        max_r = int(num_perm / b)\n",
    "        for r in range(1, max_r + 1):\n",
    "            fp = false_positive_area(threshold, b, r)\n",
    "            fn = false_negative_area(threshold, b, r)\n",
    "            error = fp * false_positive_weight + fn * false_negative_weight\n",
    "            if error < min_error:\n",
    "                min_error = error\n",
    "                opt = (b, r)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cbe8f",
   "metadata": {
    "id": "035cbe8f"
   },
   "outputs": [],
   "source": [
    "# Choose B bands and R rows per band such that B · R = num_perm.\n",
    "B, R = optimal_param(LSH_THRESHOLD, K)  # Default 0.7 , 64\n",
    "print(B, R, K)\n",
    "\n",
    "# Verify that B * R = K\n",
    "assert B * R == K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb60c71a",
   "metadata": {
    "id": "eb60c71a"
   },
   "source": [
    "Before we move to Band Generation, we need to hash our `index_col` to `int` to make downstream processing easier.\n",
    "We will keep track of the map and introduce a new column with a monotonically increasing id.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02102f8",
   "metadata": {
    "id": "d02102f8"
   },
   "outputs": [],
   "source": [
    "from daft.functions import monotonically_increasing_id\n",
    "\n",
    "df_minhash = df_minhash.with_column(\"node_id\", monotonically_increasing_id())\n",
    "id_map = df_minhash.select(index_col, \"node_id\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a4f8fe",
   "metadata": {
    "id": "a0a4f8fe"
   },
   "source": [
    "### LSH Band Generation\n",
    "\n",
    "**Previously** we calculated the minhashes for our `content_text` where we hashed each word token into an 8 byte integer, taking only 64 samples (at a uniform random sample).\n",
    "\n",
    "**Next** we took those 64 hashes and chunked them into 8 lists of 8 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5428a97",
   "metadata": {
    "id": "d5428a97"
   },
   "outputs": [],
   "source": [
    "# Band Generation\n",
    "df_bands = df_minhash.with_column(\"bands\", col(\"min_hashes\").list.chunk(R))\n",
    "df_bands.select(index_col, content_col, \"min_hashes\", \"bands\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae2c77",
   "metadata": {
    "id": "aeae2c77"
   },
   "source": [
    "**Now** we will explode our bands into new rows, keeping track of their position in the band using `band_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2368c",
   "metadata": {
    "id": "8ed2368c"
   },
   "outputs": [],
   "source": [
    "@daft.func()\n",
    "def get_band_idx(band: list[int], B: int) -> list[int]:\n",
    "    return list(range(min(len(band), B)))\n",
    "\n",
    "\n",
    "df_bands_exploded = df_bands.with_column(\"band_idx\", get_band_idx(col(\"bands\"), B)).explode(\"bands\", \"band_idx\")\n",
    "df_bands_exploded.select(\"node_id\", \"band_idx\", \"bands\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca284ec1",
   "metadata": {
    "id": "ca284ec1"
   },
   "source": [
    "### Grouping bands\n",
    "We then group the bands against their 'signature', which is a combination of their band index and the band itself. If two segments are duplicates, we expect their signatures to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec1d86",
   "metadata": {
    "id": "59ec1d86"
   },
   "outputs": [],
   "source": [
    "# Grouping Bands\n",
    "df_grouped = df_bands_exploded.groupby(col(\"band_idx\"), col(\"bands\")).agg(col(\"node_id\").agg_list().alias(\"nodes\"))\n",
    "df_grouped.select(\"band_idx\", \"bands\", \"nodes\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0591e0f6",
   "metadata": {
    "id": "0591e0f6"
   },
   "outputs": [],
   "source": [
    "# Inspecting bands with multiple nodes\n",
    "df_grouped.where(col(\"nodes\").list.length() > 1).select(\"band_idx\", \"bands\", \"nodes\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04823a",
   "metadata": {
    "id": "3e04823a"
   },
   "source": [
    "## Connected Components\n",
    "Every band whose **nodes** have more than one entry are now candidates for consideration. But there is something wrong... Our nodes are repeated across different band indices!\n",
    "\n",
    "In order to reduce our candidates into their unique set, we leverage a few tricks from graph theory to isolate the duplicates. Here we get to implement one the most important algorithms in distributed computing. [*Connected Components in MapReduce and Beyond*](https://dl.acm.org/doi/pdf/10.1145/2670979.2670997) is a seminal paper from 2014 written by researchers at Google.\n",
    "\n",
    "We’ll follow the paper’s star‑contraction recipe: alternate a Large‑star and Small‑star pass that repeatedly points each node to the smallest ID in its neighborhood. After a few rounds the edge set stabilizes; the “parent” each node points to is its component representative.\n",
    "\n",
    "Concretely, we’ll collapse band groups into a simple graph:\n",
    "- Treat each document as a node.\n",
    "- For every band with multiple nodes, connect each node to the group’s minimum ID (drop self-loops and duplicates).\n",
    "- This produces an undirected edge list that captures “co-occurred somewhere” linkage.\n",
    "\n",
    "From there we use star-contraction (Kiveris et al., 2014) to snap clusters together:\n",
    "- Large-star: for each node, point to the smallest ID in its neighborhood (including itself). Emit edges (v, m(u)) only where v > u.\n",
    "- Small-star: canonicalize edges so u ≥ v, recompute the same “point to the minimum,” and emit (v, m(u)) for all neighbors.\n",
    "\n",
    "Repeat Large-star then Small-star until the edge set stops changing. The final “parent” each node points to is its component representative (typically the component’s minimum ID). It’s fast, scalable, and after a handful of rounds, the clusters just fall out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e262bf",
   "metadata": {
    "id": "f8e262bf"
   },
   "outputs": [],
   "source": [
    "# First we must convert our list of nodes into an edge list\n",
    "df_edges = (\n",
    "    df_grouped.with_column(\"u\", col(\"nodes\").list.min())\n",
    "    .explode(\"nodes\")\n",
    "    .select(\"u\", v=col(\"nodes\"))\n",
    "    .where(col(\"u\") != col(\"v\"))\n",
    "    .where(~col(\"u\").is_null())\n",
    "    .where(~col(\"v\").is_null())\n",
    "    .distinct()\n",
    ")\n",
    "df_edges.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4a7b4d",
   "metadata": {
    "id": "0b4a7b4d"
   },
   "source": [
    "#### First we need a few utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc89315",
   "metadata": {
    "id": "fcc89315"
   },
   "outputs": [],
   "source": [
    "from daft import DataFrame, Expression, struct\n",
    "\n",
    "\n",
    "def ee(u: Expression, v: Expression):\n",
    "    \"\"\"Create a struct Expression with fields 'u' and 'v' for representing edges.\"\"\"\n",
    "    return struct(u.alias(\"u\"), v.alias(\"v\"))\n",
    "\n",
    "\n",
    "def canonicalize(edges: DataFrame) -> DataFrame:\n",
    "    \"\"\"Order edges so u < v and deduplicate for canonical representation.\"\"\"\n",
    "    return (\n",
    "        edges.with_column(\"u_can\", (col(\"u\") < col(\"v\")).if_else(col(\"u\"), col(\"v\")))\n",
    "        .with_column(\"v_can\", (col(\"u\") < col(\"v\")).if_else(col(\"v\"), col(\"u\")))\n",
    "        .select(col(\"u_can\").alias(\"u\"), col(\"v_can\").alias(\"v\"))\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "\n",
    "def symmetrize(edges: DataFrame) -> DataFrame:\n",
    "    \"\"\"Make edge list undirected by adding reverse edges.\"\"\"\n",
    "    return edges.select(\"u\", \"v\").union_all(edges.select(col(\"v\").alias(\"u\"), col(\"u\").alias(\"v\"))).collect()\n",
    "\n",
    "\n",
    "def pairs_equal(a: DataFrame, b: DataFrame) -> bool:\n",
    "    \"\"\"Check if two DataFrames have identical (u, rep) pairs via anti-joins.\"\"\"\n",
    "    left_minus = a.join(b, on=[\"u\", \"rep\"], how=\"anti\").count_rows()\n",
    "    right_minus = b.join(a, on=[\"u\", \"rep\"], how=\"anti\").count_rows()\n",
    "    return (left_minus == 0) and (right_minus == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cd6ec",
   "metadata": {
    "id": "783cd6ec"
   },
   "source": [
    "### The Alternating Algorithm - Star Contraction with Daft\n",
    "We will iteratively compress the graph using two alternating phases until convergence:\n",
    "- Large-star: Every node points to the minimum ID in its neighborhood (including itself). This quickly pulls nodes toward low-ID “hubs.”\n",
    "- Small-star: Re-orient edges to ensure u < v (canonicalize) and repeat contraction, which merges local hubs together.\n",
    "- Repeat large-star then small-star until nothing changes. The “parent” each node ends up pointing to is its component representative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f20a18",
   "metadata": {
    "id": "04f20a18"
   },
   "source": [
    "### Large-star\n",
    "\n",
    "- Group neighbors by u.\n",
    "- Compute min_neighbor = min(neighbors).\n",
    "- Use min(u, min_neighbor) as the node’s “parent.”\n",
    "- Emit edges (u, parent) but only where parent > u to avoid self-loops and duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf247926",
   "metadata": {
    "id": "cf247926"
   },
   "outputs": [],
   "source": [
    "def large_star(edges: DataFrame) -> DataFrame:\n",
    "    \"\"\"Perform large-star operation: connect nodes to min in extended neighborhood.\"\"\"\n",
    "    # 1. Emit U,V and V,U\n",
    "    undirected = edges.select(\"u\", \"v\").union_all(edges.select(col(\"v\").alias(\"u\"), col(\"u\").alias(\"v\"))).collect()\n",
    "\n",
    "    # Step 2: Group by u, and aggregate the list of v's\n",
    "    neigh = undirected.groupby(\"u\").agg_list(\"v\").with_column(\"nbrs\", col(\"v\"))\n",
    "\n",
    "    # Step 3: Compute m = min over nbrs union {u}\n",
    "    neigh = neigh.with_column(\"m\", col(\"nbrs\").list.min())\n",
    "    neigh = neigh.with_column(\n",
    "        \"m\", col(\"m\").is_null().if_else(col(\"u\"), (col(\"u\") < col(\"m\")).if_else(col(\"u\"), col(\"m\")))\n",
    "    )\n",
    "\n",
    "    # Step 4: Emit (v, m(u)) for v > u\n",
    "    out = (\n",
    "        neigh.explode(\"nbrs\")\n",
    "        .where(col(\"nbrs\") > col(\"u\"))\n",
    "        .select(col(\"nbrs\").alias(\"u\"), col(\"m\").alias(\"v\"))\n",
    "        .where(col(\"u\") != col(\"v\"))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02bb5d4",
   "metadata": {
    "id": "c02bb5d4"
   },
   "source": [
    "### Small-star\n",
    "- Re-orient all edges so u < v (canonical).\n",
    "- Group neighbors by u, compute min_neighbor, connect (u, parent) like above.\n",
    "- This step merges local minima across previously separate stars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb865da0",
   "metadata": {
    "id": "fb865da0"
   },
   "outputs": [],
   "source": [
    "def small_star(edges: DataFrame) -> DataFrame:\n",
    "    \"\"\"Perform small-star operation: connect to min in direct smaller neighborhood.\"\"\"\n",
    "    # Step 1: For each edge, emit to the larger node as key, smaller as value\n",
    "    directed = (\n",
    "        edges.select((col(\"u\") < col(\"v\")).if_else(ee(col(\"u\"), col(\"v\")), ee(col(\"v\"), col(\"u\"))).alias(\"e\"))\n",
    "        .select(col(\"e\")[\"*\"])\n",
    "        .where(col(\"u\") != col(\"v\"))\n",
    "        .distinct()\n",
    "    )\n",
    "\n",
    "    # Step 2: Group by larger u, nbrs are smaller neighbors\n",
    "    neigh = directed.groupby(\"u\").agg_list(\"v\").with_column(\"nbrs\", col(\"v\"))\n",
    "\n",
    "    # Step 3: Compute m = min over nbrs union {u}\n",
    "    neigh = neigh.with_column(\"m\", col(\"nbrs\").list.min())\n",
    "    neigh = neigh.with_column(\n",
    "        \"m\", col(\"m\").is_null().if_else(col(\"u\"), (col(\"u\") < col(\"m\")).if_else(col(\"u\"), col(\"m\")))\n",
    "    )\n",
    "\n",
    "    # Emit (v, m(u)) for all v in N(u)\n",
    "    out = (\n",
    "        neigh.explode(\"nbrs\")\n",
    "        .select(col(\"nbrs\").alias(\"u\"), col(\"m\").alias(\"v\"))\n",
    "        .where(col(\"u\") != col(\"v\"))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758754a",
   "metadata": {
    "id": "b758754a"
   },
   "source": [
    "### Convergence check - Canonical Set Equality (strict)\n",
    "- Compare a stable summary of edges before/after\n",
    "- If stable, stop; otherwise repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed580ce",
   "metadata": {
    "id": "6ed580ce"
   },
   "outputs": [],
   "source": [
    "def check_canonical_set_equality(prev_edges: DataFrame, curr_edges: DataFrame) -> bool:\n",
    "    \"\"\"Check if two edge DataFrames represent the same set after canonicalization.\"\"\"\n",
    "    prev_can = canonicalize(prev_edges).collect().to_pydict()\n",
    "    curr_can = canonicalize(curr_edges).collect().to_pydict()\n",
    "    prev_set = set(zip(prev_can[\"u\"], prev_can[\"v\"]))\n",
    "    curr_set = set(zip(curr_can[\"u\"], curr_can[\"v\"]))\n",
    "    return prev_set == curr_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a1a3c3",
   "metadata": {
    "id": "87a1a3c3"
   },
   "source": [
    "### Full Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e841973",
   "metadata": {
    "id": "7e841973"
   },
   "outputs": [],
   "source": [
    "# The Alternating Algorithm\n",
    "b = df_edges\n",
    "while True:\n",
    "    a = large_star(b)\n",
    "    b_next = small_star(a)\n",
    "\n",
    "    if check_canonical_set_equality(b, b_next):\n",
    "        b = b_next\n",
    "        break\n",
    "    b = b_next\n",
    "\n",
    "b_final = b\n",
    "clear_output()  # cleans up the cell output for this operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dd3cb",
   "metadata": {
    "id": "904dd3cb"
   },
   "source": [
    "### Constructing Component Assignments\n",
    "\n",
    "After the alternating star operations converge, we have a **stable edge list** that implicitly defines connected components.  \n",
    "The final step is to turn this edge list into an explicit **assignment table**:  \n",
    "`[node_id → component_representative]`.\n",
    "\n",
    "We do this in three small, deterministic steps:\n",
    "\n",
    "1. **Collect every node** that appears in the graph (sources *and* destinations).  \n",
    "2. **For each node**, find the **smallest node ID** it is directly connected to (its tentative root).  \n",
    "   - Nodes with no outgoing edges simply become their own root.  \n",
    "3. **Materialize the result** as a DataFrame with columns `[\"u\", \"rep\"]`, where  \n",
    "   - `u` is the original node ID,  \n",
    "   - `rep` is the globally smallest node in its component (the canonical representative).\n",
    "\n",
    "This table is what we use to filter duplicates: keep only the row whose `index` equals its `rep`, discarding the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee4b32",
   "metadata": {
    "id": "d0ee4b32"
   },
   "outputs": [],
   "source": [
    "# Build the set of all unique node IDs that appear in the edge list\n",
    "# (both as source 'u' and destination 'v')\n",
    "nodes = (\n",
    "    b.select(col(\"u\").alias(\"u\"))  # grab all source nodes\n",
    "    .union_all(b.select(col(\"v\").alias(\"u\")))  # grab all destination nodes\n",
    "    .distinct()  # deduplicate to get unique nodes\n",
    ")\n",
    "\n",
    "# For every node, compute the smallest node ID it is connected to\n",
    "# (i.e., its tentative representative / root in the current component)\n",
    "rep_map = (\n",
    "    b.groupby(\"u\").agg(col(\"v\").min().alias(\"rep\"))  # group edges by source node  # find the smallest neighbor\n",
    ")\n",
    "\n",
    "# Join each node with its tentative representative.\n",
    "# Nodes that have no outgoing edges (and thus no entry in rep_map)\n",
    "# become their own representative.\n",
    "assignments = (\n",
    "    nodes.join(rep_map, on=\"u\", how=\"left\")  # left join to keep all nodes\n",
    "    .with_column(\n",
    "        \"rep\",\n",
    "        col(\"rep\")\n",
    "        .is_null()  # if no neighbor was found\n",
    "        .if_else(col(\"u\"), col(\"rep\")),  # use the node itself as rep\n",
    "    )\n",
    "    .select(\"u\", \"rep\")  # keep only node and its rep\n",
    "    .distinct()  # deduplicate any duplicates\n",
    "    .collect()  # materialize the result\n",
    ")\n",
    "\n",
    "assignments.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527cb5c2",
   "metadata": {
    "id": "527cb5c2"
   },
   "source": [
    "## Validation with igraph\n",
    "\n",
    "[igraph](https://python.igraph.org) is a high-performance graph analysis library that provides robust implementations of fundamental graph algorithms. We use it here as our ground truth for connected component detection because:\n",
    "\n",
    "- **Battle-tested**: igraph's connected components algorithm has been validated across millions of use cases\n",
    "- **Deterministic**: It guarantees consistent results regardless of node ordering or edge insertion sequence\n",
    "- **Efficient**: Handles large graphs with millions of nodes/edges using optimized C implementations\n",
    "- **Simple API**: Provides direct access to component membership without manual traversal\n",
    "By comparing our Daft-based implementation against igraph's results, we ensure our custom connected components logic correctly identifies all weakly connected subgraphs in the duplicate detection pipeline.\n",
    "\n",
    "See docs: [Connected Components](https://python.igraph.org/en/main/tutorials/connected_components.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbecf0d",
   "metadata": {
    "id": "0dbecf0d"
   },
   "outputs": [],
   "source": [
    "import igraph as ig\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure integer dtype and materialize edges\n",
    "pdf_edges = (\n",
    "    df_edges.select(col(\"u\").cast(daft.DataType.int64()), col(\"v\").cast(daft.DataType.int64()))\n",
    "    .where(~col(\"u\").is_null())\n",
    "    .where(~col(\"v\").is_null())\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "# Build explicit vertex list and index mapping to avoid dtype/label ambiguity\n",
    "unique_nodes = pd.unique(pd.concat([pdf_edges[\"u\"], pdf_edges[\"v\"]], ignore_index=True))\n",
    "\n",
    "# Convert to Python ints for stable hashing\n",
    "node_ids = [int(x) for x in unique_nodes.tolist()]\n",
    "id_to_idx = {nid: idx for idx, nid in enumerate(node_ids)}\n",
    "\n",
    "# Map edges to contiguous indices\n",
    "edges_idx = [(id_to_idx[int(u)], id_to_idx[int(v)]) for u, v in zip(pdf_edges[\"u\"], pdf_edges[\"v\"])]\n",
    "g = ig.Graph(n=len(node_ids), edges=edges_idx, directed=False)\n",
    "comps = g.connected_components(mode=\"weak\")\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c2898",
   "metadata": {
    "id": "324c2898"
   },
   "outputs": [],
   "source": [
    "# We can inspect the components to see how many there are and what they look like\n",
    "print(comps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b35cfd",
   "metadata": {
    "id": "70b35cfd"
   },
   "source": [
    "#### Visualizing what a connected component looks like (Top 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f12163e",
   "metadata": {
    "id": "6f12163e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Just grab the top 10 most connected nodes and their neighbors\n",
    "# Get component sizes and sort by number of nodes\n",
    "comp_sizes = [(i, len(comp)) for i, comp in enumerate(comps)]\n",
    "top_comp_indices = [i for i, _ in sorted(comp_sizes, key=lambda x: x[1], reverse=True)[:10]]\n",
    "\n",
    "# Get all nodes from these top components\n",
    "top_nodes = set()\n",
    "for comp_idx in top_comp_indices:\n",
    "    comp_nodes = comps[comp_idx]\n",
    "    top_nodes.update(comp_nodes)\n",
    "\n",
    "# Create subgraph from these nodes\n",
    "subgraph = g.subgraph(list(top_nodes))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "layout = subgraph.layout_fruchterman_reingold()\n",
    "ig.plot(\n",
    "    subgraph,\n",
    "    target=ax,\n",
    "    layout=layout,\n",
    "    vertex_size=5,\n",
    "    palette=ig.RainbowPalette(),\n",
    "    vertex_color=list(map(int, ig.rescale(comps.membership, (0, 250)))),\n",
    "    edge_width=0.5,\n",
    "    bbox=(1000, 1000),  # bigger drawing box\n",
    "    margin=40,\n",
    "    autocurve=True,  # curves to reduce edge overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704752bd",
   "metadata": {
    "id": "704752bd"
   },
   "source": [
    "### Validating Results against iGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cb2ed",
   "metadata": {
    "id": "a35cb2ed"
   },
   "outputs": [],
   "source": [
    "ours_grouped = assignments.groupby(\"rep\").agg(col(\"u\").agg_list().alias(\"members\")).collect()\n",
    "pdf = ours_grouped.to_pandas()\n",
    "ours_comps = {frozenset(m) for m in pdf[\"members\"]}\n",
    "ig_comps = {frozenset(node_ids[i] for i in comp) for comp in comps}\n",
    "if ours_comps == ig_comps:\n",
    "    print(f\"[VALIDATION] PASSED: components match igraph (n={len(ours_comps)})\")\n",
    "else:\n",
    "    only_ours = ours_comps - ig_comps\n",
    "    only_ig = ig_comps - ours_comps\n",
    "\n",
    "    def _preview(sets, k=3):\n",
    "        out = []\n",
    "        for comp in list(sets)[:k]:\n",
    "            out.append(sorted(list(int(c) for c in comp))[:10])\n",
    "        return out\n",
    "\n",
    "    print(f\"[VALIDATION] MISMATCH: ours={len(ours_comps)} vs igraph={len(ig_comps)}\")\n",
    "    print(f\"  examples only in ours: {_preview(only_ours)}\")\n",
    "    print(f\"  examples only in igraph: {_preview(only_ig)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f3cdde",
   "metadata": {
    "id": "44f3cdde"
   },
   "source": [
    "### Getting our results to match: Global minimum label propagation\n",
    "Why this is needed:\n",
    "- After alternating Large-/Small-Star and applying path compression, components can still\n",
    "    stabilize with multiple local minima (distinct labels) within the same true component.\n",
    "- This deterministic min-label diffusion ensures every node in a connected component adopts\n",
    "    the single global minimum node-id as its representative, restoring exact parity to igraph.\n",
    "\n",
    "**Algorithm:**\n",
    "1) Symmetrize edges to build an undirected adjacency (both directions present).\n",
    "2) Initialize labels(u) from assignments.rep.\n",
    "3) Iterate up to lp_max_iters times:\n",
    "    a) For each node, compute nbr_min(u) = min(label(v)) over neighbors v of u.\n",
    "    b) Update label(u) = min(label(u), nbr_min(u)) with null-safe handling.\n",
    "    c) Deduplicate and compare to prior labels; stop when the (u, label) pair set stabilizes.\n",
    "4) Return labels as assignments with schema [\"u\", \"rep\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe2dfd",
   "metadata": {
    "id": "ecfe2dfd"
   },
   "outputs": [],
   "source": [
    "# Build an undirected view of the graph so labels can flow in both directions\n",
    "E = symmetrize(b_final)\n",
    "\n",
    "# Initialize labels from current assignments: rep becomes the working label per node\n",
    "labels = assignments.select(col(\"u\"), col(\"rep\").alias(\"label\")).collect()\n",
    "\n",
    "lp_iters = 0\n",
    "lp_max_iters = 100\n",
    "while lp_iters < lp_max_iters:\n",
    "    lp_iters += 1\n",
    "\n",
    "    # For each node u, compute the minimum label among its neighbors\n",
    "    nbr_min = (\n",
    "        E.join(labels, left_on=\"v\", right_on=\"u\", how=\"left\")\n",
    "        .select(col(\"u\").alias(\"node\"), col(\"label\"))\n",
    "        .groupby(\"node\")\n",
    "        .agg(col(\"label\").min().alias(\"nbr_min\"))\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # Lower each node's label to min(current_label, neighbor_min_label)\n",
    "    labels_next = (\n",
    "        labels.join(nbr_min, left_on=\"u\", right_on=\"node\", how=\"left\")\n",
    "        .with_column(\n",
    "            \"label\",\n",
    "            col(\"nbr_min\")\n",
    "            .is_null()\n",
    "            .if_else(\n",
    "                col(\"label\"),\n",
    "                (col(\"label\") <= col(\"nbr_min\")).if_else(col(\"label\"), col(\"nbr_min\")),\n",
    "            ),\n",
    "        )\n",
    "        .select(col(\"u\"), col(\"label\"))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # Convergence: compare pair sets after casting back to (u, rep)\n",
    "    a = assignments.select(col(\"u\"), col(\"rep\").alias(\"label\")).select(col(\"u\"), col(\"label\").alias(\"rep\"))\n",
    "    b = labels_next.select(col(\"u\"), col(\"label\").alias(\"rep\"))\n",
    "    if pairs_equal(a, b):\n",
    "        break\n",
    "\n",
    "    # Continue iterating with updated assignments/labels\n",
    "    assignments = labels_next.select(col(\"u\"), col(\"label\").alias(\"rep\")).collect()\n",
    "    labels = labels_next\n",
    "\n",
    "assignments_globally_reduced = assignments\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9da21",
   "metadata": {
    "id": "8ae9da21"
   },
   "source": [
    "Checking one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1448ae",
   "metadata": {
    "id": "9e1448ae"
   },
   "outputs": [],
   "source": [
    "ours_grouped = assignments_globally_reduced.groupby(\"rep\").agg(col(\"u\").agg_list().alias(\"members\")).collect()\n",
    "pdf = ours_grouped.to_pandas()\n",
    "ours_comps = {frozenset(m) for m in pdf[\"members\"]}\n",
    "ig_comps = {frozenset(node_ids[i] for i in comp) for comp in comps}\n",
    "if ours_comps == ig_comps:\n",
    "    print(f\"[VALIDATION] PASSED: components match igraph (n={len(ours_comps)})\")\n",
    "else:\n",
    "    only_ours = ours_comps - ig_comps\n",
    "    only_ig = ig_comps - ours_comps\n",
    "\n",
    "    def _preview(sets, k=3):\n",
    "        out = []\n",
    "        for comp in list(sets)[:k]:\n",
    "            out.append(sorted(list(int(c) for c in comp))[:10])\n",
    "        return out\n",
    "\n",
    "    print(f\"[VALIDATION] MISMATCH: ours={len(ours_comps)} vs igraph={len(ig_comps)}\")\n",
    "    print(f\"  examples only in ours: {_preview(only_ours)}\")\n",
    "    print(f\"  examples only in igraph: {_preview(only_ig)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a51506",
   "metadata": {
    "id": "72a51506"
   },
   "source": [
    "## Merge Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c273376b",
   "metadata": {
    "id": "c273376b"
   },
   "outputs": [],
   "source": [
    "# First, create a mapping from node IDs to their string representations\n",
    "assignments_unique = assignments_globally_reduced.groupby(\"u\").agg(col(\"rep\").min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b0455",
   "metadata": {
    "id": "017b0455"
   },
   "outputs": [],
   "source": [
    "# Join the assignments with the ID mapping to get string representations\n",
    "a1 = assignments_unique.join(id_map.with_column_renamed(index_col, \"__u_str\"), left_on=\"u\", right_on=\"node_id\")\n",
    "a2 = a1.join(id_map.with_column_renamed(index_col, \"__rep_str\"), left_on=\"rep\", right_on=\"node_id\")\n",
    "assignments_unique_str = a2.select(col(\"__u_str\").alias(index_col), col(\"__rep_str\").alias(\"component\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7714fd",
   "metadata": {
    "id": "7d7714fd"
   },
   "outputs": [],
   "source": [
    "# Join back to original df and filter to keep only rows where the row is its own representative or isolated\n",
    "df_joined = df_text.join(assignments_unique_str, on=index_col, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26452bff",
   "metadata": {
    "id": "26452bff"
   },
   "outputs": [],
   "source": [
    "# Return the deduplicated dataset with only unique representatives\n",
    "deduplicated_df = (\n",
    "    df_joined.filter(col(\"component\").is_null() | (col(\"component\") == col(index_col))).exclude(\"component\")\n",
    ").collect()\n",
    "deduplicated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f80c501",
   "metadata": {
    "id": "8f80c501"
   },
   "outputs": [],
   "source": [
    "# Create a dataframe of the duplicates by filtering for rows that are NOT their own representative\n",
    "duplicates_df = (\n",
    "    df_joined.filter(col(\"component\").not_null() & (col(\"component\") != col(index_col)))\n",
    "    .where(col(\"block\") != \" \")\n",
    "    .exclude(\"component\")\n",
    ").collect()\n",
    "\n",
    "print(f\"Found {len(duplicates_df)} duplicate blocks\")\n",
    "print(\"\\nSample of duplicates:\")\n",
    "duplicates_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08557c7c",
   "metadata": {
    "id": "08557c7c"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we built an end‑to‑end, scalable deduplication pipeline for web text:\n",
    "\n",
    "- Ingested WARC from Common Crawl (S3) and extracted meaningful HTML blocks\n",
    "- Normalized text to a consistent, noise‑reduced representation\n",
    "- Computed MinHash signatures (K, ngram_size, seed) and bucketed with LSH (optimal B, R)\n",
    "- Collapsed buckets into a graph and found connected components via alternating Large‑/Small‑Star\n",
    "- Verified components against igraph and applied global minimum label propagation for a single representative per component\n",
    "- Produced both a deduplicated dataset and a duplicates table for inspection\n",
    "\n",
    "Why this is helpful/important\n",
    "- Reduces memorization and regurgitation in LLMs, improving generalization and safety\n",
    "- Eliminates redundant tokens to lower training cost and sharpen downstream evaluations\n",
    "- Transparent, parameterized method that scales with Daft and S3; easy to tune and reproduce\n",
    "\n",
    "Where to take it next\n",
    "- Your use case will most likely specializes in a specific domain or area of expertise so filter content for whats most relevent to you.\n",
    "- Experiment with Tuning K and the LSH similarity threshold to balance recall vs precision at your scale\n",
    "- Persist intermediate artifacts (minhashes, bands, edges) to accelerate iterations\n",
    "- Add domain/language filters and stronger boilerplate removal before hashing\n",
    "- Operationalize as a scheduled job over new Common Crawl snapshots\n",
    "- Integrate additional quality signals (toxicity, heuristics) pre‑/post‑dedup\n",
    "\n",
    "Outputs to expect\n",
    "- A deduplicated view (`deduplicated_df`) ready for downstream training\n",
    "- A `duplicates_df` sample to spot‑check clusters and validate quality"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
