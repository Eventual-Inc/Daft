{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8n0CLwwb5dZ"
   },
   "source": [
    "# Multimodal Structured Outputs with Daft, Gemma-3n, and vLLM\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Eventual-Inc/Daft/blob/main/tutorials/structured_outputs/Structured_Outputs_with_Gemma3_vLLM.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook walks through a practical example of evaluating model performance on structured generation and multimodal reasoning. We will explore how to scale multimodal image understanding evaluation using a combination of powerful technologies:\n",
    "\n",
    "1. Daft for data processing\n",
    "2. Gemma-3n-E2B model for multimodal capabilities\n",
    "3. vLLM/OpenRouter for efficient inference serving.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. Setup and Install Dependencies\n",
    "2. Launch a vLLM OpenAI API compatible server\n",
    "3. Testing the OpenAI client Gemma-3n client with an API key and new base url.\n",
    "4. Preprocess the AI2D dataset from HuggingFace's *TheCauldron* superset\n",
    "5. Multimodal Inference with Structured Outputs using 3 approaches\n",
    "6. Analysis of the inference results\n",
    "7. Putting everything together: Evaluating Gemma-3n-e4b-it across the AI2D subset\n",
    "8. Conclusion\n",
    "9. Appendix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8P5OPUCtIkO"
   },
   "source": [
    "## Quickstart\n",
    "\n",
    "You will be prompted to restart the session following installation, which simply clears local variables. If you ever need to kill the session or restart further into the notebook, you will not need to reinstall dependencies or authenticate with HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZuYcV6s7R43"
   },
   "outputs": [],
   "source": [
    "!pip install \"daft[huggingface]==0.6.1\" vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXBgajW4ssGZ"
   },
   "source": [
    " #### Login to HF for access gemma-3n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIC6JtSPseqB"
   },
   "outputs": [],
   "source": [
    "!hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtIxKYwIrjG7"
   },
   "source": [
    "---\n",
    "## OpenAI Compatible Online Serving \n",
    "\n",
    "### Option 1: Launch vLLM OpenAI Compatible Server\n",
    "\n",
    "Run the following vllm cli command in your terminal\n",
    "\n",
    "If you are in Google Colab, you can open a terminal by clicking the terminal icon in the bottom left of the ui.\n",
    "\n",
    "```bash\n",
    " python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model google/gemma-3n-e4b-it \\\n",
    "  --enable-chunked-prefill \\\n",
    "  --guided-decoding-backend guidance \\\n",
    "  --dtype bfloat16 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "* This config is optimized for Google Colab's A100 instance and gemma-3n-e4b-it. \n",
    "* It usually takes at least **7.5** minutes before the vLLM server is ready. \n",
    "* If you are in Google Colab, you can open a terminal by clicking the terminal icon in the bottom left of the ui.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Connect to a Provider\n",
    "\n",
    "Simply overwrite the `base_url` and `api_key` in the cell below!\n",
    "\n",
    "* Both OpenRouter and LMStudio have model support for google/gemma-3n-e4b-it.\n",
    "* Not all Gemma 3 series text-generation models support Image inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqjRsfxH891x"
   },
   "source": [
    "---\n",
    "### Verify OpenAI Client Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jKoQGBfQ8zqi"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"none\"\n",
    "base_url = \"http://0.0.0.0:8000/v1\"\n",
    "model_id = \"google/gemma-3n-e4b-it\"\n",
    "client = OpenAI(api_key=api_key, base_url=base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yL-XBvtFv0Q"
   },
   "outputs": [],
   "source": [
    "# Test Client model list contains `google/gemma-3n-e4b-it`\n",
    "result = client.models.list()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmywS9kiTZLO"
   },
   "outputs": [],
   "source": [
    "# Test Simple Text Completion\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What is the capital of the United States?\"}],\n",
    "    model=model_id,\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(\"Chat completion output: \\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "071agIW4Qg4n"
   },
   "outputs": [],
   "source": [
    "# Test Structured Output\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Classify this sentiment: Daft is wicked fast!\"}],\n",
    "    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjj5_JFQczgt"
   },
   "outputs": [],
   "source": [
    "# Test Image Understanding\n",
    "image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osXVHWQ7eHim"
   },
   "source": [
    "### Test Combining Image Inputs with Structured Output\n",
    "\n",
    "We can play with prompting/structured outputs to understand how prompting and structured outputs can affect results.\n",
    "\n",
    "Try commenting out the `extra_body` argument or the third user content text prompt to see how results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ril6tbiTeLn4"
   },
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=model_id,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Which insect is portrayed in the image: A. Ladybug, B. Beetle, C. Bee, D. Wasp \",\n",
    "                },\n",
    "                # {\"type\": \"text\", \"text\": \"Answer with only the letter from the multiple choice. \"} # Try comment me out\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},  # Try comment out\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhgBWDkm9RVy"
   },
   "source": [
    "---\n",
    "#  Step 1: Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfIxkMxf8Nc"
   },
   "source": [
    "### Prepping the [HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/viewer?views%5B%5D=ai2d) ,  Dataset for inference (ai2d subset)\n",
    "\n",
    "We can read directly from huggingface datasets by leveraging the `hf://` prefix in the url string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27J-inULb4kn"
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "# There are a total of 2,434 images in this dataset, at a size of ~ 500 MB\n",
    "df_raw = daft.read_parquet(\n",
    "    \"hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet\"\n",
    ").collect()\n",
    "df_raw.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEVwwhXDPohX"
   },
   "source": [
    " Taking a look at the schema we can see the familiar messages nested datatype we are used to in chat completions inside the `texts` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZHGfSd_74K9"
   },
   "outputs": [],
   "source": [
    "print(df_raw.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpxeWOz8P4pO"
   },
   "source": [
    "Lets decode the image bytes to see a preview of the images and add one more column for the base64 encoding. You can click on a cell to have a preview pop up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFmi01VkPmUA"
   },
   "outputs": [],
   "source": [
    "from daft import col\n",
    "\n",
    "df = df_raw.explode(col(\"images\")).with_columns(\n",
    "    {\n",
    "        \"image\": df_raw[\"images\"].struct.get(\"bytes\").image.decode(),\n",
    "        \"image_base64\": col(\"image\").encode(\"base64\"),\n",
    "    }\n",
    ")\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJFwixJpQTAn"
   },
   "source": [
    "#### Preprocessing the 'texts' column to extract Question, Choices, and Answer Columns\n",
    "\n",
    "Copy/Pasting an entry from the `texts` column yields an openai messages list of dicts of the form:\n",
    "\n",
    "```python\n",
    "[{\n",
    "    \"user\": \"\"\"Question:\n",
    "            \n",
    "        From the above food web diagram, what cause kingfisher to increase\n",
    "\n",
    "        Choices:\n",
    "            A. decrease in fish\n",
    "            B. decrease in water boatman\n",
    "            C. increase in fish\n",
    "            D. increase in algae\n",
    "\n",
    "        Answer with the letter.\"\"\",\n",
    "\n",
    "    \"assistant\": \"Answer: C\",\n",
    "    \"source\": \"AI2D\",\n",
    "}, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMW536eHUJ6Y"
   },
   "outputs": [],
   "source": [
    "# Explode the List of Dicts inside \"texts\" to extract \"user\" and \"assistant\" messages\n",
    "df = df.explode(col(\"texts\")).collect()\n",
    "\n",
    "# Extract User and Assistant Messages\n",
    "df = df.with_columns(\n",
    "    {\"user\": df[\"texts\"].struct.get(\"user\"), \"assistant\": df[\"texts\"].struct.get(\"assistant\")}\n",
    ").collect()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brCTE8koRhfb"
   },
   "source": [
    "We can also go above an beyond to parse each text input into individual question, choices, and answer columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUD64rVnkWYE"
   },
   "outputs": [],
   "source": [
    "# Parsing \"user\" and \"assistant\" messages for question, choices, and answer\"\"\n",
    "df_prepped = df.with_columns(\n",
    "    {\n",
    "        \"question\": df[\"user\"]\n",
    "        .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
    "        .str.replace(\"Choices:\", \"\")\n",
    "        .str.replace(\"Question:\", \"\"),\n",
    "        \"choices_string\": df[\"user\"]\n",
    "        .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
    "        .str.replace(\"Choices:\\n\", \"\")\n",
    "        .str.replace(\"Answer\", \"\"),\n",
    "        \"answer\": df[\"assistant\"].str.extract(r\"Answer:\\s*(.*)$\").str.replace(\"Answer:\", \"\"),\n",
    "    }\n",
    ").collect()\n",
    "\n",
    "df_prepped.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkVMvG9cg30_"
   },
   "source": [
    "---\n",
    "## Step 2: Multimodal Inference with Structured Outputs\n",
    "\n",
    "* Now we will move on to scaling our OpenAI client calls with Daft UDFs. \n",
    "* We will explore three methods of implementing structured outputs on images\n",
    "    * Naive Row-Wise UDF\n",
    "    * Naive Async Batch UDF\n",
    "    * Production Batch UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RUkXRwcJ99dL"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "model_id = \"google/gemma-3n-e4b-it\"\n",
    "api_key = \"none\"\n",
    "base_url = \"http://0.0.0.0:8000/v1\"\n",
    "client = AsyncOpenAI(api_key=api_key, base_url=base_url)\n",
    "row_limit = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7MGO3n-cZB3"
   },
   "source": [
    "### Minimal Row-Wise UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1hMVg3jbQw3"
   },
   "outputs": [],
   "source": [
    "@daft.func()\n",
    "async def struct_output_rowwise(model_id: str, text_col: str, image_col: str, extra_body: dict | None = None) -> str:\n",
    "    client = OpenAI(api_key=api_key, base_url=base_url)\n",
    "    content = [{\"type\": \"text\", \"text\": text_col}]\n",
    "    if image_col:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{image_col}\"},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    result = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        model=model_id,\n",
    "        extra_body=extra_body,\n",
    "    )\n",
    "    return result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4G_yjdNdayW"
   },
   "outputs": [],
   "source": [
    "from daft import col\n",
    "from daft.functions import format\n",
    "\n",
    "# Run the Rowwise UDF\n",
    "start = time.time()\n",
    "df_rowwise_udf = (\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        struct_output_rowwise(\n",
    "            model_id=model_id,\n",
    "            text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),\n",
    "            image_col=col(\"image_base64\"),\n",
    "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .limit(row_limit)\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"Row- wise UDF - Processed {df_rowwise_udf.count_rows()} rows in {end-start} seconds, {df_rowwise_udf.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGrFoxmyheX4"
   },
   "source": [
    "Write down each of your runs here:\n",
    "- Row-wise UDF - Processed ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILuaaIIbcbXG"
   },
   "source": [
    "### Minimal Async Batch UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WaluSJaQS1l"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from daft import DataType as dt\n",
    "\n",
    "\n",
    "@daft.udf(return_dtype=dt.string())\n",
    "def struct_output_batch(\n",
    "    model_id: str, text_col: daft.Series, image_col: daft.Series, extra_body: dict | None = None\n",
    ") -> list[str]:\n",
    "    async def generate(model_id: str, text: str, image: str) -> str:\n",
    "        content = [{\"type\": \"text\", \"text\": text}]\n",
    "        if image:\n",
    "            content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        result = await client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            model=model_id,\n",
    "            extra_body=extra_body,\n",
    "        )\n",
    "        return result.choices[0].message.content\n",
    "\n",
    "    texts = text_col.to_pylist()\n",
    "    images = image_col.to_pylist()\n",
    "\n",
    "    async def gather_completions() -> list[str]:\n",
    "        tasks = [generate(model_id, t, i) for t, i in zip(texts, images)]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    return asyncio.run(gather_completions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoHeK1XccrT7"
   },
   "outputs": [],
   "source": [
    "# 2. Run the Batch UDF\n",
    "start = time.time()\n",
    "df_batch_udf = (\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        struct_output_batch(\n",
    "            model_id=model_id,\n",
    "            text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "            image_col=col(\"image_base64\"),\n",
    "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .limit(row_limit)\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"Batch UDF - Processed {df_batch_udf.count_rows()} rows in {end-start} seconds, {df_batch_udf.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u4-JB8BhpWl"
   },
   "source": [
    "Write down each of your runs here:\n",
    "- Batch UDF - Processed ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apneVbIRhw4N"
   },
   "source": [
    "Before you move on to the Production UDF, try increasing the row_limit variable to 500, 1000, and 2000 rows.\n",
    "- What happens if you try to run the full dataset (7462 rows)?\n",
    "- How does row processing rate change when you increase the row_limit?\n",
    "- Do you run into any issues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB1Zn_nP8PPc"
   },
   "source": [
    "## Production UDF\n",
    "\n",
    "Here is what a more productionized version of our minimal user defined functions looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9XdLVpw-8im"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "concurrency = 4\n",
    "max_conn = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qR7GdUOM7-MV"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.string(), concurrency=concurrency, batch_size=batch_size)\n",
    "class StructuredOutputsProdUDF:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
    "        try:\n",
    "            self.loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(self.loop)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        text_col: daft.Series,\n",
    "        image_col: daft.Series,\n",
    "        sampling_params: dict[str, Any] | None = None,\n",
    "        extra_body: dict[str, Any] | None = None,\n",
    "    ) -> list[str]:\n",
    "        async def generate(text: str, image: str) -> str:\n",
    "            content = []\n",
    "            if image:\n",
    "                content.append(\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "                    }\n",
    "                )\n",
    "            if text:\n",
    "                content.append({\"type\": \"text\", \"text\": text})\n",
    "\n",
    "            result = await self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content,  # Dataset prefers image first\n",
    "                    }\n",
    "                ],\n",
    "                model=model_id,\n",
    "                extra_body=extra_body,\n",
    "                **sampling_params,\n",
    "            )\n",
    "            return result.choices[0].message.content\n",
    "\n",
    "        async def infer_with_semaphore(t, i):\n",
    "            return await generate(t, i)\n",
    "\n",
    "        async def gather_completions(texts, images) -> list[str]:\n",
    "            tasks = [infer_with_semaphore(t, i) for t, i in zip(texts, images)]\n",
    "            return await asyncio.gather(*tasks)\n",
    "\n",
    "        texts = text_col.to_pylist()\n",
    "        images = image_col.to_pylist()\n",
    "\n",
    "        return self.loop.run_until_complete(gather_completions(texts, images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eae4qPp-_hQI"
   },
   "outputs": [],
   "source": [
    "# 3. Production UDF\n",
    "start = time.time()\n",
    "df_prod_udf = (\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        StructuredOutputsProdUDF.with_init_args(\n",
    "            base_url=base_url,\n",
    "            api_key=api_key,\n",
    "        ).with_concurrency(concurrency)(\n",
    "            model_id=model_id,\n",
    "            text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "            image_col=col(\"image_base64\"),\n",
    "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .limit(row_limit)\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"Prod UDF - Processed {df_prod_udf.count_rows()} rows in {end-start} seconds, {df_prod_udf.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVgjnPjfkEcz"
   },
   "source": [
    "___\n",
    "# Analysis\n",
    "Evaluating Gemma-3's performance on image understanding by comparing structured output responses to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_GdTLRm_vgx"
   },
   "outputs": [],
   "source": [
    "pass_fail_rate = df_prod_udf.where(col(\"is_correct\")).count_rows() / df_prod_udf.count_rows()\n",
    "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nlo1hCzM1_l"
   },
   "outputs": [],
   "source": [
    "# How does this compare without images?\n",
    "# Here we will use Daft's native inference function llm_generate\n",
    "from daft.functions import llm_generate\n",
    "\n",
    "start = time.time()\n",
    "df_prod_no_img = (\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        llm_generate(\n",
    "            input_column=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "            model=model_id,\n",
    "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "            api_key=api_key,\n",
    "            base_url=base_url,\n",
    "            provider=\"openai\",\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"llm_generate - Processed {df_prod_no_img.count_rows()} rows in {end-start} seconds,  {df_prod_no_img.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2EJnsabBZpj"
   },
   "outputs": [],
   "source": [
    "pass_fail_rate_no_img = df_prod_no_img.where(col(\"is_correct\")).count_rows() / df_prod_no_img.count_rows()\n",
    "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eALCQifQPmfp"
   },
   "source": [
    "---\n",
    "# Putting everything together: Evaluating Gemma across the AI2D Dataset\n",
    "Now that we have walked through implementing this image understanding evaluation pipeline from end to end, lets put it all together so we can take full advantage of lazy evaluation and provide opportunities for future extensibility and re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TObFYF7xeKFJ"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from typing import Any\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import daft\n",
    "from daft import col\n",
    "from daft.functions import format\n",
    "\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.string(), concurrency=4)\n",
    "class StructuredOutputsProdUDF:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
    "        try:\n",
    "            self.loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(self.loop)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        text_col: daft.Series,\n",
    "        image_col: daft.Series,\n",
    "        sampling_params: dict[str, Any] | None = None,\n",
    "        extra_body: dict[str, Any] | None = None,\n",
    "    ):\n",
    "        async def generate(text: str, image: str) -> str:\n",
    "            content = []\n",
    "            if image:\n",
    "                content.append(\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "                    }\n",
    "                )\n",
    "            if text:\n",
    "                content.append({\"type\": \"text\", \"text\": text})\n",
    "\n",
    "            result = await self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content,  # Dataset prefers image first\n",
    "                    }\n",
    "                ],\n",
    "                model=model_id,\n",
    "                extra_body=extra_body,\n",
    "                **sampling_params,\n",
    "            )\n",
    "            return result.choices[0].message.content\n",
    "\n",
    "        async def infer_with_semaphore(t, i):\n",
    "            return await generate(t, i)\n",
    "\n",
    "        async def gather_completions(texts, images) -> list[str]:\n",
    "            tasks = [infer_with_semaphore(t, i) for t, i in zip(texts, images)]\n",
    "            return await asyncio.gather(*tasks)\n",
    "\n",
    "        texts = text_col.to_pylist()\n",
    "        images = image_col.to_pylist()\n",
    "\n",
    "        return self.loop.run_until_complete(gather_completions(texts, images))\n",
    "\n",
    "\n",
    "class TheCauldronImageUnderstandingEvaluationPipeline:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        dataset_uri: str,\n",
    "        sampling_params: dict[str, Any] | None = None,\n",
    "        concurrency: int = 4,\n",
    "        row_limit: int | None = None,\n",
    "        is_eager: bool = False,\n",
    "    ) -> daft.DataFrame:\n",
    "        \"\"\"Executes dataset loading, preprocessing, inference, and post-processing.\n",
    "\n",
    "        Evalutation must be run seperately since it requires materialization.\n",
    "        \"\"\"\n",
    "        if is_eager:\n",
    "            # Load Dataset and Materialize\n",
    "            df = self.load_dataset(dataset_uri)\n",
    "            df = df.limit(row_limit) if row_limit else df\n",
    "            df = self._log_processing_time(df)\n",
    "\n",
    "            # Preprocess\n",
    "            df = self.preprocess(df)\n",
    "            df = self._log_processing_time(df)\n",
    "\n",
    "            # Perform Inference\n",
    "            df = self.infer(df, model_id, sampling_params)\n",
    "            df = self._log_processing_time(df)\n",
    "\n",
    "            # Post-Process\n",
    "            df = self.postprocess(df)\n",
    "            df = self._log_processing_time(df)\n",
    "        else:\n",
    "            df = self.load_dataset(dataset_uri)\n",
    "            df = self.preprocess(df)\n",
    "            df = self.infer(df, model_id, sampling_params)\n",
    "            df = self.postprocess(df)\n",
    "            df = df.limit(row_limit) if row_limit else df\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _log_processing_time(df: daft.DataFrame):\n",
    "        start = time.time()\n",
    "        df_materialized = df.collect()\n",
    "        end = time.time()\n",
    "        num_rows = df_materialized.count_rows()\n",
    "        print(f\"Processed {num_rows} rows in {end-start} sec, {num_rows/(end-start)} rows/s\")\n",
    "        return df_materialized\n",
    "\n",
    "    def load_dataset(self, uri: str) -> daft.DataFrame:\n",
    "        return daft.read_parquet(uri)\n",
    "\n",
    "    def preprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
    "        # Convert png image byte string to base64\n",
    "        df = df.explode(col(\"images\")).with_column(\n",
    "            \"image_base64\",\n",
    "            df_raw[\"images\"]\n",
    "            .struct.get(\"bytes\")\n",
    "            .apply(lambda x: base64.b64encode(x).decode(\"utf-8\"), return_dtype=daft.DataType.string()),\n",
    "        )\n",
    "\n",
    "        # Explode Lists of User Prompts and Assistant Answer Pairs\n",
    "        df = df.explode(col(\"texts\")).with_columns(\n",
    "            {\"user\": df[\"texts\"].struct.get(\"user\"), \"assistant\": df[\"texts\"].struct.get(\"assistant\")}\n",
    "        )\n",
    "\n",
    "        # Parse the Question/Answer Strings\n",
    "        df = df.with_columns(\n",
    "            {\n",
    "                \"question\": df[\"user\"]\n",
    "                .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
    "                .str.replace(\"Choices:\", \"\")\n",
    "                .str.replace(\"Question:\", \"\"),\n",
    "                \"choices_string\": df[\"user\"]\n",
    "                .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
    "                .str.replace(\"Choices:\\n\", \"\")\n",
    "                .str.replace(\"Answer\", \"\"),\n",
    "                \"answer\": df[\"assistant\"].str.extract(r\"Answer:\\s*(.*)$\").str.replace(\"Answer:\", \"\"),\n",
    "            }\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        df: daft.DataFrame,\n",
    "        model_id: str = \"google/gemma-3n-e4b-it\",\n",
    "        sampling_params: dict[str, Any] = {\"temperature\": 0.0},\n",
    "        concurrency: int = 4,\n",
    "        extra_body: dict[str, Any] = {\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "    ) -> daft.DataFrame:\n",
    "        return df.with_column(\n",
    "            \"result\",\n",
    "            StructuredOutputsProdUDF.with_init_args(\n",
    "                base_url=self.base_url,\n",
    "                api_key=self.api_key,\n",
    "            ).with_concurrency(concurrency)(\n",
    "                model_id=model_id,\n",
    "                text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "                image_col=col(\"image_base64\"),\n",
    "                sampling_params=sampling_params,\n",
    "                extra_body=extra_body,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def postprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
    "        df = df.with_column(\n",
    "            \"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip()\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, df: daft.DataFrame) -> float:\n",
    "        pass_fail_rate = df.where(col(\"is_correct\")).count_rows() / df.count_rows()\n",
    "        return pass_fail_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A98SLoXALhxR"
   },
   "outputs": [],
   "source": [
    "# Our entire pipeline collapses into a three lines\n",
    "dataset_uri = \"hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet\"\n",
    "pipeline = TheCauldronImageUnderstandingEvaluationPipeline(api_key=\"none\", base_url=\"http://0.0.0.0:8000/v1\")\n",
    "df = pipeline(model_id=\"google/gemma-3n-e4b-it\", sampling_params={\"temperature\": 0.1}, is_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFdEC8JDMj8E"
   },
   "outputs": [],
   "source": [
    "# Materialize if not eager\n",
    "df_mat = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0v6x19XxNyJa"
   },
   "outputs": [],
   "source": [
    "# Print the Pass/Fail Rate\n",
    "print(f\"Pass/Fail Rate: {pipeline.evaluate(df_mat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbzOoC62uDu1"
   },
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we explored how to evaluate Gemma-3's image understanding using a subset from HuggingFace's TheCauldron Dataset. The AI2D subset we used is just one of a massive collection of 50 vision-language datasets that can be used for evaluating or training vision language models totaling millions of rows. You can also leverage this pipeline to evaluate model performance across sampling parameters or model variants. Please note that not all Gemma-3 series models support image inputs, and leveraging datasets outside of the TheCauldron would require different preprocessing stages.\n",
    "\n",
    "A natural next step would be to parallelize this pipeline across multiple datasets leveraging multiple gpus. In this scenario, I recommend transitioning daft's execution context to leverage Ray, a distributed compute framework.\n",
    "\n",
    "```bash\n",
    "pip install \"daft[huggingface,ray]\"\n",
    "```\n",
    "\n",
    "You can set daft's execution context to ray adding the `ray` optional dependency during installation and running the following at the top of your script.\n",
    "\n",
    "```python\n",
    "import daft\n",
    "\n",
    "daft.set_runner_ray()\n",
    "```\n",
    "\n",
    "Simply run your pipeline across each dataset uri and collect the results, Daft will orchestrate ray in the background for you. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gqjRsfxH891x",
    "J_wIalJJs0ki"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
