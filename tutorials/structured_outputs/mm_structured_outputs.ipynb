{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8n0CLwwb5dZ"
   },
   "source": [
    "# Multimodal Structured Outputs with Daft, Gemma-3n, and vLLM\n",
    "\n",
    "*An end-to-end example of **Multimodal Structured Outputs** with Daft's high performance data engine.*\n",
    "\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Eventual-Inc/Daft/blob/main/tutorials/structured_outputs/mm_structured_outputs.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Structured Outputs** refers to a family of features that enables LLMs to respond in a constrained format. While LLMs continue to improve and demonstrate emergent abilities, their dynamic nature make them difficult to integrate them with traditional software systems. Almost all emergent AI uses cases from agents to synthetic data and knowledge extraction leverage structured outputs, whether that be to execute tool calls or adhere to Pydantic Models. \n",
    "\n",
    "Structured Outputs strategies consist of 5 strategies that define the desired output type: \n",
    "\n",
    "- Basic Python Types: `int`, `float`, `bool`...\n",
    "- Multiple Choices: using `Literal` or `Enum`\n",
    "- JSON Schemas: using Pydantic models or dataclasses\n",
    "- Regex\n",
    "- Context-free Grammars\n",
    "\n",
    "While there are tremendous number of examples in pure python, few tutorials exist demonstrate structured outputs in distributed batch processing context. Even fewer, if any, examples exist that demonstrate how to run batch structured outputs with multimodal data. Here we will take things one step further - using your own OpenAI-compatible server.  \n",
    "\n",
    "This is a real use-case that every enterprise team faces when attempting to work with massive amounts of internal/private data. These teams face significant hurdles with traditional tooling, especially for cutting-edge uses cases like batch tool calls for background agents or reinforcement learning with verifiable rewards. \n",
    "\n",
    "Daft's unified multimodal data processing engine is purpose built to support workloads like this and is rapidly becoming the default engine of choice for teams deploying frontier AI solutions in production.\n",
    "\n",
    "In this notebook, we will leverage daft to accomplish to evaluate the image understanding accuracy of Gemma‑3n‑e4b‑it using the AI2D dataset. By the end of this notebook, you will be ready to implement your own distributed batch structured outputs pipeline with a copy-paste script you can use in your own environment. \n",
    "\n",
    "NOTE: This Notebook contains an advanced path where you can use vLLM as your inference solution. In this case, Google Colab's A100 GPU instance is recommended. Additionally, in order to access to [google/gemma-3n-e4b-it](https://huggingface.co/google/gemma-3n-E4B-it) you will need accept Google's usage policy and authenticate with HuggingFace.\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Setup](#1-setup) \n",
    "2. [Choose an Inference Solution](#2-choose-an-inference-solution)\n",
    "3. [Test OpenAI client Requests](#3-sanity-check-openai-client-requests)\n",
    "4. [Dataset Preprocessing](#4-dataset-preprocessing)\n",
    "5. [Multimodal Structured Outputs](#5-multimodal-inference-with-structured-outputs)\n",
    "6. Post Processing and Analysis \n",
    "7. Evaluation \n",
    "8. Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8P5OPUCtIkO"
   },
   "source": [
    "## 1. Setup \n",
    "\n",
    "### Install Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install  \"daft[huggingface]>=0.6.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Dataset\n",
    "MODEL_ID = \"google/gemma-3-4b-it\"  # OpenRouter free version -> google/gemma-3-4b-it:free\n",
    "DATASET_URI = \"hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet\"\n",
    "IMAGE_URL = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "\n",
    "# Inference Parameters\n",
    "ROW_LIMIT = 100\n",
    "TEMPERATURE = 0.1\n",
    "CONCURRENCY = 4\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Choose an Inference Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtIxKYwIrjG7"
   },
   "source": [
    "### Option 1: Connect to an Inference Provider\n",
    "\n",
    "* Both OpenRouter and LMStudio have model support for google/gemma-3n-e4b-it.\n",
    "* Export OPENAI_API_KEY and OPENAI_BASE_URL for your chosen inference provider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Launch vLLM OpenAI Compatible Server (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install vLLM\n",
    "\n",
    "After you install vllm you will be prompted to restart the session, then proceed to the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZuYcV6s7R43"
   },
   "outputs": [],
   "source": [
    "!pip install -q vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXBgajW4ssGZ"
   },
   "source": [
    "#### Log in to HF for access google/gemma-3n-e4b-it\n",
    "\n",
    "The [google/gemma-3n-e4b-it repository](https://huggingface.co/google/gemma-3n-E4B-it) is publicly accessible, but will need to login to HuggingFace and accept Google's conditions to access its files and content. Requests are processed immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIC6JtSPseqB"
   },
   "outputs": [],
   "source": [
    "!hf auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch vLLM OpenAI Compatible Server\n",
    "\n",
    "Run the following vllm cli command in your terminal\n",
    "\n",
    "If you are in Google Colab, you can open a terminal by clicking the terminal icon in the bottom left of the ui.\n",
    "\n",
    "```bash\n",
    " python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model google/gemma-3n-e4b-it \\\n",
    "  --enable-chunked-prefill \\\n",
    "  --guided-decoding-backend guidance \\\n",
    "  --dtype bfloat16 \\\n",
    "  --gpu-memory-utilization 0.85 \\\n",
    "  --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "* This config is optimized for Google Colab's A100 instance and gemma-3n-e4b-it. \n",
    "* For vLLM online serving, set `api_key = \"none\"` and `base_url = \"http://0.0.0.0:8000/v1\"`\n",
    "* Server readiness may take ~7–8 minutes; ‘guided_choice’ requires guided decoding enabled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqjRsfxH891x"
   },
   "source": [
    "## 3. Sanity Check OpenAI Client Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export your API key and base url environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export OPENAI_API_KEY=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 && export OPENAI_BASE_URL=https://openrouter.ai/api/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# OpenAI Client Environment Variables\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"sk-or-v1-...\")\n",
    "OPENAI_BASE_URL = os.environ.get(\"OPENAI_BASE_URL\", \"https://openrouter.ai/api/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yL-XBvtFv0Q"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "\n",
    "# Test Client connects to Server\n",
    "result = client.models.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FmywS9kiTZLO"
   },
   "outputs": [],
   "source": [
    "# Test Simple Text Completion\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"How many strawberries are in the word r?\"}],\n",
    "    model=MODEL_ID,\n",
    ")\n",
    "\n",
    "result = chat_completion.choices[0].message.content\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "071agIW4Qg4n"
   },
   "outputs": [],
   "source": [
    "# Test Structured Output\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Classify this sentiment: Daft is wicked fast!\"}],\n",
    "    extra_body={\"guided_choice\": [\"positive\", \"negative\"]},\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjj5_JFQczgt"
   },
   "outputs": [],
   "source": [
    "# Test Image Understanding\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
    "                {\"type\": \"text\", \"text\": \"Describe this image in detail.\"},\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osXVHWQ7eHim"
   },
   "source": [
    "### Test Combining Image Inputs with Structured Output\n",
    "\n",
    "We can play with prompting/structured outputs to understand how prompting and structured outputs can affect results.\n",
    "\n",
    "Try commenting out the `response_model` argument or the third text prompt to see how results change.\n",
    "\n",
    "vLLM also supports a simpler usage pattern of `extra_body={guided_choice:[\"A\",\"B\",\"C\",\"D\"]}`, but for compatibility with OpenRouter we use the Pydantic Json Schema approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ril6tbiTeLn4"
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "# Define a Pydantic Model for the Choice Response (overkill)\n",
    "class Choices(str, Enum):\n",
    "    A = \"A\"\n",
    "    B = \"B\"\n",
    "    C = \"C\"\n",
    "    D = \"D\"\n",
    "\n",
    "\n",
    "class ChoiceResponse(BaseModel):\n",
    "    choice: Choices = Field(..., description=\"Provide the letter of the correct choice with no other text.\")\n",
    "\n",
    "\n",
    "# Test Image Understanding\n",
    "completion = client.chat.completions.create(\n",
    "    model=MODEL_ID,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": IMAGE_URL}},\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Which insect is portrayed in the image: A. Ladybug, B. Beetle, C. Bee, D. Wasp \",\n",
    "                },\n",
    "                # {\"type\": \"text\", \"text\": \"Answer with only the letter from the multiple choice. \"} # Try comment me out\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"math-response\",\n",
    "            \"schema\": ChoiceResponse.model_json_schema(),\n",
    "        },\n",
    "    },\n",
    ")\n",
    "response = completion.choices[0].message.content\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Valiation\n",
    "choice_obj = ChoiceResponse.model_validate_json(response)\n",
    "print(choice_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYfIxkMxf8Nc"
   },
   "source": [
    "## 4: Dataset Preprocessing\n",
    "\n",
    "[HuggingFaceM4/the_cauldron](https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/viewer?views%5B%5D=ai2d) is a massive collection of 50 vision-language datasets that were used for the fine-tuning of the vision-language model Idefics2. We will use the AI2D subset to develop and test our pipeline. \n",
    "\n",
    "We can begin by reading directly from huggingface datasets, using the `hf://` prefix in the url string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27J-inULb4kn"
   },
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "# There are a total of 2,434 images in this dataset, at a size of ~ 500 MB\n",
    "# DATASET_URI =\"hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet\"\n",
    "df_raw = daft.read_parquet(DATASET_URI).limit(ROW_LIMIT).collect()\n",
    "df_raw.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEVwwhXDPohX"
   },
   "source": [
    " Taking a look at the schema we can see the familiar messages nested datatype we are used to in chat completions inside the `texts` column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZHGfSd_74K9"
   },
   "outputs": [],
   "source": [
    "print(df_raw.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpxeWOz8P4pO"
   },
   "source": [
    "Lets decode the image bytes to see a preview of the images and add one more column for the base64 encoding. \n",
    "\n",
    "Note: You can click on any cell to preview its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFmi01VkPmUA"
   },
   "outputs": [],
   "source": [
    "from daft import col\n",
    "\n",
    "df_img = df_raw.explode(col(\"images\")).with_columns(\n",
    "    {\n",
    "        \"image\": col(\"images\").struct.get(\"bytes\").image.decode(),\n",
    "        \"image_base64\": col(\"images\").struct.get(\"bytes\").encode(\"base64\"),\n",
    "    }\n",
    ")\n",
    "df_img.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KJFwixJpQTAn"
   },
   "source": [
    "#### Preprocessing the 'texts' column to extract Question, Choices, and Answer Columns\n",
    "\n",
    "Copy/Pasting an entry from the `texts` column yields an openai messages list of dicts of the form:\n",
    "\n",
    "```python\n",
    "[{\n",
    "    \"user\": \"\"\"Question:\n",
    "            \n",
    "        From the above food web diagram, what cause kingfisher to increase\n",
    "\n",
    "        Choices:\n",
    "            A. decrease in fish\n",
    "            B. decrease in water boatman\n",
    "            C. increase in fish\n",
    "            D. increase in algae\n",
    "\n",
    "        Answer with the letter.\"\"\",\n",
    "\n",
    "    \"assistant\": \"Answer: C\",\n",
    "    \"source\": \"AI2D\",\n",
    "}, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMW536eHUJ6Y"
   },
   "outputs": [],
   "source": [
    "# Explode the List of Dicts inside \"texts\" to extract \"user\" and \"assistant\" messages\n",
    "df_text = df_img.explode(col(\"texts\")).collect()\n",
    "\n",
    "# Extract User and Assistant Messages\n",
    "df_text = df_text.with_columns(\n",
    "    {\"user\": df_text[\"texts\"].struct.get(\"user\"), \"assistant\": df_text[\"texts\"].struct.get(\"assistant\")}\n",
    ").collect()\n",
    "df_text.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brCTE8koRhfb"
   },
   "source": [
    "We can also go above an beyond to parse each text input into individual question, choices, and answer columns.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUD64rVnkWYE"
   },
   "outputs": [],
   "source": [
    "# Parsing \"user\" and \"assistant\" messages for question, choices, and answer\"\"\n",
    "df_prepped = df_text.with_columns(\n",
    "    {\n",
    "        \"question\": col(\"user\")\n",
    "        .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
    "        .str.replace(\"Choices:\", \"\")\n",
    "        .str.replace(\"Question:\", \"\"),\n",
    "        \"choices_string\": col(\"user\")\n",
    "        .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
    "        .str.replace(\"Choices:\\n\", \"\")\n",
    "        .str.replace(\"Answer\", \"\"),\n",
    "        \"answer\": col(\"assistant\").str.extract(r\"Answer:\\s*(.*)$\").str.replace(\"Answer:\", \"\"),\n",
    "    }\n",
    ").collect()\n",
    "\n",
    "df_prepped.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkVMvG9cg30_"
   },
   "source": [
    "## 5. Multimodal Inference with Structured Outputs\n",
    "\n",
    "Now we will move on to scaling our OpenAI client calls with Daft UDFs, exploring three methods of implementing structured outputs on images:\n",
    "1. Naive Row-Wise UDF\n",
    "2. Naive Async Batch UDF\n",
    "3. Production Batch UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7MGO3n-cZB3"
   },
   "source": [
    "### Minimal Row-Wise UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j1hMVg3jbQw3"
   },
   "outputs": [],
   "source": [
    "@daft.func()\n",
    "async def struct_output_rowwise(\n",
    "    model_id: str, text_col: str, image_col: str, model_json_schema: dict | None = None\n",
    ") -> str:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "\n",
    "    content = [{\"type\": \"text\", \"text\": text_col}]\n",
    "\n",
    "    if image_col:\n",
    "        content.append(\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{image_col}\"},\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if model_json_schema:\n",
    "        response_format = {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"math-response\",\n",
    "                \"schema\": ChoiceResponse.model_json_schema(),\n",
    "            },\n",
    "        }\n",
    "\n",
    "    result = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        model=model_id,\n",
    "        response_format=response_format,\n",
    "    )\n",
    "    return result.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4G_yjdNdayW"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from daft import col\n",
    "from daft.functions import format\n",
    "\n",
    "# Run the Rowwise UDF\n",
    "start = time.time()\n",
    "df_rowwise_udf = (\n",
    "    # Inference\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        struct_output_rowwise(\n",
    "            model_id=MODEL_ID,\n",
    "            text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),\n",
    "            image_col=col(\"image_base64\"),\n",
    "            model_json_schema=ChoiceResponse.model_json_schema(),\n",
    "        ),\n",
    "    )\n",
    "    # Postprocessing\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .limit(ROW_LIMIT)\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"Row-wise UDF - Processed {df_rowwise_udf.count_rows()} rows in {end-start} seconds, {df_rowwise_udf.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGrFoxmyheX4"
   },
   "source": [
    "Write down each of your runs here:\n",
    "- Row-wise UDF - Processed ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILuaaIIbcbXG"
   },
   "source": [
    "### Minimal Async Batch UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WaluSJaQS1l"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "from daft import DataType as dt\n",
    "\n",
    "client = AsyncOpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "\n",
    "\n",
    "@daft.udf(return_dtype=dt.string())\n",
    "def struct_output_batch(\n",
    "    model_id: str,\n",
    "    text_col: daft.Series,\n",
    "    image_col: daft.Series,\n",
    "    model_json_schema: dict | None = None,\n",
    "    extra_body: dict | None = None,\n",
    ") -> list[str]:\n",
    "    # Nested Async Function\n",
    "    async def generate(model_id: str, text: str, image: str) -> str:\n",
    "        content = [{\"type\": \"text\", \"text\": text}]\n",
    "\n",
    "        # Argument Handling\n",
    "        if image:\n",
    "            content.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "                }\n",
    "            )\n",
    "\n",
    "        if model_json_schema:\n",
    "            response_format = {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"math-response\",\n",
    "                    \"schema\": model_json_schema,\n",
    "                },\n",
    "            }\n",
    "\n",
    "        # Inference\n",
    "        result = await client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": content}],\n",
    "            model=model_id,\n",
    "            response_format=response_format,\n",
    "            extra_body=extra_body,\n",
    "        )\n",
    "        return result.choices[0].message.content\n",
    "\n",
    "    # Input Handling\n",
    "    texts = text_col.to_pylist()\n",
    "    images = image_col.to_pylist()\n",
    "\n",
    "    # Async\n",
    "    async def gather_completions() -> list[str]:\n",
    "        tasks = [generate(model_id, t, i) for t, i in zip(texts, images)]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "    return asyncio.run(gather_completions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoHeK1XccrT7"
   },
   "outputs": [],
   "source": [
    "# 2. Run the Batch UDF\n",
    "start = time.time()\n",
    "df_batch_udf = (\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        struct_output_batch(\n",
    "            model_id=MODEL_ID,\n",
    "            text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "            image_col=col(\"image_base64\"),\n",
    "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .limit(ROW_LIMIT)\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"Batch UDF - Processed {df_batch_udf.count_rows()} rows in {end-start} seconds, {df_batch_udf.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u4-JB8BhpWl"
   },
   "source": [
    "Write down each of your runs here:\n",
    "- Batch UDF - Processed ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apneVbIRhw4N"
   },
   "source": [
    "#### Challenge\n",
    "Before you move on to the Production UDF, try increasing the ROW_LIMIT to 500, 1000, and 2000 rows.\n",
    "- What happens if you try to run the full dataset (7462 rows)?\n",
    "- How does row processing rate change when you increase the row_limit?\n",
    "- Do you run into any issues?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB1Zn_nP8PPc"
   },
   "source": [
    "## Production UDF\n",
    "\n",
    "Here is what a production version of our minimal user defined functions looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9XdLVpw-8im"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CONCURRENCY = 4\n",
    "MAX_CONN = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qR7GdUOM7-MV"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.string(), concurrency=CONCURRENCY, batch_size=BATCH_SIZE)\n",
    "class StructuredOutputsProdUDF:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
    "\n",
    "        # Handle Event Loop Exhaustion\n",
    "        try:\n",
    "            self.loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(self.loop)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        text_col: daft.Series,\n",
    "        image_col: daft.Series,\n",
    "        sampling_params: dict[str, Any] | None = None,\n",
    "        model_json_schema: dict | None = None,\n",
    "        extra_body: dict[str, Any] | None = None,\n",
    "    ) -> list[str]:\n",
    "        # Argument Handling\n",
    "        if model_json_schema:\n",
    "            response_format = {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"math-response\",\n",
    "                    \"schema\": model_json_schema,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            response_format = None\n",
    "\n",
    "        # Nested Async Function\n",
    "        async def generate(text: str, image: str) -> str:\n",
    "            content = []\n",
    "            if image:\n",
    "                content.append(\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            if text:\n",
    "                content.append({\"type\": \"text\", \"text\": text})\n",
    "\n",
    "            result = await self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content,  # Dataset prefers image first\n",
    "                    }\n",
    "                ],\n",
    "                model=model_id,\n",
    "                response_format=response_format,\n",
    "                extra_body=extra_body,\n",
    "                **sampling_params,\n",
    "            )\n",
    "            return result.choices[0].message.content\n",
    "\n",
    "        async def gather_completions(texts, images) -> list[str]:\n",
    "            tasks = [generate(t, i) for t, i in zip(texts, images)]\n",
    "            return await asyncio.gather(*tasks)\n",
    "\n",
    "        texts = text_col.to_pylist()\n",
    "        images = image_col.to_pylist()\n",
    "\n",
    "        return self.loop.run_until_complete(gather_completions(texts, images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eae4qPp-_hQI"
   },
   "outputs": [],
   "source": [
    "# 3. Production UDF\n",
    "start = time.time()\n",
    "df_prod_udf = (\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        StructuredOutputsProdUDF.with_init_args(\n",
    "            base_url=OPENAI_BASE_URL,\n",
    "            api_key=OPENAI_API_KEY,\n",
    "        ).with_concurrency(CONCURRENCY)(\n",
    "            model_id=MODEL_ID,\n",
    "            text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "            image_col=col(\"image_base64\"),\n",
    "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .limit(ROW_LIMIT)\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"Prod UDF - Processed {df_prod_udf.count_rows()} rows in {end-start} seconds, {df_prod_udf.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVgjnPjfkEcz"
   },
   "source": [
    "___\n",
    "# Analysis\n",
    "Evaluating Gemma-3's performance on image understanding by comparing structured output responses to the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_GdTLRm_vgx"
   },
   "outputs": [],
   "source": [
    "pass_fail_rate = df_prod_udf.where(col(\"is_correct\")).count_rows() / df_prod_udf.count_rows()\n",
    "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nlo1hCzM1_l"
   },
   "outputs": [],
   "source": [
    "# How does this compare without images?\n",
    "# Here we will use Daft's native inference function llm_generate\n",
    "from daft.functions import llm_generate\n",
    "\n",
    "start = time.time()\n",
    "df_prod_no_img = (\n",
    "    df_prepped.with_column(\n",
    "        \"result\",\n",
    "        llm_generate(\n",
    "            input_column=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "            model=MODEL_ID,\n",
    "            extra_body={\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            base_url=OPENAI_BASE_URL,\n",
    "            provider=\"openai\",\n",
    "        ),\n",
    "    )\n",
    "    .with_column(\"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip())\n",
    "    .collect()\n",
    ")\n",
    "end = time.time()\n",
    "print(\n",
    "    f\"llm_generate - Processed {df_prod_no_img.count_rows()} rows in {end-start} seconds,  {df_prod_no_img.count_rows()/(end-start)} rows/s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d2EJnsabBZpj"
   },
   "outputs": [],
   "source": [
    "pass_fail_rate_no_img = df_prod_no_img.where(col(\"is_correct\")).count_rows() / df_prod_no_img.count_rows()\n",
    "print(f\"Pass/Fail Rate: {pass_fail_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eALCQifQPmfp"
   },
   "source": [
    "---\n",
    "# Putting everything together: Evaluating Gemma across the AI2D Dataset\n",
    "Now that we have walked through implementing this image understanding evaluation pipeline from end to end, lets put it all together so we can take full advantage of lazy evaluation and provide opportunities for future extensibility and re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TObFYF7xeKFJ"
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "import daft\n",
    "from daft import col\n",
    "from daft.functions import format\n",
    "\n",
    "\n",
    "@daft.udf(return_dtype=daft.DataType.string(), concurrency=4)\n",
    "class StructuredOutputsProdUDF:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.client = AsyncOpenAI(base_url=base_url, api_key=api_key)\n",
    "        try:\n",
    "            self.loop = asyncio.get_running_loop()\n",
    "        except RuntimeError:\n",
    "            self.loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(self.loop)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        text_col: daft.Series,\n",
    "        image_col: daft.Series,\n",
    "        sampling_params: dict[str, Any] | None = None,\n",
    "        model_json_schema: dict | None = None,\n",
    "        extra_body: dict[str, Any] | None = None,\n",
    "    ):\n",
    "        # Argument Handling\n",
    "        if model_json_schema:\n",
    "            response_format = {\n",
    "                \"type\": \"json_schema\",\n",
    "                \"json_schema\": {\n",
    "                    \"name\": \"math-response\",\n",
    "                    \"schema\": model_json_schema,\n",
    "                },\n",
    "            }\n",
    "        else:\n",
    "            response_format = None\n",
    "\n",
    "        async def generate(text: str, image: str) -> str:\n",
    "            content = []\n",
    "            if image:\n",
    "                content.append(\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{image}\"},\n",
    "                    }\n",
    "                )\n",
    "            if text:\n",
    "                content.append({\"type\": \"text\", \"text\": text})\n",
    "\n",
    "            result = await self.client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": content,  # Dataset prefers image first\n",
    "                    }\n",
    "                ],\n",
    "                model=model_id,\n",
    "                response_format=response_format,\n",
    "                extra_body=extra_body,\n",
    "                **sampling_params,\n",
    "            )\n",
    "            return result.choices[0].message.content\n",
    "\n",
    "        async def gather_completions(texts, images) -> list[str]:\n",
    "            tasks = [generate(t, i) for t, i in zip(texts, images)]\n",
    "            return await asyncio.gather(*tasks)\n",
    "\n",
    "        texts = text_col.to_pylist()\n",
    "        images = image_col.to_pylist()\n",
    "\n",
    "        return self.loop.run_until_complete(gather_completions(texts, images))\n",
    "\n",
    "\n",
    "class TheCauldronImageUnderstandingEvaluationPipeline:\n",
    "    def __init__(self, base_url: str, api_key: str):\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        model_id: str,\n",
    "        dataset_uri: str,\n",
    "        sampling_params: dict[str, Any] | None = None,\n",
    "        concurrency: int = 4,\n",
    "        row_limit: int | None = None,\n",
    "        is_eager: bool = False,\n",
    "    ) -> daft.DataFrame:\n",
    "        \"\"\"Executes dataset loading, preprocessing, inference, and post-processing.\n",
    "\n",
    "        Evaluation must be run separately since it requires materialization.\n",
    "        \"\"\"\n",
    "        if is_eager:\n",
    "            # Load Dataset and Materialize\n",
    "            df = self.load_dataset(dataset_uri)\n",
    "            df = df.limit(row_limit) if row_limit else df\n",
    "            df = self._log_processing_time(df)\n",
    "\n",
    "            # Preprocess\n",
    "            df = self.preprocess(df)\n",
    "            df = self._log_processing_time(df)\n",
    "\n",
    "            # Perform Inference\n",
    "            df = self.infer(df, model_id, sampling_params)\n",
    "            df = self._log_processing_time(df)\n",
    "\n",
    "            # Post-Process\n",
    "            df = self.postprocess(df)\n",
    "            df = self._log_processing_time(df)\n",
    "        else:\n",
    "            df = self.load_dataset(dataset_uri)\n",
    "            df = self.preprocess(df)\n",
    "            df = self.infer(df, model_id, sampling_params)\n",
    "            df = self.postprocess(df)\n",
    "            df = df.limit(row_limit) if row_limit else df\n",
    "\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _log_processing_time(df: daft.DataFrame):\n",
    "        start = time.time()\n",
    "        df_materialized = df.collect()\n",
    "        end = time.time()\n",
    "        num_rows = df_materialized.count_rows()\n",
    "        print(f\"Processed {num_rows} rows in {end-start} sec, {num_rows/(end-start)} rows/s\")\n",
    "        return df_materialized\n",
    "\n",
    "    def load_dataset(self, uri: str) -> daft.DataFrame:\n",
    "        return daft.read_parquet(uri)\n",
    "\n",
    "    def preprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
    "        # Convert png image byte string to base64\n",
    "        df = df.explode(col(\"images\")).with_column(\n",
    "            \"image_base64\",\n",
    "            df[\"images\"].struct.get(\"bytes\").encode(\"base64\"),\n",
    "        )\n",
    "\n",
    "        # Explode Lists of User Prompts and Assistant Answer Pairs\n",
    "        df = df.explode(col(\"texts\")).with_columns(\n",
    "            {\"user\": df[\"texts\"].struct.get(\"user\"), \"assistant\": df[\"texts\"].struct.get(\"assistant\")}\n",
    "        )\n",
    "\n",
    "        # Parse the Question/Answer Strings\n",
    "        df = df.with_columns(\n",
    "            {\n",
    "                \"question\": df[\"user\"]\n",
    "                .str.extract(r\"(?s)Question:\\s*(.*?)\\s*Choices:\")\n",
    "                .str.replace(\"Choices:\", \"\")\n",
    "                .str.replace(\"Question:\", \"\"),\n",
    "                \"choices_string\": df[\"user\"]\n",
    "                .str.extract(r\"(?s)Choices:\\s*(.*?)\\s*Answer?\\.?\")\n",
    "                .str.replace(\"Choices:\\n\", \"\")\n",
    "                .str.replace(\"Answer\", \"\"),\n",
    "                \"answer\": df[\"assistant\"].str.extract(r\"Answer:\\s*(.*)$\").str.replace(\"Answer:\", \"\"),\n",
    "            }\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def infer(\n",
    "        self,\n",
    "        df: daft.DataFrame,\n",
    "        model_id: str = \"google/gemma-3n-e4b-it\",\n",
    "        sampling_params: dict[str, Any] = {\"temperature\": 0.0},\n",
    "        concurrency: int = 4,\n",
    "        extra_body: dict[str, Any] = {\"guided_choice\": [\"A\", \"B\", \"C\", \"D\"]},\n",
    "    ) -> daft.DataFrame:\n",
    "        return df.with_column(\n",
    "            \"result\",\n",
    "            StructuredOutputsProdUDF.with_init_args(\n",
    "                base_url=self.base_url,\n",
    "                api_key=self.api_key,\n",
    "            ).with_concurrency(concurrency)(\n",
    "                model_id=model_id,\n",
    "                text_col=format(\"{} \\n {}\", col(\"question\"), col(\"choices_string\")),  # Prompt Template\n",
    "                image_col=col(\"image_base64\"),\n",
    "                sampling_params=sampling_params,\n",
    "                extra_body=extra_body,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def postprocess(self, df: daft.DataFrame) -> daft.DataFrame:\n",
    "        df = df.with_column(\n",
    "            \"is_correct\", col(\"result\").str.lstrip().str.rstrip() == col(\"answer\").str.lstrip().str.rstrip()\n",
    "        )\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, df: daft.DataFrame) -> float:\n",
    "        pass_fail_rate = df.where(col(\"is_correct\")).count_rows() / df.count_rows()\n",
    "        return pass_fail_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A98SLoXALhxR"
   },
   "outputs": [],
   "source": [
    "# Our entire pipeline collapses into a three lines\n",
    "dataset_uri = \"hf://datasets/HuggingFaceM4/the_cauldron/ai2d/train-00000-of-00001-2ce340398c113b79.parquet\"\n",
    "pipeline = TheCauldronImageUnderstandingEvaluationPipeline(\n",
    "    api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL, row_limit=ROW_LIMIT\n",
    ")\n",
    "df = pipeline(model_id=MODEL_ID, sampling_params={\"temperature\": 0.1}, is_eager=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mFdEC8JDMj8E"
   },
   "outputs": [],
   "source": [
    "# Materialize if not eager\n",
    "df_mat = df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0v6x19XxNyJa"
   },
   "outputs": [],
   "source": [
    "# Print the Pass/Fail Rate\n",
    "print(f\"Pass/Fail Rate: {pipeline.evaluate(df_mat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbzOoC62uDu1"
   },
   "source": [
    "---\n",
    "## Conclusion\n",
    "\n",
    "In this notebook we explored how to evaluate Gemma-3's image understanding using a subset from HuggingFace's TheCauldron Dataset. The AI2D subset we used is just one of a massive collection of 50 vision-language datasets that can be used for evaluating or training vision language models totaling millions of rows. You can also leverage this pipeline to evaluate model performance across sampling parameters or model variants. Please note that not all Gemma-3 series models support image inputs, and leveraging datasets outside of the TheCauldron would require different preprocessing stages.\n",
    "\n",
    "A natural next step would be to parallelize this pipeline across multiple datasets leveraging multiple gpus. In this scenario, I recommend transitioning daft's execution context to leverage Ray, a distributed compute framework.\n",
    "\n",
    "```bash\n",
    "pip install \"daft[huggingface,ray]\"\n",
    "```\n",
    "\n",
    "You can set daft's execution context to ray adding the `ray` optional dependency during installation and running the following at the top of your script.\n",
    "\n",
    "```python\n",
    "import daft\n",
    "\n",
    "daft.set_runner_ray()\n",
    "```\n",
    "\n",
    "Simply run your pipeline across each dataset uri and collect the results, Daft will orchestrate ray in the background for you. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "gqjRsfxH891x",
    "J_wIalJJs0ki"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
