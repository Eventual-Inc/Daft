{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAoKI4IEYZ2I"
   },
   "source": [
    "# Document Processing with Daft\n",
    "\n",
    "ðŸ‘‹ Hello and welcome to [Daft](http://www.daft.ai/)! This tutorial shows how to use Daft to create a typical PDF processing pipeline. By the end of the tutorial, we will have a fully functional pipeline that:\n",
    "- [starts with downloading PDFs from an S3](https://colab.research.google.com/drive/1QeYdSz87DBauPsokN3RNBLXOVB1zexD5#scrollTo=Pg8UgK_3XqWN)\n",
    "- [extracts text boxes either using OCR or by reading the file format](https://colab.research.google.com/drive/1QeYdSz87DBauPsokN3RNBLXOVB1zexD5#scrollTo=Bilsa6-2zCk5)\n",
    "- [performs spatial layout analysis to group text boxes into either lines or paragraphs](https://colab.research.google.com/drive/1QeYdSz87DBauPsokN3RNBLXOVB1zexD5#scrollTo=ycj_Q7tA8dBf)\n",
    "- [computes embeddings using a lightweight LLM, running locally](https://colab.research.google.com/drive/1QeYdSz87DBauPsokN3RNBLXOVB1zexD5#scrollTo=YWzrR3sY8tBo)\n",
    "- [saves everything as Parquet files](https://colab.research.google.com/drive/1QeYdSz87DBauPsokN3RNBLXOVB1zexD5#scrollTo=wvl_Our9-N4o)\n",
    "\n",
    "\n",
    "**tl;dr**: If you'd like, you can [jump to the end to see the full pipeline](https://colab.research.google.com/drive/1QeYdSz87DBauPsokN3RNBLXOVB1zexD5#scrollTo=R1_rosls8ykn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9f7JcJvXIqP"
   },
   "source": [
    "Before we see any code, let's install Daft and all of the dependencies we'll use in this tutorial!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNAO_yjB5Paj",
    "outputId": "cb211331-6b19-4093-aeed-003b6ad028d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-zuu20m1e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-zuu20m1e\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.3.0)\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.7)\n",
      "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n",
      "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
      "Requirement already satisfied: pydantic-to-pyarrow in /usr/local/lib/python3.11/dist-packages (0.1.6)\n",
      "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: daft[aws] in /usr/local/lib/python3.11/dist-packages (0.5.11)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from daft[aws]) (18.1.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from daft[aws]) (2025.7.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from daft[aws]) (1.39.11)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
      "Requirement already satisfied: botocore<1.40.0,>=1.39.11 in /usr/local/lib/python3.11/dist-packages (from boto3->daft[aws]) (1.39.11)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->daft[aws]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3->daft[aws]) (0.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.11->boto3->daft[aws]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.40.0,>=1.39.11->boto3->daft[aws]) (2.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.40.0,>=1.39.11->boto3->daft[aws]) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install daft[aws] pillow pydantic PyMuPDF pytesseract sentence-transformers pydantic-to-pyarrow pdf2image accelerate ftfy regex tqdm git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiLAhoISymVh"
   },
   "source": [
    "This is the complete set of imports that we'll use throughout this notebook tutorial. We'll evaluate them once here and reuse in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2QwupDTyaxi"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import traceback\n",
    "from collections.abc import Iterator\n",
    "from datetime import datetime\n",
    "from types import NoneType\n",
    "from typing import Any, Optional, Union, get_args, get_origin\n",
    "\n",
    "import clip\n",
    "import fitz\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import pytesseract\n",
    "import torch\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel\n",
    "from pydantic_to_pyarrow import get_pyarrow_schema\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import daft\n",
    "from daft import Series, col, udf\n",
    "from daft.udf import UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyXAgb-Nyrt2"
   },
   "source": [
    "# Our PDF Data\n",
    "\n",
    "First, we get the S3 URLs for all of the PDFs that we'll use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgW2BPFS5ySn"
   },
   "outputs": [],
   "source": [
    "IO_CONFIG = daft.io.IOConfig(s3=daft.io.S3Config(anonymous=True))\n",
    "\n",
    "df_sample = daft.from_glob_path(\n",
    "    \"s3://daft-public-data/tutorials/document-processing/industry_documents_library/pdfs/*\",\n",
    "    io_config=IO_CONFIG,\n",
    ").limit(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKr3ltir33Ps"
   },
   "source": [
    "To get a sense for what this data looks like, we can use the [`show`](https://docs.daft.ai/en/stable/api/dataframe/#daft.DataFrame.show) method on the `daft.DataFrame`. This doesn't materialize the entire DataFrame. Instead, it only does enough computation to show us the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCS3-Ccw4CkK"
   },
   "outputs": [],
   "source": [
    "df_sample.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pg8UgK_3XqWN"
   },
   "source": [
    "#### Downloading PDFs\n",
    "\n",
    "We can use Daft to download these PDFs from S3 in parallel! Let's see what that looks like on this sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQyl6ZtAXrBK"
   },
   "outputs": [],
   "source": [
    "df_sample = df_sample.collect()\n",
    "\n",
    "_start = datetime.now()\n",
    "df_sample_downloaded = df_sample.with_column(\"pdf_bytes\", col(\"path\").url.download(io_config=IO_CONFIG))\n",
    "df_sample_downloaded = df_sample_downloaded.collect()\n",
    "_end = datetime.now()\n",
    "print(f\"Downloaded {df_sample_downloaded.count_rows()} PDFs from S3 in {_end - _start}\")\n",
    "\n",
    "print(df_sample_downloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zn561WnWWRre"
   },
   "outputs": [],
   "source": [
    "df_sample_downloaded[\"path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kd8vHnW-ykBh"
   },
   "source": [
    "Daft knows about URLs and has built-in support for downloading their contents! This is exposed via the [`.url.download()` method](https://docs.daft.ai/en/stable/api/expressions/#daft.expressions.expressions.ExpressionUrlNamespace.download) on a column expression (that's the [`col('path')`](https://docs.getdaft.io/en/stable/api/expressions/#daft.expressions.col))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZceRbrSTaMxl"
   },
   "source": [
    "### Pydantic Document Classes\n",
    "\n",
    "Let's switch back to building out a document processing pipeline. We know that we can get the PDF bytes and load up each document. But, for our pipeline, we'd like to have a structured representation for the content we care about in each document. Namely, the text!\n",
    "\n",
    " Documents are two-dimensional: when doing document processing, we care about what the document says and _where_ it says it. What page? Where on the page? We can often make inferences on what role a piece of text fills by where it occurs on a page. For example, if we're processing forms, something right next to the \"First Name:\" field is _probably_ someone's first name.\n",
    "\n",
    " If we're doing ML after this pipeline, we will absolutely want to provide this spatial information to our model.\n",
    "\n",
    " So, before we can define any steps in our pipeline, we will need to define some Pydantic classes to help us represent a document!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKupgf6fyWxB"
   },
   "outputs": [],
   "source": [
    "class BoundingBox(BaseModel):\n",
    "    x: int\n",
    "    y: int\n",
    "    w: int\n",
    "    h: int\n",
    "\n",
    "    def as_cropbox(self) -> tuple[int, int, int, int]:\n",
    "        \"\"\"Returns (x0, y0, x1, y1)\"\"\"\n",
    "        return (\n",
    "            self.x,\n",
    "            self.y,\n",
    "            self.x + self.w,\n",
    "            self.y + self.h,\n",
    "        )\n",
    "\n",
    "\n",
    "class TextBlock(BaseModel):\n",
    "    text: str\n",
    "    bounding_box: BoundingBox\n",
    "\n",
    "\n",
    "class DPI(BaseModel):\n",
    "    height: float\n",
    "    width: float\n",
    "\n",
    "\n",
    "class ParsedPage(BaseModel):\n",
    "    page_index: int\n",
    "    text_blocks: list[TextBlock]\n",
    "    images: list[BoundingBox]\n",
    "    page_height: int\n",
    "    page_width: int\n",
    "    dpi: Optional[DPI]\n",
    "\n",
    "\n",
    "class ParsedPdf(BaseModel):\n",
    "    pdf_path: str\n",
    "    total_pages: int\n",
    "    pages: list[ParsedPage]\n",
    "\n",
    "\n",
    "class TextLine(BaseModel):\n",
    "    words: list[TextBlock]\n",
    "    bounding_box: BoundingBox\n",
    "\n",
    "\n",
    "class IndexedTextBlock(BaseModel):\n",
    "    index: int\n",
    "    text: TextBlock\n",
    "\n",
    "\n",
    "class Processed(BaseModel):\n",
    "    page_index: int\n",
    "    indexed_texts: list[IndexedTextBlock]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CUdKECl0D9h"
   },
   "source": [
    "### Generating Daft Datatypes from Pydantic\n",
    "\n",
    "We also need to define a function that will let us easily generate Daft DataTypes from our Pydantic classes using PyArrow. We'll use this function, `daft_pyarrow_datatype`, to let us automatically generate the [`daft.DataType`](https://docs.getdaft.io/en/v0.2.13/api_docs/datatype.html#):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMip_0zi0Msa"
   },
   "outputs": [],
   "source": [
    "def daft_pyarrow_datatype(f_type: type[Any]) -> daft.DataType:\n",
    "    return daft.DataType.from_arrow_type(pyarrow_datatype(f_type))\n",
    "\n",
    "\n",
    "def pyarrow_datatype(f_type: type[Any]) -> pyarrow.DataType:\n",
    "    if get_origin(f_type) is Union:\n",
    "        targs = get_args(f_type)\n",
    "        if len(targs) == 2:\n",
    "            if targs[0] is NoneType and targs[1] is not NoneType:\n",
    "                refined_inner = targs[1]\n",
    "            elif targs[0] is not NoneType and targs[1] is NoneType:\n",
    "                refined_inner = targs[0]\n",
    "            else:\n",
    "                raise TypeError(f\"Cannot convert a general union type {f_type} into a pyarrow.DataType!\")\n",
    "            inner_type = pyarrow_datatype(refined_inner)\n",
    "        else:\n",
    "            raise TypeError(f\"Cannot convert a general union type {f_type} into a pyarrow.DataType!\")\n",
    "\n",
    "    elif get_origin(f_type) is list:\n",
    "        targs = get_args(f_type)\n",
    "        if len(targs) != 1:\n",
    "            raise TypeError(\n",
    "                f\"Expected list type {f_type} with inner element type but \" f\"got {len(targs)} inner-types: {targs}\"\n",
    "            )\n",
    "        element_type = targs[0]\n",
    "        inner_type = pyarrow.list_(pyarrow_datatype(element_type))\n",
    "\n",
    "    elif get_origin(f_type) is dict:\n",
    "        targs = get_args(f_type)\n",
    "        if len(targs) != 2:\n",
    "            raise TypeError(\n",
    "                f\"Expected dict type {f_type} with inner key-value types but got \" f\"{len(targs)} inner-types: {targs}\"\n",
    "            )\n",
    "        kt, vt = targs\n",
    "        pyarrow_kt = pyarrow_datatype(kt)\n",
    "        pyarrow_vt = pyarrow_datatype(vt)\n",
    "        inner_type = pyarrow.map_(pyarrow_kt, pyarrow_vt)\n",
    "\n",
    "    elif get_origin(f_type) is tuple:\n",
    "        raise TypeError(f\"Cannot support tuple types: {f_type}\")\n",
    "\n",
    "    elif issubclass(f_type, BaseModel):\n",
    "        schema = get_pyarrow_schema(f_type)\n",
    "        inner_type = pyarrow.struct([(f, schema.field(f).type) for f in schema.names])\n",
    "\n",
    "    elif issubclass(f_type, str):\n",
    "        inner_type = pyarrow.string()\n",
    "\n",
    "    elif issubclass(f_type, int):\n",
    "        inner_type = pyarrow.int64()\n",
    "\n",
    "    elif issubclass(f_type, float):\n",
    "        inner_type = pyarrow.float64()\n",
    "\n",
    "    elif issubclass(f_type, bool):\n",
    "        inner_type = pyarrow.bool_()\n",
    "\n",
    "    elif issubclass(f_type, bytes):\n",
    "        inner_type = pyarrow.binary()\n",
    "\n",
    "    elif issubclass(f_type, datetime):\n",
    "        inner_type = pyarrow.date64()\n",
    "\n",
    "    else:\n",
    "        raise TypeError(f\"Cannot handle general Python objects in Arrow: {f_type}\")\n",
    "\n",
    "    return inner_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUZ01A3daa4b"
   },
   "source": [
    "We will use `daft_pyarrow_datatype` when we define the `return_dtype` in our upcoming user-defined functions (UDF)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bilsa6-2zCk5"
   },
   "source": [
    "# Loading & Parsing PDFs using UDFs\n",
    "\n",
    "The first part of our pipeline is to load the PDF's contents, locate all text, and put these results into our `ParsedPdf` class.\n",
    "\n",
    "This procedure can either perform OCR to locate text boxes or it can inspect the PDF and, if it is supported, directly extract text boxes. Note that there are no guarentees that a PDF will support text. And if supported, there are no guarentees that the text boxes will make sense from a human readability standpoint.\n",
    "\n",
    "We will create a user defined function (UDF) to allow Daft to load and parse our PDFs. This UDF, `LoadDirectAndParsePdf`, will use supporting functions for performing OCR with Tesseract or for extracting text out of the file via PyMuPDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UWxwZzSj1dV5"
   },
   "outputs": [],
   "source": [
    "# Daft needs you to tell it what the expected output of any UDF looks like.\n",
    "# We do this by specifying the return_dtype value.\n",
    "#\n",
    "# We're using our:\n",
    "#     (a) automatic Pydantic-to-Daft datatype function\n",
    "#     (b) Pydantic class\n",
    "#\n",
    "#                   (a) here\n",
    "#                  âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„\n",
    "#                                        (b) and here!\n",
    "#                                       âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„\n",
    "@udf(return_dtype=daft_pyarrow_datatype(ParsedPdf))\n",
    "class LoadDirectAndParsePdf:\n",
    "    def __init__(self, ocr: bool, page_limit: Optional[int], extract_images: bool) -> None:\n",
    "        self.ocr = ocr\n",
    "        self.page_limit = page_limit\n",
    "        self.extract_images = extract_images\n",
    "\n",
    "    def handle(self, url: str, pdf_bytes: bytes) -> ParsedPdf:\n",
    "        bytes_buffer = io.BytesIO(pdf_bytes)\n",
    "        with fitz.open(stream=bytes_buffer, filetype=\"pdf\") as pdf:\n",
    "            if self.ocr:\n",
    "                parsed_doc = ocr_document(pdf, page_limit=self.page_limit)\n",
    "            else:\n",
    "                parsed_doc = process_all_pages(\n",
    "                    pdf,\n",
    "                    extract_images=self.extract_images,\n",
    "                    page_limit=self.page_limit,\n",
    "                    get_image_dpi=False,\n",
    "                )\n",
    "        parsed_doc.pdf_path = url\n",
    "        return parsed_doc\n",
    "\n",
    "    def __call__(self, urls: Series, pdf_bytes: Series) -> Series:\n",
    "        return Series.from_pylist(\n",
    "            # NOTE: it is **vital** to call .model_dump() on each Pydantic class.\n",
    "            #       Daft handles converting the data into an Arrow record, using\n",
    "            #       the DataType derrived from the Pydantic class. However, it\n",
    "            #       expects the data to be in a regular Python dictionary.\n",
    "            [self.handle(u, p).model_dump() for u, p in zip(urls, pdf_bytes)]\n",
    "        )\n",
    "\n",
    "\n",
    "def ocr_document(doc: fitz.Document, *, page_limit: Optional[int] = None) -> ParsedPdf:\n",
    "    pages: list[ParsedPage] = []\n",
    "    for page_index in range(min(page_limit, len(doc)) if page_limit is not None else len(doc)):\n",
    "        page = doc[page_index]\n",
    "\n",
    "        image = rasterize_page(page)\n",
    "        dpi_height, dpi_width = image.info.get(\"dpi\", (72, 72))\n",
    "        dpi = DPI(height=dpi_height, width=dpi_width)\n",
    "\n",
    "        text_blocks = ocr_page(image)\n",
    "\n",
    "        ocred_page = ParsedPage(\n",
    "            page_index=page_index,\n",
    "            text_blocks=text_blocks,\n",
    "            images=extract_image_with_bbox(page),\n",
    "            page_height=int(round(page.rect.height)),\n",
    "            page_width=int(round(page.rect.width)),\n",
    "            dpi=dpi,\n",
    "        )\n",
    "        pages.append(ocred_page)\n",
    "\n",
    "    return ParsedPdf(\n",
    "        pdf_path=\"\",\n",
    "        total_pages=len(doc),\n",
    "        pages=pages,\n",
    "    )\n",
    "\n",
    "\n",
    "def rasterize_page(page: fitz.Page, scale: float = 2.0) -> Image.Image:\n",
    "    # Create transformation matrix for scaling\n",
    "    mat = fitz.Matrix(scale, scale)\n",
    "\n",
    "    # Render page to pixmap\n",
    "    pix = page.get_pixmap(matrix=mat)\n",
    "\n",
    "    img_data = pix.tobytes(\"png\")\n",
    "    image = Image.open(io.BytesIO(img_data))\n",
    "    return image\n",
    "\n",
    "\n",
    "def ocr_page(image: Image.Image) -> list[TextBlock]:\n",
    "    results = []\n",
    "    data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)\n",
    "    for i in range(len(data[\"text\"])):\n",
    "        text_content = data[\"text\"][i].strip()\n",
    "        if text_content:\n",
    "            text_block = TextBlock(\n",
    "                text=text_content,\n",
    "                bounding_box=BoundingBox(\n",
    "                    x=data[\"left\"][i],\n",
    "                    y=data[\"top\"][i],\n",
    "                    h=data[\"height\"][i],\n",
    "                    w=data[\"width\"][i],\n",
    "                ),\n",
    "            )\n",
    "            results.append(text_block)\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_all_pages(\n",
    "    doc: fitz.Document,\n",
    "    *,\n",
    "    extract_images: bool,\n",
    "    page_limit: Optional[int] = None,\n",
    "    get_image_dpi: bool = False,\n",
    ") -> ParsedPdf:\n",
    "    pages = []\n",
    "    for page_index in range(min(page_limit, len(doc)) if page_limit is not None else len(doc)):\n",
    "        pages.append(\n",
    "            process_page(\n",
    "                page_index,\n",
    "                doc[page_index],\n",
    "                extract_images=extract_images,\n",
    "                get_image_dpi=get_image_dpi,\n",
    "            )\n",
    "        )\n",
    "    return ParsedPdf(pdf_path=\"\", total_pages=len(doc), pages=pages)\n",
    "\n",
    "\n",
    "def process_page(\n",
    "    page_index: int,\n",
    "    page: fitz.Page,\n",
    "    *,\n",
    "    extract_images: bool,\n",
    "    get_image_dpi: bool = False,\n",
    ") -> ParsedPage:\n",
    "    text_data = extract_text_with_bbox(page)\n",
    "\n",
    "    if extract_images:\n",
    "        image_data = extract_image_with_bbox(page)\n",
    "    else:\n",
    "        image_data = []\n",
    "\n",
    "    if get_image_dpi:\n",
    "        image = rasterize_page(page)\n",
    "        dpi_height, dpi_width = image.info.get(\"dpi\", (72, 72))\n",
    "        maybe_dpi = DPI(height=dpi_height, width=dpi_width)\n",
    "    else:\n",
    "        maybe_dpi = None\n",
    "\n",
    "    page_data: ParsedPage = ParsedPage(\n",
    "        page_index=page_index,\n",
    "        text_blocks=text_data,\n",
    "        images=image_data,\n",
    "        page_height=int(round(page.rect.height)),\n",
    "        page_width=int(round(page.rect.width)),\n",
    "        dpi=maybe_dpi,\n",
    "    )\n",
    "    return page_data\n",
    "\n",
    "\n",
    "def extract_text_with_bbox(page: fitz.Page) -> list[TextBlock]:\n",
    "    text_blocks = []\n",
    "    blocks = page.get_text(\"dict\")  # type: ignore\n",
    "    for block in blocks.get(\"blocks\", []):\n",
    "        if \"lines\" in block:  # Text block\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    text = span[\"text\"].strip()\n",
    "                    if text:\n",
    "                        x0, y0, x1, y1 = tuple(map(lambda z: int(round(z)), span[\"bbox\"]))\n",
    "                        w = x1 - x0\n",
    "                        h = y1 - y0\n",
    "                        text_block = TextBlock(\n",
    "                            text=text,\n",
    "                            bounding_box=BoundingBox(x=x0, y=y0, w=w, h=h),\n",
    "                        )\n",
    "                        text_blocks.append(text_block)\n",
    "    return text_blocks\n",
    "\n",
    "\n",
    "def extract_image_with_bbox(page: fitz.Page) -> list[BoundingBox]:\n",
    "    image_bboxes = []\n",
    "    for inst in page.get_image_info():  # type: ignore\n",
    "        x0, y0, x1, y1 = tuple(map(lambda z: int(round(z)), inst[\"bbox\"]))\n",
    "        w = x1 - x0\n",
    "        h = y1 - y0\n",
    "        image_bboxes.append(BoundingBox(x=x0, y=y0, w=w, h=h))\n",
    "    return image_bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lgmkF9dk3zp-"
   },
   "source": [
    "### Sample on first PDF\n",
    "\n",
    "Let's see what it looks like to perform OCR and extract text from the first PDF in our collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCMIjN6F4nsX"
   },
   "outputs": [],
   "source": [
    "df_first_1 = df_sample.limit(1)\n",
    "df_first_1 = df_first_1.with_column(\"pdf_bytes\", df_first_1[\"path\"].url.download(io_config=IO_CONFIG))\n",
    "df_first_1 = df_first_1.collect()\n",
    "print(df_first_1)\n",
    "\n",
    "pdf = fitz.open(stream=io.BytesIO(df_first_1.to_pylist()[0][\"pdf_bytes\"]), filetype=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxETUsND6Xmr"
   },
   "outputs": [],
   "source": [
    "ocr_text_boxes = ocr_document(pdf, page_limit=1).pages[0]\n",
    "print(f\"{len(ocr_text_boxes.text_blocks)} OCR'd text boxes on the first page. Sample:\")\n",
    "for i in range(25):\n",
    "    tb = ocr_text_boxes.text_blocks[i]\n",
    "    print(f\"text='{tb.text}' {tb.bounding_box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXDfzilc6-v_"
   },
   "outputs": [],
   "source": [
    "extracted_text_boxes = process_all_pages(pdf, extract_images=True, page_limit=1, get_image_dpi=False).pages[0]\n",
    "print(f\"{len(extracted_text_boxes.text_blocks)} text boxes listed in PDF on the first page. Sample:\")\n",
    "for i in range(25):\n",
    "    tb = extracted_text_boxes.text_blocks[i]\n",
    "    print(f\"text='{tb.text}' {tb.bounding_box}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_virz1OC6Xp_"
   },
   "outputs": [],
   "source": [
    "pdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycj_Q7tA8dBf"
   },
   "source": [
    "# Document Processing\n",
    "\n",
    "Now that we can load PDFs and format them into our `ProcessedPdf` Pydantic class, we can start to define the steps of our document processing pipeline!\n",
    "\n",
    "Here, we will define the `DocProcessor` UDF, which uses spatial heuristics to group the extracted `TextBlock`s into more coherent and usable groups of text. We also make sure to sort the text into reading order, which we define here as left-to-right, top-to-bottom. We can choose to only group things into lines (only looking at the Y axis). Or we can choose to group into paragraphs (looking at the Y and X axes).\n",
    "\n",
    "These heuristics have controllable thresholds (`row_tolerance` for the lines and `{x,y}_thresh` for the paragraph grouping). They _will not_ work in all usecases and domains! In general, Document Layout Analysis is hard. There are excellent research models for performing layout analysis which can easily be ran with Daft :), but we'll save that for another day!\n",
    "\n",
    "Below is the code for this UDF and its supporting helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abpx-rce1de2"
   },
   "outputs": [],
   "source": [
    "class PipelineConfig(BaseModel):\n",
    "    row_tolerance: int = 10\n",
    "    y_thresh: int = 15\n",
    "    x_thresh: int = 60\n",
    "    group_paragraphs: bool = True\n",
    "\n",
    "\n",
    "@udf(return_dtype=daft_pyarrow_datatype(list[Processed]))\n",
    "class DocProcessor:\n",
    "    def __init__(self, *, row_tolerance: int, y_thresh: int, x_thresh: int, group_paragraphs: bool) -> None:\n",
    "        self.row_tolerance = row_tolerance\n",
    "        self.y_thresh = y_thresh\n",
    "        self.x_thresh = x_thresh\n",
    "        self.group_paragraphs = group_paragraphs\n",
    "\n",
    "    def handle(self, doc: ParsedPdf | dict) -> Iterator[Processed]:\n",
    "        parsed_doc = ParsedPdf.model_validate(doc) if isinstance(doc, dict) else doc\n",
    "        for page_index, page in enumerate(parsed_doc.pages):\n",
    "            text_blocks = page_pipeline(\n",
    "                page,\n",
    "                row_tolerance=self.row_tolerance,\n",
    "                y_thresh=self.y_thresh,\n",
    "                x_thresh=self.x_thresh,\n",
    "                group_paragraphs=self.group_paragraphs,\n",
    "            )\n",
    "            yield Processed(\n",
    "                page_index=page_index,\n",
    "                indexed_texts=[IndexedTextBlock(index=i, text=t) for i, t in enumerate(text_blocks)],\n",
    "            )\n",
    "\n",
    "    def __call__(self, parsed: Series) -> Series:\n",
    "        return Series.from_pylist(\n",
    "            # Again, note the call to .model_dump() on each Pydantic object.\n",
    "            [[processed.model_dump() for processed in self.handle(doc)] for doc in parsed]\n",
    "        )\n",
    "\n",
    "\n",
    "def page_pipeline(\n",
    "    parsed_page: ParsedPage,\n",
    "    *,\n",
    "    row_tolerance: int,\n",
    "    y_thresh: int,\n",
    "    x_thresh: int,\n",
    "    group_paragraphs: bool,\n",
    ") -> list[TextBlock]:\n",
    "    reading_order_text_blocks = sort_bounding_boxes_reading_order(parsed_page.text_blocks, row_tolerance=row_tolerance)\n",
    "    text_lines: list[TextLine] = group_into_text_lines(\n",
    "        reading_order_text_blocks,\n",
    "        y_thresh=y_thresh,\n",
    "        x_thresh=x_thresh,\n",
    "    )\n",
    "\n",
    "    if group_paragraphs:\n",
    "        simplified_text_lines = group_into_paragraphs(text_lines, row_tolerance=row_tolerance)\n",
    "    else:\n",
    "        simplified_text_lines = [revert_to_tb(tl) for tl in text_lines]\n",
    "\n",
    "    final_texts = sort_bounding_boxes_reading_order(simplified_text_lines, row_tolerance=row_tolerance)\n",
    "\n",
    "    return final_texts\n",
    "\n",
    "\n",
    "def sort_bounding_boxes_reading_order(boxes: list[TextBlock], *, row_tolerance: int) -> list[TextBlock]:\n",
    "    \"\"\"Sort a list of BoundingBox objects into reading order (left-to-right, top-to-bottom).\n",
    "\n",
    "    This function implements a multi-line sorting algorithm that:\n",
    "    1. Groups bounding boxes by approximate row (within a tolerance)\n",
    "    2. Sorts boxes within each row from left to right\n",
    "    3. Sorts rows from top to bottom\n",
    "\n",
    "    Args:\n",
    "        boxes: list of BoundingBox objects to sort\n",
    "        row_tolerance: Tolerance for grouping boxes into the same row (in pixels)\n",
    "                       Adjust this value based on your document's line spacing.\n",
    "\n",
    "    Returns:\n",
    "        list of BoundingBox objects sorted in reading order\n",
    "    \"\"\"\n",
    "    if not boxes:\n",
    "        return []\n",
    "\n",
    "    boxes = boxes.copy()\n",
    "\n",
    "    # Group boxes by approximate row\n",
    "    rows: dict[int, list[TextBlock]] = {}\n",
    "\n",
    "    for tbox in boxes:\n",
    "        # Find which row this box belongs to\n",
    "        placed = False\n",
    "        for row_y, tboxes_in_row in rows.items():\n",
    "            if abs(tbox.bounding_box.y - row_y) <= row_tolerance:\n",
    "                tboxes_in_row.append(tbox)\n",
    "                placed = True\n",
    "                break\n",
    "\n",
    "        if not placed:\n",
    "            # Create a new row\n",
    "            rows[tbox.bounding_box.y] = [tbox]\n",
    "\n",
    "    # Sort each row from left to right\n",
    "    for row_y, tboxes_in_row in rows.items():\n",
    "        tboxes_in_row.sort(key=lambda box: box.bounding_box.x)\n",
    "\n",
    "    # Sort rows from top to bottom\n",
    "    fininished_rows = [(row_y, tboxes_in_row) for row_y, tboxes_in_row in rows.items()]\n",
    "    fininished_rows.sort(key=lambda x: x[0])\n",
    "\n",
    "    # Flatten the result\n",
    "    result: list[TextBlock] = []\n",
    "    for _, tboxes_in_row in fininished_rows:\n",
    "        result.extend(tboxes_in_row)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def group_into_text_lines(\n",
    "    reading_order_text_boxes: list[TextBlock],\n",
    "    *,\n",
    "    y_thresh: int,\n",
    "    x_thresh: int,\n",
    ") -> list[TextLine]:\n",
    "    # Group into lines\n",
    "    lines: list[list[TextBlock]] = []\n",
    "    for word in reading_order_text_boxes:\n",
    "        assigned = False\n",
    "        for line in lines:\n",
    "            last_word: TextBlock = line[-1]\n",
    "            same_line_y: bool = abs(word.bounding_box.y - last_word.bounding_box.y) <= y_thresh\n",
    "            close_x: bool = word.bounding_box.x - (last_word.bounding_box.x + last_word.bounding_box.w) <= x_thresh\n",
    "            if same_line_y and close_x:\n",
    "                line.append(word)\n",
    "                assigned = True\n",
    "                break\n",
    "\n",
    "        if not assigned:\n",
    "            lines.append([word])\n",
    "\n",
    "    # Aggregate lines with bounding boxes\n",
    "    results: list[TextLine] = [form_text_line(line_words) for line_words in lines]\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def form_text_line(line_words: list[TextBlock]) -> TextLine:\n",
    "    xs: list[int] = [w.bounding_box.x for w in line_words]\n",
    "    ys: list[int] = [w.bounding_box.y for w in line_words]\n",
    "    ws: list[int] = [w.bounding_box.w for w in line_words]\n",
    "    hs: list[int] = [w.bounding_box.h for w in line_words]\n",
    "\n",
    "    x_min = min(xs)\n",
    "    y_min = min(ys)\n",
    "    x_max = max(x + w for x, w in zip(xs, ws))\n",
    "    y_max = max(y + h for y, h in zip(ys, hs))\n",
    "\n",
    "    return TextLine(\n",
    "        words=line_words,\n",
    "        bounding_box=BoundingBox(x=x_min, y=y_min, w=x_max - x_min, h=y_max - y_min),\n",
    "    )\n",
    "\n",
    "\n",
    "def group_into_paragraphs(text_lines: list[TextLine], *, row_tolerance: int) -> list[TextBlock]:\n",
    "    paragraphs: list[list[TextLine]] = []\n",
    "    for tl in text_lines:\n",
    "        assigned = False\n",
    "        for p_group in paragraphs:\n",
    "            p = p_group[-1]\n",
    "            if abs(p.bounding_box.y + p.bounding_box.h - tl.bounding_box.y) <= row_tolerance:\n",
    "                p_group.append(tl)\n",
    "                assigned = True\n",
    "                break\n",
    "        if not assigned:\n",
    "            paragraphs.append([tl])\n",
    "\n",
    "    simplified_text_lines: list[TextBlock] = []\n",
    "    for p_group in paragraphs:\n",
    "        tbs: list[TextBlock] = [revert_to_tb(tl) for tl in p_group]\n",
    "        paragraph: TextBlock = revert_to_tb(form_text_line(tbs))\n",
    "        simplified_text_lines.append(paragraph)\n",
    "\n",
    "    return simplified_text_lines\n",
    "\n",
    "\n",
    "def revert_to_tb(tl: TextLine) -> TextBlock:\n",
    "    return TextBlock(\n",
    "        text=\" \".join(w.text for w in tl.words),\n",
    "        bounding_box=tl.bounding_box,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWzrR3sY8tBo"
   },
   "source": [
    "# Text Embedding\n",
    "\n",
    "Now that we have nice groups of text, we can generate embeddings for them! We will define a functor that makes a UDF from a `SentenceTransformer` model. The resulting UDF will make sure that the generated values are (1) of a fixed size and (2) of a known datatype (your classic 32-bit floating point number!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2UzaTdFf8uuQ"
   },
   "outputs": [],
   "source": [
    "def text_embedder_udf(model_name: str) -> UDF:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    dimensionality = model.get_sentence_embedding_dimension()\n",
    "    assert dimensionality is not None, f\"Must supply model with known dimensionality. Invalid {model=}\"\n",
    "    model = model.eval()\n",
    "    try:\n",
    "        model.compile()\n",
    "    except Exception:\n",
    "        print(\"Could not torch.compile the SentenceTransformer model. \" \"Proceeding with unoptimized inference.\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    @udf(return_dtype=daft.DataType.embedding(daft.DataType.float32(), dimensionality))\n",
    "    def embed(texts: Series) -> Series:\n",
    "        if len(texts) == 0:\n",
    "            return Series.from_pylist([])\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            embeddings: np.ndarray[float] = model.encode(\n",
    "                texts.to_pylist(), convert_to_numpy=True, show_progress_bar=False\n",
    "            )\n",
    "\n",
    "        return Series.from_numpy(embeddings)\n",
    "\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cAf0A7JxHv-7"
   },
   "source": [
    "# Image Embedding\n",
    "\n",
    "In both the OCR and PDF file extracting flows, we are able to interrogate the PDF file and ask it to provide us bounding boxes of all elements in the page that are marked as images. It is often useful to be able to analyze all embedded images in a PDF. For instance, we may want to grab these because they're non-vector graphics figures in a scientific paper or charts in a business report.\n",
    "\n",
    "Similiar to the text embedding UDF, we can define an image embedding UDF and use it to produce embeddings for all extracted image elements from our documents.\n",
    "\n",
    "We will also need to define a UDF to crop each page image according to the bounding boxes for each embedded image. This `ImageBboxProcessor` will need to get access to the original PDF bytes as well as the image bounding box information we have in our `ParsedPdf` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJSb2IH7HwIh"
   },
   "outputs": [],
   "source": [
    "@udf(return_dtype=daft.DataType.image())\n",
    "class ImageBboxProcessor:\n",
    "    def __init__(self, scale: float = 2.0) -> None:\n",
    "        self.scale = scale\n",
    "\n",
    "    def handle(self, pdf_bytes: bytes, doc: Union[ParsedPdf, dict]) -> Iterator[np.ndarray]:\n",
    "        parsed_doc = ParsedPdf.model_validate(doc) if isinstance(doc, dict) else doc\n",
    "        bytes_buffer = io.BytesIO(pdf_bytes)\n",
    "        with fitz.open(stream=bytes_buffer, filetype=\"pdf\") as pdf:\n",
    "            for i, parsed_page in enumerate(parsed_doc.pages):\n",
    "                page = pdf[i]\n",
    "                # rasterize page\n",
    "                pix = page.get_pixmap(matrix=fitz.Matrix(self.scale, self.scale))  # type: ignore\n",
    "                image = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "\n",
    "                for image_bbox in parsed_page.images:\n",
    "                    sub_image = image.crop(image_bbox.as_cropbox())\n",
    "                    sub_image_np = np.array(sub_image)\n",
    "                    yield sub_image_np\n",
    "\n",
    "    def __call__(self, pdf_bytes: Series, parsed: Series) -> list[np.ndarray]:\n",
    "        images = []\n",
    "        for pdf, doc in zip(pdf_bytes, parsed):\n",
    "            for image in self.handle(pdf, doc):\n",
    "                images.append(image)\n",
    "        return images\n",
    "\n",
    "\n",
    "def image_embedder_udf(model_name: str) -> UDF:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model, preprocess = clip.load(model_name, device=device, jit=True)\n",
    "\n",
    "    @udf(return_dtype=daft.DataType.embedding(daft.DataType.float32(), 512))\n",
    "    def embed(images: Series) -> Series:\n",
    "        if len(images) == 0:\n",
    "            return np.empty((0,), dtype=np.float32)\n",
    "\n",
    "        # B x C x H x W\n",
    "        image_batch = torch.stack([preprocess(Image.fromarray(image) for image in images.to_pylist())]).to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            embeddings_pt = model.encode_image(image_batch)\n",
    "\n",
    "        embeddings_np = embeddings_pt.detach().cpu().numpy()\n",
    "        return Series.from_numpy(embeddings_np)\n",
    "\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1_rosls8ykn"
   },
   "source": [
    "# Entire End-to-End Pipeline\n",
    "\n",
    "In Daft, we express our pipeline as a `daft.DataFrame` instance. We use the DataFrame's methods to produce transformations from one view of the data into another. Here, we'll combine all of the functionality defined in this tutorial into a complete DataFrame-based pipeline.\n",
    "\n",
    "\n",
    "### Options\n",
    "We will start out by defining the configuration options our pipeline uses -- change _any_ of these values and rerun to see how they affect the pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5EDaKsZ9TNc"
   },
   "outputs": [],
   "source": [
    "# Uses Tesseract to perform OCR if true. Otherwise tries to get text\n",
    "# directly from the file format.\n",
    "ocr: bool = False\n",
    "\n",
    "# Only handle the first N pages of each PDF. Some PDFs are very long and\n",
    "# can bog-down the pipeline as it waits for stragglers. For fast exploration,\n",
    "# set this limit. If you want to run on all pages of each PDF, set this to None.\n",
    "page_limit: Optional[int] = 10\n",
    "\n",
    "# If true, then extract bounding boxes pertaining to embedded images in the PDF.\n",
    "extract_images: bool = True\n",
    "\n",
    "# Determine how text boxes are grouped together to form more semantically\n",
    "# relevant passages of text. These options control herusitics in the\n",
    "# DocProcessor UDF. Increasing the thresholds and tolerance will cause more\n",
    "# distance text boxes to be grouped together.\n",
    "config = PipelineConfig(\n",
    "    row_tolerance=10,\n",
    "    y_thresh=15,\n",
    "    x_thresh=60,\n",
    "    group_paragraphs=True,\n",
    ")\n",
    "\n",
    "# The text embedding model to use. See HuggingFace for a more complete list!\n",
    "text_model = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "# This is the image embedding model to use. See the CLIP repository for more!\n",
    "image_model = \"ViT-B/32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ax-dpFeO9eH_"
   },
   "source": [
    "Checking some assumptions on our options for the dataframe-based processing pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJ0IN1bX8066"
   },
   "outputs": [],
   "source": [
    "if page_limit is not None:\n",
    "    if page_limit <= 0:\n",
    "        raise ValueError(f\"Page limit must be positive if specified! Invalid: {page_limit=}\")\n",
    "    print(f\"Limiting each PDF to the first {page_limit} pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_9dozpv9sOf"
   },
   "source": [
    "### Step 1: Enumerate S3 Keys\n",
    "\n",
    "Read the S3 bucket & key prefix and get full keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2Qf6DmW9sX4"
   },
   "outputs": [],
   "source": [
    "df = daft.from_glob_path(\n",
    "    \"s3://daft-public-data/tutorials/document-processing/industry_documents_library/pdfs/*\",\n",
    "    io_config=IO_CONFIG,\n",
    ")\n",
    "print(df.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ejLA1xw9xqt"
   },
   "source": [
    "### Step 2: Download PDFs\n",
    "\n",
    "Downloads the contents of each PDF file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIMq14mq9xyg"
   },
   "outputs": [],
   "source": [
    "df = df.select(\"path\").with_column_renamed(\"path\", \"url\")\n",
    "df = df.with_column(\"pdf_bytes\", col(\"url\").url.download(io_config=IO_CONFIG))\n",
    "print(df.schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rgx_35xx93NB"
   },
   "source": [
    "### Step 3: Load PDFs, Maybe Apply OCR\n",
    "\n",
    "Use the PDF and OCR libraries to load all of the documents and extract text boxes on their pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2KTrnqV93Tk"
   },
   "outputs": [],
   "source": [
    "df = df.with_column(\n",
    "    \"parsed\",\n",
    "    # NOTE: We can easily define a UDF that operates on multiple columns!\n",
    "    #\n",
    "    #       This UDF mainly uses the downloaded PDF contents, but it also\n",
    "    #       includes the URL to make a well-formed ParsedPdf instance.\n",
    "    #\n",
    "    #       We're configuring how the UDF operates by supplying the\n",
    "    #       constructor arguments using the `with_init_args` class method.\n",
    "    #\n",
    "    #       These arguments are applied here   and here\n",
    "    #                                    âŒ„âŒ„âŒ„  âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„âŒ„\n",
    "    LoadDirectAndParsePdf.with_init_args(ocr, page_limit, extract_images)(col(\"url\"), col(\"pdf_bytes\")),\n",
    "    #       We're providing the two columns to our UDF\n",
    "    #                                                     ^^^^^^^^^^  ^^^^^^^^^^^^^^^^\n",
    "    #                                                         here        and here\n",
    ")\n",
    "print(df.exclude(\"pdf_bytes\").schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xYiAmvITJEB"
   },
   "source": [
    "#### UDF Application\n",
    "\n",
    "A note on how Daft works -- in our above UDF application, we're providing [column expressions](https://docs.getdaft.io/en/stable/core_concepts/#exploding-columns) as input. Specifically, the [`col`](https://docs.getdaft.io/en/stable/api/expressions/#daft.expressions.col) part. When we write `col(\"url\")`, we're telling Daft to wire things up under the hood to reference the data in the `url` column of the dataframe.\n",
    "\n",
    "Breaking down the PDF loading and parsing UDF call, the first part is actually constructing the UDF instance:\n",
    "```python\n",
    "LoadDirectAndParsePdf.with_init_args(ocr, page_limit)\n",
    "```\n",
    "\n",
    "While the second part is actually applying that UDF to our two columns, `url` and `pdf_bytes`:\n",
    "```python\n",
    "(col(\"url\"), col(\"pdf_bytes\"))\n",
    "```\n",
    "\n",
    "Note that this is equivalent:\n",
    "```python\n",
    "f = LoadDirectAndParsePdf.with_init_args(ocr, page_limit)\n",
    "f(col(\"url\"), col(\"pdf_bytes\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buXEaRp996tg"
   },
   "source": [
    "### Step 4: Text Box Processing\n",
    "\n",
    "Process the parsed document representation: perform custom logic grouping text boxes into more cohesive units (e.g. lines or paragraphs). Configurable grouping logic uses spatial information to determine each text box's group membership.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7aY977I9632"
   },
   "outputs": [],
   "source": [
    "df = df.with_column(\n",
    "    \"processed_raw\",\n",
    "    DocProcessor.with_init_args(**config.model_dump())(col(\"parsed\")),\n",
    ")\n",
    "print(df.exclude(\"pdf_bytes\", \"parsed\").schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cE3zxai-Abv"
   },
   "source": [
    "Reformat the text boxes structured objects (the `ParsedPdf` pydantic class instances) into rows. Each row has the reading order index, the text, the page index, and the bounding box coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh3k2Bst-AkR"
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.explode(\"processed_raw\")\n",
    "    .with_column(\"page_index\", col(\"processed_raw\").struct.get(\"page_index\"))\n",
    "    .with_column(\"indexed_texts\", col(\"processed_raw\").struct.get(\"indexed_texts\"))\n",
    "    .explode(\"indexed_texts\")\n",
    "    .exclude(\"processed_raw\")\n",
    ")\n",
    "print(df.exclude(\"pdf_bytes\", \"parsed\").schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORvDbQG7gcGX"
   },
   "source": [
    "#### Explaining Structure Access Expressions\n",
    "\n",
    "Note that we're using [`.struct`](https://docs.getdaft.io/en/v0.4.6/api_docs/doc_gen/expression_methods/daft.struct.html) to construct an expression that allows Daft to extract individual field values from our complex document structure.\n",
    "\n",
    "When write `col(\"text_blocks\").struct.get(\"bounding_box\")`, we're telling Daft that we want to access the `bounding_box` field of each element from the `text_blocks` column. From this, we can provide additional field-selecting logic (e.g. `[\"x\"]` to get the value for field `x` on the `bounding_box` value from each structure in `text_blocks`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQNF0AMu-F02"
   },
   "source": [
    "The last part of our text box processing step is to extract the text and bounding box coordinates into their own columns. We also want to preserve the reading order index as its own column too.\n",
    "\n",
    "This format makes it easier to form follow up queries on our data, such as:\n",
    "\"what are the first 10 pieces of text on the first page of each document?\" or\n",
    "\"what text appears in the bottom-right quadrant of each page?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6yr5H-v-F9j"
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    df.with_column(\"text_blocks\", col(\"indexed_texts\").struct.get(\"text\"))\n",
    "    .with_column(\"reading_order_index\", col(\"indexed_texts\").struct.get(\"index\"))\n",
    "    .exclude(\"indexed_texts\")\n",
    "    .with_column(\"text\", col(\"text_blocks\").struct.get(\"text\"))\n",
    "    .with_column(\"x\", col(\"text_blocks\").struct.get(\"bounding_box\")[\"x\"])\n",
    "    .with_column(\"y\", col(\"text_blocks\").struct.get(\"bounding_box\")[\"y\"])\n",
    "    .with_column(\"h\", col(\"text_blocks\").struct.get(\"bounding_box\")[\"h\"])\n",
    "    .with_column(\"w\", col(\"text_blocks\").struct.get(\"bounding_box\")[\"w\"])\n",
    "    .exclude(\"text_blocks\")\n",
    ")\n",
    "print(df.exclude(\"pdf_bytes\", \"parsed\").schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3m2juDOJ-JoX"
   },
   "source": [
    "### Step 5: Text Embeddings\n",
    "\n",
    "The penultimate step is to produce embeddings for each piece of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPmkTKZT-Jy-"
   },
   "outputs": [],
   "source": [
    "df = df.with_column(\"text_embeddings\", text_embedder_udf(text_model)(col(\"text\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rzePpv1K-i1"
   },
   "source": [
    "### Step 6: Image Embeddings\n",
    "\n",
    "The final step is to produce embeddings for all embedded images in the PDF. Note that this only ocurrs if `extract_images is True`. Change the configuration to change behavior here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItrY4z4MK-rs"
   },
   "outputs": [],
   "source": [
    "if extract_images:\n",
    "    df = df.with_column(\"images\", ImageBboxProcessor(df[\"pdf_bytes\"], df[\"parsed\"]))\n",
    "    df = df.with_column(\"image_embeddings\", image_embedder_udf(image_model)(df[\"images\"]))\n",
    "    df = df.exclude(\"images\")\n",
    "else:\n",
    "    print(\"Not embedding images because embedded images were not extracted.\")\n",
    "df = df.exclude(\"pdf_bytes\", \"parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2XVOl6eUBC-"
   },
   "source": [
    "## Executing a Lazily Constructed Pipeline\n",
    "\n",
    "At this point, our `df` value contains the entire pipeline. We can show the final schema with a simple method call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ye3tsBmUBO5"
   },
   "outputs": [],
   "source": [
    "print(f\"Pipeline constructed:\\n{df.schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZf-SsL_UEvu"
   },
   "source": [
    "And we can peek under the hood and ask Daft to show us the entire logical plan  that it will run when we ask it to execute our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VGTKUMEUNNR"
   },
   "outputs": [],
   "source": [
    "print(f\"Execution plan for pipeline:\\n{df.explain()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvl_Our9-N4o"
   },
   "source": [
    "#### Writing to Parquet\n",
    "\n",
    "In Daft, [DataFrames are _lazy_](https://docs.getdaft.io/en/stable/core_concepts/#dataframe): they are only evaluated once it is necessary.\n",
    "\n",
    "By invoking the [`write_parquet`](https://docs.daft.ai/en/stable/api/io/#daft.dataframe.DataFrame.write_parquet) method, we force the DataFrame to evaluate so that its contents can be written out to disk!\n",
    "\n",
    "If we wanted to keep the values in-memory, we would call [`.collect()`](https://docs.daft.ai/en/stable/api/io/#daft.dataframe.DataFrame.collect) on the DataFrame. This process is known as _materializing the DataFrame_. Note that the `collect` method returns a new DataFrame holding the actual computed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xU3nDcJb-sNH"
   },
   "outputs": [],
   "source": [
    "output_dir = \"./doc_proc_tutorial_parquet\"\n",
    "print(f\"Writing out as Parquet files to: {output_dir}\")\n",
    "\n",
    "parquet_out = df.write_parquet(output_dir).to_pydict()\n",
    "print(f\"Complete! Wrote {df.count_rows()} rows:\\n{parquet_out}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
