{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e6d950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "217b31c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 64.0,\n",
      " 'GPU': 8.0,\n",
      " 'accelerator_type:V100': 8.0,\n",
      " 'memory': 556793856000.0,\n",
      " 'node:10.0.0.132': 1.0,\n",
      " 'node:10.0.0.157': 1.0,\n",
      " 'node:10.0.0.25': 1.0,\n",
      " 'node:10.0.1.107': 1.0,\n",
      " 'node:10.0.1.154': 1.0,\n",
      " 'node:10.0.1.175': 1.0,\n",
      " 'node:10.0.1.198': 1.0,\n",
      " 'node:10.0.2.127': 1.0,\n",
      " 'node:10.0.2.203': 1.0,\n",
      " 'object_store_memory': 166766611563.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DaftContext(runner_config=_RayRunnerConfig(address=None, max_tasks_per_core=None, max_refs_per_core=None, batch_dispatch_coeff=None), disallow_set_runner=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import daft\n",
    "from daft import DataFrame\n",
    "from PIL import Image\n",
    "import io\n",
    "import ray\n",
    "from pprint import pprint\n",
    "USE_RAY_REMOTE = True\n",
    "\n",
    "if USE_RAY_REMOTE:\n",
    "    ray.init(\n",
    "        address=\"ray://localhost:10001\",\n",
    "        runtime_env={\"pip\": [\n",
    "            \"https://anaconda.org/daft-nightly/getdaft/0.0.22%2Bdev0034.d218a0c/download/getdaft-0.0.22%2Bdev0034.d218a0c-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\",\n",
    "            \"git+https://github.com/openai/CLIP.git\",\n",
    "            \"pillow\",\n",
    "            \"s3fs\",\n",
    "        ]},\n",
    "    )\n",
    "    pprint(ray.available_resources())\n",
    "daft.context.set_runner_ray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a87b68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_df = DataFrame.read_parquet(\"s3://daft-public-data/coco-2017/mscoco.parquet\")\n",
    "\n",
    "if USE_RAY_REMOTE:\n",
    "    coco_df = coco_df.repartition(64).limit(10000)\n",
    "else:\n",
    "    coco_df = coco_df.repartition(8).limit(200)\n",
    "\n",
    "coco_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e99ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = coco_df.select('URL').distinct()\n",
    "images_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aced0ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images_df = images_df.with_column(\n",
    "    \"image\",\n",
    "    images_df[\"URL\"].url.download().apply(\n",
    "        lambda data: Image.open(io.BytesIO(data)).resize((512, 512)) if data else None, \n",
    "        return_type=Image.Image,\n",
    "    )\n",
    ").where(~col('image').is_null())\n",
    "images_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c6f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from daft import udf, col\n",
    "from daft.resource_request import ResourceRequest\n",
    "from typing import List\n",
    "import clip\n",
    "import torch\n",
    "\n",
    "USE_GPU = True\n",
    "class ClipExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"ViT-B/32\",\n",
    "        batch_size: int = 128,\n",
    "    ) -> None:\n",
    "        self.device = \"cuda\" if USE_GPU else \"cpu\"\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.model, self.preprocess = clip.load(model_name, device=self.device, jit=True)\n",
    "        self.num_dims = 512\n",
    "    \n",
    "    @staticmethod\n",
    "    def batched(iterable, n=32):\n",
    "        l = len(iterable)\n",
    "        for ndx in range(0, l, n):\n",
    "            yield iterable[ndx:min(ndx + n, l)]\n",
    "    \n",
    "        \n",
    "@udf(return_type=np.ndarray)\n",
    "class ImageClipExtractor(ClipExtractor):\n",
    "    \"\"\"Extracts CLIP embeddings from images\"\"\"\n",
    "    def __call__(self, images: List[Image.Image | None]) -> np.ndarray:\n",
    "        if not images:\n",
    "            return []\n",
    "        \n",
    "        clip_embeddings = np.zeros((0, self.num_dims))\n",
    "        for images_batch in ClipExtractor.batched(images, n=self.batch_size):\n",
    "            image = torch.stack([self.preprocess(img) for img in images_batch]).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                image_features = self.model.encode_image(image)\n",
    "                image_features = image_features.detach().cpu().float()\n",
    "                norm = image_features.norm(p=2, dim=1, keepdim=True)\n",
    "            clip_embeddings = np.concatenate([clip_embeddings, (image_features / norm).numpy()])\n",
    "            \n",
    "        return clip_embeddings    \n",
    "    \n",
    "\n",
    "\n",
    "@udf(return_type=np.ndarray)\n",
    "class TextClipExtractor(ClipExtractor):    \n",
    "    def __call__(self, text: List[str]) -> np.ndarray:\n",
    "        if not text:\n",
    "            return []\n",
    "        \n",
    "        clip_embeddings = np.zeros((0, self.num_dims))\n",
    "        for text_batch in ClipExtractor.batched(text, n=self.batch_size):\n",
    "            tokens = clip.tokenize(text_batch).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                features = self.model.encode_text(tokens)\n",
    "                features = features.detach().cpu().float()\n",
    "                features /= features.norm(p=2, dim=-1, keepdim=True)\n",
    "            clip_embeddings = np.concatenate([clip_embeddings, features.numpy()], axis=0)\n",
    "            \n",
    "        return clip_embeddings    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_df = images_df.with_column('image_clip_embedding', \n",
    "                                  ImageClipExtractor(col('image')),\n",
    "                                  resource_request=ResourceRequest(num_gpus=0.25))\n",
    "\n",
    "\n",
    "text_df = coco_df.with_column('text_clip_embedding',\n",
    "                              TextClipExtractor(col('TEXT')),\n",
    "                              resource_request=ResourceRequest(num_gpus=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60d064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = images_df.join(text_df, on='URL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "@udf(return_type=float)\n",
    "def cosine_similarity(A: List[np.ndarray], B: List[np.ndarray]) -> List[float]:\n",
    "    return [np.dot(a,b) for a,b in zip(A, B)]\n",
    "\n",
    "joined_df = joined_df.with_column(\"cosine_similarity\", cosine_similarity(col(\"image_clip_embedding\"), col(\"text_clip_embedding\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0652ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ab1b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result = joined_df.select('URL', 'TEXT', 'image', 'cosine_similarity').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a7b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_caption = (result.groupby('URL')\n",
    "                .max('cosine_similarity') \n",
    "                .join(result,\n",
    "                      on=['URL', 'cosine_similarity'])\n",
    "                .sort(\"cosine_similarity\", desc=True)).collect()\n",
    "best_caption.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_caption.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6cc51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e9819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
