{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b8a5f16-3d51-4690-80b3-95c7899c5474",
   "metadata": {},
   "source": [
    "# Using Ray for Scaling Up\n",
    "\n",
    "Daft's default PyRunner is great for experimentation on your laptop, but when it comes times to running much more computationally expensive jobs that need to take advantage of large scale parallelism, you can run Daft on a [Ray](https://www.ray.io/) cluster instead.\n",
    "\n",
    "## What is a Ray Cluster, and why do I need it?\n",
    "\n",
    "Ray is a framework that exposes a Python interface for running distributed computation over a cluster of machines. Daft is built to use Ray as a backend for running dataframe operations, allowing it to scale to huge amounts of data and computation.\n",
    "\n",
    "However even if you do not have a big cluster to use Ray, you can run Ray locally on your laptop (in which case it would spin up a Ray cluster of just a single machine: your laptop), and using Daft's Ray backend would allow Daft to fully utilize your machine's cores.\n",
    "\n",
    "## Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe8f7c1-ae08-49b9-96c7-4b16cf48f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install getdaft --pre --extra-index-url https://pypi.anaconda.org/daft-nightly/simple\n",
    "!pip install Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3ef6f2-550b-4668-9d41-0417e98e22f7",
   "metadata": {},
   "source": [
    "First, we will run our code without changing the runner. By default, Daft uses the \"Python Runner\" which runs all processing in a single Python process.\n",
    "\n",
    "Let's try to download the images from our previous [Text-to-Image Generatation tutorial](https://colab.research.google.com/github/Eventual-Inc/Daft/blob/main/tutorials/text_to_image/text_to_image_generation.ipynb) with the PyRunner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a17a23-593d-45f4-8ec5-48d25d56cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "PARQUET_URL = \"https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_6.5plus/resolve/main/data/train-00000-of-00001-6f24a7497df494ae.parquet\"\n",
    "PARQUET_PATH = \"laion_improved_aesthetics_6_5.parquet\"\n",
    "\n",
    "if not os.path.exists(PARQUET_PATH):\n",
    "    with open(PARQUET_PATH, \"wb\") as f:\n",
    "        response = urllib.request.urlopen(PARQUET_URL)\n",
    "        f.write(response.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448057cd-52f7-449d-9c1c-6d6368de9cb2",
   "metadata": {},
   "source": [
    "We limit the dataset to 160 rows and repartition it into 8 partitions for demonstration purposes. This just means that our data will be divided into 8 approximately equal-sized \"chunks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2639220-6c8f-48e2-9d8b-8301de21b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import DataFrame, col, udf\n",
    "\n",
    "parquet_df = DataFrame.read_parquet(PARQUET_PATH).limit(160).repartition(8)\n",
    "parquet_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601555b-ab09-4c6a-87dd-77a761b351b3",
   "metadata": {},
   "source": [
    "## Use the PyRunner to download data from URLs\n",
    "\n",
    "Now, let's try downloading the data from the URLs with `.url.download()` with the default PyRunner backend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bf3fde-5a64-49e3-8f32-cbf180905efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "@udf(return_type=PIL.Image.Image)\n",
    "def to_image(bytes_col):\n",
    "    images = []\n",
    "    for b in bytes_col:\n",
    "        if b is not None:\n",
    "            try:\n",
    "                images.append(PIL.Image.open(io.BytesIO(b)))\n",
    "            except:\n",
    "                images.append(None)\n",
    "        else:\n",
    "            images.append(None)\n",
    "    return images\n",
    "\n",
    "images_df = parquet_df.with_column(\"images\", to_image(col(\"URL\").url.download()))\n",
    "images_df_pandas = images_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ece805-b991-4b75-a3c0-ca00354365f2",
   "metadata": {},
   "source": [
    "Note how long this took (on Google Colab, it should take approximately 20 seconds)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd3c834-103b-46f4-9aba-10db0db06e14",
   "metadata": {},
   "source": [
    "## Using the RayRunner with a local cluster\n",
    "\n",
    "Great, now let's use the RayRunner instead and see how we can leverage some parallelism to speed up our workload!\n",
    "\n",
    "To activate the RayRunner, you can either set environment variables for program execution like so:\n",
    "\n",
    "```\n",
    "export DAFT_RUNNER=ray\n",
    "export DAFT_RAY_ADDRESS=...\n",
    "```\n",
    "\n",
    "The `DAFT_RAY_ADDRESS` variable can be left unset to have Daft initialize a default local Ray cluster for you, set to `auto` to automatically detect a Ray cluster running locally, or set to `ray://...` to access a remote Ray cluster.\n",
    "\n",
    "Alternatively, you can set the configs programatically at the start of your program execution, which is what we will demonstrate here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4929364e-b74b-4573-bdcf-cdd9d97e80a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: TO PROCEED WITH THE TUTORIAL, WE WILL RESTART THE NOTEBOOK RUNTIME\n",
    "# We do this because we need to reset the Runner that Daft is using. Daft expects any config changes to be performed at program initialization.\n",
    "exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dfdb29-3ed8-45c3-8ad1-67c13cbdc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "RAY_ADDRESS = None\n",
    "daft.context.set_runner_ray(\n",
    "    # You may provide Daft with the address to an existing Ray cluster if you have one!\n",
    "    # If this is not provided, Daft will default to spinning up a single-node Ray cluster consisting of just your current local machine\n",
    "    address=RAY_ADDRESS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7ddec-00c3-471f-807a-bcedcf5c98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from daft import DataFrame, col, udf\n",
    "\n",
    "PARQUET_PATH = \"laion_improved_aesthetics_6_5.parquet\"\n",
    "parquet_df = DataFrame.read_parquet(PARQUET_PATH).limit(160).repartition(8)\n",
    "parquet_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32519989-870e-46ee-be9a-5fc417ec7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "@udf(return_type=PIL.Image.Image)\n",
    "def to_image(bytes_col):\n",
    "    images = []\n",
    "    for b in bytes_col:\n",
    "        if b is not None:\n",
    "            try:\n",
    "                images.append(PIL.Image.open(io.BytesIO(b)))\n",
    "            except:\n",
    "                images.append(None)\n",
    "        else:\n",
    "            images.append(None)\n",
    "    return images\n",
    "\n",
    "images_df = parquet_df.with_column(\"images\", to_image(col(\"URL\").url.download()))\n",
    "images_df_pandas = images_df.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2278cc6-f3c6-44da-9f38-684e220a43e1",
   "metadata": {},
   "source": [
    "With exactly the same code, we were able to achieve a 2x speedup in execution (in Google Colab, this step takes about 10 seconds) - what happened here?\n",
    "\n",
    "It turns out that our workload is [IO Bound](https://en.wikipedia.org/wiki/I/O_bound) because most of the time is spent waiting for data to be downloaded from the URL.\n",
    "\n",
    "By default, the `.url.download()` UDF requests `num_cpus=1`. Since our Google Colab machine has 2 CPUs, the RayRunner is able to run two of these UDFs in parallel, hence achieving a 2x increase in throughput!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8430f756-f641-4bfa-9425-8d72e200d726",
   "metadata": {},
   "source": [
    "## Remote Ray Clusters\n",
    "\n",
    "We have seen that using the RayRunner even locally provides us with some speedup already. However, the real power of distributed computing is in allowing us to access thousands of CPUs and GPUs in the cloud, on a remote Ray cluster.\n",
    "\n",
    "For example, UDFs that request for a single GPU with `@udf(num_gpus=1)` can run in parallel across hundreds of GPUs on a remote Ray cluster, effortlessly scaling your workloads up to take full advantage of the available hardware.\n",
    "\n",
    "To run Daft on large clusters, check out [Eventual](https://www.eventualcomputing.com) where you have access to a fully managed platform for running Daft at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f159a45-dbf5-4e19-b2c6-0d15474b270c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "abe0cbefa28213872cf2c91d6aa47443089aa2a6ddfc370b260793ec957ca67a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
