{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab5e477-5077-4fe3-b0f1-908473b1f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "import threading\n",
    "import PIL.Image\n",
    "import ray.data\n",
    "import boto3\n",
    "import io\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import hub\n",
    "\n",
    "import daft\n",
    "from daft.fields import DaftImageField\n",
    "from daft.experimental.dataclasses import dataclass\n",
    "from daft.datarepo import get_client\n",
    "from daft.datarepo.query import functions as F\n",
    "\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfe5029-9a7d-483a-9239-b522aa5c7c4a",
   "metadata": {},
   "source": [
    "# Daft Benchmarking\n",
    "\n",
    "Benchmarking the performance of Daft on a subset of the OpenImages dataset\n",
    "\n",
    "The OpenImages dataset is hosted publicly in AWS through the `s3://open-images-dataset` bucket. However this bucket is slow to access because we share bandwidth with all other users of the data, and so we first copy data into our own AWS account/bucket using the command line:\n",
    "\n",
    "```\n",
    "aws s3 sync s3://open-images-dataset/validation s3://eventual-data-test-bucket/benchmarking/open-images-dataset/validation\n",
    "```\n",
    "\n",
    "## Ingesting Datarepo\n",
    "\n",
    "Now that the data is in our own buckets, the first step in this notebook will ingest the data as a Daft Datarepo.\n",
    "\n",
    "We do the following:\n",
    "\n",
    "1. Get a list of all the images in our S3 bucket\n",
    "2. Use a ThreadPoolExecutor to download these JPEG images efficiently into a Ray cluster\n",
    "3. Resize the images to 256 x 256\n",
    "4. Write the images to a Datarepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57299c9b-3f71-4405-b363-a10917b78ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "daft.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856cb88a-056b-4cec-8a00-a94165ce1f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"eventual-data-test-bucket\"\n",
    "DATA_PREFIX = \"benchmarking/open-images-dataset/validation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790ce4d0-d87c-4cb1-9396-a266c6ba6b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_paginator = boto3.client(\"s3\").get_paginator('list_objects_v2')\n",
    "\n",
    "objs = []\n",
    "for page in s3_paginator.paginate(Bucket=BUCKET, Prefix=DATA_PREFIX):\n",
    "    for content in page.get('Contents', ()):\n",
    "        objs.append(content['Key'])\n",
    "        \n",
    "print(f\"Number of objects: {len(objs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09711567-dd88-4cfc-be82-2137e3a5fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OpenImageRaw:\n",
    "    key: str\n",
    "    img: PIL.Image.Image = DaftImageField()\n",
    "\n",
    "\n",
    "def download_batch(batch: List[str]) -> List[bytes]:\n",
    "    def download_single(key: str) -> bytes:\n",
    "        local = threading.local()\n",
    "        if \"boto_session\" not in local.__dict__:\n",
    "            local.boto_session = boto3.session.Session()\n",
    "        s3 = local.boto_session.client('s3')\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=key)\n",
    "        body = response[\"Body\"]\n",
    "        contents = body.read()\n",
    "        body.close()\n",
    "        return (key, contents)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor : \n",
    "        return [res for res in executor.map(download_single, batch)]\n",
    "\n",
    "\n",
    "def resized_pil_image(payload: bytes, size:int=256) -> PIL.Image.Image:\n",
    "    \"\"\"Loads a payload of bytes as a PIL image and resizes it to specified given size\"\"\"\n",
    "    with io.BytesIO(payload) as f:\n",
    "        try:\n",
    "            img = PIL.Image.open(f)\n",
    "            img = img.resize((size,size))\n",
    "            img = img.convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            img = PIL.Image.new(\"RGB\", (size, size))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be81dc7a-318d-4015-a6cc-7218ad4b0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = ray.data.from_items(objs)\n",
    "ds = ds.map_batches(\n",
    "    download_batch\n",
    ").map(\n",
    "    lambda tup: OpenImageRaw(key=tup[0], img=resized_pil_image(tup[1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537043e4-2763-410b-8ef5-b917720dee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client()\n",
    "client.create(\"open-images-validation-resized\", dtype=OpenImageRaw, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3dee3-1448-43d9-a179-7d9072cf9adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "datarepo = client.from_id(\"open-images-validation-resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e8845-acdf-45be-a8c9-56ef24c34d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "written_files = datarepo.overwrite(ds, rows_per_partition=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1244a6-6e65-4a1d-ac1d-8c917a8cf14c",
   "metadata": {},
   "source": [
    "## Running queries/processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5005eb70-5996-4ffa-972d-c84960724e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.batch_func(batch_size=8)\n",
    "class BatchInferModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here we init our model as well as needed data transforms\n",
    "        \"\"\"\n",
    "        hub.set_dir(\"/tmp/.torchcache\")\n",
    "        self.model_name = \"resnet18\"\n",
    "        model = torchvision.models.resnet18(pretrained=True).eval()\n",
    "        self.feature_extractor = torchvision.models.feature_extraction.create_feature_extractor(\n",
    "            model=model, \n",
    "            return_nodes={'avgpool': 'embedding'}\n",
    "        )\n",
    "        self.to_tensor = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            )]\n",
    "        )\n",
    "    \n",
    "    def prepare_batch(self, image_data: List[PIL.Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Here we convert our PIL image to a normalized tensor\n",
    "        \"\"\"\n",
    "        return torch.stack([self.to_tensor(img) for img in image_data])\n",
    "    \n",
    "    def __call__(self, image_data: List[PIL.Image.Image]) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Here we extract our embedding with resnet 18\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            tensor = self.prepare_batch(image_data)\n",
    "            embedding =  self.feature_extractor(tensor.float())['embedding'].view(len(image_data), -1)\n",
    "            np_embedding = embedding.cpu().numpy()\n",
    "            dim = np_embedding.shape[1]\n",
    "            per_image_embedding = np.vsplit(np_embedding, np.arange(1, len(image_data)))\n",
    "            return per_image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a5d4b0-c4fd-4020-bb7a-9823f6f610ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datarepo = client.from_id(\"open-images-validation-resized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580fb414-af31-4ad3-810b-009026ff0935",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = datarepo.query(OpenImageRaw).with_column(\"embeddings\", BatchInferModel(\"img\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5113d5-9954-426e-834d-fe349fc305b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = query.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e47415-4f15-4f65-a6c0-1d29f72b5e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdf6b15f3401b15b2ff3a55b53b645bfbfd7be17162aec88a7c30eb1caeda578"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
