{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c813429",
   "metadata": {},
   "source": [
    "# Daft Demo\n",
    "\n",
    "## Here we show a demo of Daft of the following: \n",
    "- Initializing our cluster\n",
    "- Daft Data Repos\n",
    "- Use a Python Dataclass to define a Schema\n",
    "- Load existing data from our Data Repos\n",
    "- Write a function to download data from the web\n",
    "- Write a function to decode and resize the image\n",
    "- Write our own Schema for image storage\n",
    "- Save downloaded images to the cloud\n",
    "- Write our own embedding extractor for batch inference\n",
    "- Save our embeddings to a data repo\n",
    "- Preview our Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf4290",
   "metadata": {},
   "source": [
    "## Initializing our cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdff99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import daft\n",
    "\n",
    "daft.init(ray_address=\"ray://localhost:10001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9efc9b",
   "metadata": {},
   "source": [
    "## Daft Data Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e501a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from daft import datarepo\n",
    "\n",
    "datarepo_client = datarepo.get_client()\n",
    "datarepo_client.list_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b598a",
   "metadata": {},
   "source": [
    "## Defining our Own Schema\n",
    "- ORM for binary data\n",
    "- Translates to parquet under the hood\n",
    "- Support for logical types like images, numpy arrays and any other types that you can define yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca769a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from daft import dataclass\n",
    "\n",
    "@dataclass\n",
    "class OpenImagesMetadata:\n",
    "    url: str\n",
    "    size: int\n",
    "    id: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e133283",
   "metadata": {},
   "source": [
    "## Reading Data from our Data Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78703eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "datarepo = datarepo_client.from_id(\"openimages-dc-8000-v6\")\n",
    "query = datarepo.query(OpenImagesMetadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c7adf-503f-49a6-9eab-cf5e52c17f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde50161",
   "metadata": {},
   "source": [
    "## Previewing our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99920d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = query.limit(5)\n",
    "print(sample)\n",
    "sample.execute().show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c4ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import requests\n",
    "from typing import List\n",
    "\n",
    "from daft.datarepo.query import functions as F\n",
    "\n",
    "def download_single(url: str) -> bytes:\n",
    "    r = requests.get(url)\n",
    "    if r.status_code == 200:\n",
    "        return r.content\n",
    "    else:\n",
    "        return b''\n",
    "\n",
    "@F.batch_func(batch_size=64)\n",
    "def download_batch(batch: List[str]) -> List[bytes]:\n",
    "    with concurrent.futures.ThreadPoolExecutor() as exector : \n",
    "        futures = exector.map(download_single, batch)\n",
    "        return list(futures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1853fe",
   "metadata": {},
   "source": [
    "### Download via a batched map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e69109-6014-4064-be81-2e050959da16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with_downloaded_column = query.with_column(\"payload\", download_batch(\"url\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3da2fc-303c-4259-a95a-9e5b20246ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_downloaded_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc076a",
   "metadata": {},
   "source": [
    "## Decode and Resize our downloaded images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8adb791-1319-414f-94b8-57d71ba28cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import io\n",
    "\n",
    "@F.func\n",
    "def resized_pil_image(payload: bytes, size:int=256) -> PIL.Image.Image:\n",
    "    \"\"\"Loads a payload of bytes as a PIL image and resizes it to specified given size\"\"\"\n",
    "    with io.BytesIO(payload) as f:\n",
    "        try:\n",
    "            img = PIL.Image.open(f)\n",
    "            img = img.resize((size,size))\n",
    "            img = img.convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            img = PIL.Image.new(\"RGB\", (size, size))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbcde0-5d3a-4447-91d7-f00f3afefd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_pil_column = with_downloaded_column.with_column(\"img\", resized_pil_image(\"payload\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0229f79-28eb-4b39-9b90-87d5b1211bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_pil_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8b66b9",
   "metadata": {},
   "source": [
    "### Lets look at our images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = with_pil_column.limit(5).execute()\n",
    "items = ds.take(5)\n",
    "items[0].img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e1806",
   "metadata": {},
   "source": [
    "### Defining our function for Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca0ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "@F.batch_func(batch_size=8)\n",
    "class BatchInferModel:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Here we init our model as well as needed data transforms\n",
    "        \"\"\"\n",
    "        self.model_name = \"resnet18\"\n",
    "        model = torchvision.models.resnet18(pretrained=True).eval()\n",
    "        self.feature_extractor = torchvision.models.feature_extraction.create_feature_extractor(\n",
    "            model=model, \n",
    "            return_nodes={'avgpool': 'embedding'}\n",
    "        )\n",
    "        self.to_tensor = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            )]\n",
    "        )\n",
    "    \n",
    "    def prepare_batch(self, image_data: List[PIL.Image.Image]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Here we convert our PIL image to a normalized tensor\n",
    "        \"\"\"\n",
    "        return torch.stack([self.to_tensor(img) for img in image_data])\n",
    "    \n",
    "    def __call__(self, image_data: List[PIL.Image.Image]) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Here we extract our embedding with resnet 18\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            tensor = self.prepare_batch(image_data)\n",
    "            embedding =  self.feature_extractor(tensor.float())['embedding'].view(len(image_data), -1)\n",
    "            np_embedding = embedding.cpu().numpy()\n",
    "            dim = np_embedding.shape[1]\n",
    "            per_image_embedding = np.vsplit(np_embedding, np.arange(1, len(image_data)))\n",
    "            return per_image_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7640d6e",
   "metadata": {},
   "source": [
    "## Running large scale batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c4c5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with_embeddings = with_pil_column \\\n",
    "    .with_column(\"embedding\", BatchInferModel(\"img\")) \\\n",
    "    .with_column(\"mean\", F.func(lambda e: e.mean(), return_type=float)(\"embedding\")) \\\n",
    "    .with_column(\"std\", F.func(lambda e: e.std(), return_type=float)(\"embedding\")) \\\n",
    "    .with_column(\"dim\", F.func(lambda e: e.shape[1], return_type=int)(\"embedding\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42fb575-e3c3-4498-ad9e-db72d7748dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382f72f",
   "metadata": {},
   "source": [
    "## Save our extracted embeddings to the cloud in Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824efc35-2c31-48a0-b91b-bd1b28cecc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datarepo if it does not exist\n",
    "\n",
    "@dataclass\n",
    "class ProcessedEmbedding:\n",
    "    url: str\n",
    "    dim: int\n",
    "    mean: float\n",
    "    std: float\n",
    "    embedding: np.ndarray\n",
    "\n",
    "embeddings_datarepo = datarepo_client.create(\"open-images-8k-processed-embeddings\", ProcessedEmbedding, exists_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ccb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query = with_embeddings.write_datarepo(\n",
    "    embeddings_datarepo,\n",
    "    ProcessedEmbedding,\n",
    "    mode=\"overwrite\",\n",
    "    rows_per_partition=128,\n",
    ")\n",
    "\n",
    "write_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5167d6-ee45-43d6-8227-5bc3c569ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = write_query.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a08f1",
   "metadata": {},
   "source": [
    "## Take a look at what was written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9c362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_datarepo.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f336a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_datarepo = datarepo_client.from_id(\"open-images-8k-processed-embeddings\")\n",
    "ds = embeddings_datarepo.query(ProcessedEmbedding).limit(5).execute()\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3cd964-32fb-49c9-b41e-a94b9b5a0d53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95adfe95-b820-4567-a0a8-661b5fb791ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
