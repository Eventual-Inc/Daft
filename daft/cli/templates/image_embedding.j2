"""
Image Embedding Project
Generated by Daft CLI
"""

import daft
from daft.functions import embed_image, decode_image


def main():
    # Load data
    {% if data_source == "CSV" %}
    df = daft.read_csv("{{ data_source_path }}")
    {% elif data_source == "JSONL" %}
    df = daft.read_json("{{ data_source_path }}")
    {% elif data_source == "Parquet" %}
    df = daft.read_parquet("{{ data_source_path }}")
    {% elif data_source == "Huggingface" %}
    df = daft.read_huggingface("{{ data_source_path }}")
    {% elif data_source == "Raw Files" %}
    df = daft.from_glob_path("{{ data_source_path }}")
    # Create content column from image files
    df = df.with_column("content", decode_image(df["path"].download()))
    {% else %}
    # TODO: Load your data here
    # Example: df = daft.read_csv("path/to/your/data.csv")
    df = None
    {% endif %}

    # Generate embeddings
    # If the selected column contains file paths (string), decode them first
    {% if data_source != "Raw Files" %}
    # Assuming {{ selected_columns[0] }} contains image file paths or binary data
    # If paths, use: decode_image(df["{{ selected_columns[0] }}"].download())
    # If binary, use: decode_image(df["{{ selected_columns[0] }}"])
    df = df.with_column(
        "{{ output_column }}",
        embed_image(decode_image(df["{{ selected_columns[0] }}"].download()))
    )
    {% else %}
    df = df.with_column(
        "{{ output_column }}",
        embed_image(df["content"])
    )
    {% endif %}

    # Write output
    {% if output_data_source == "Parquet" %}
    df.write_parquet("output.parquet")
    {% elif output_data_source == "CSV" %}
    df.write_csv("output.csv")
    {% elif output_data_source == "JSON" %}
    df.write_json("output.jsonl")
    {% elif output_data_source == "Turbopuffer" %}
    # Prepare dataframe for Turbopuffer: id and vector columns
    {% if data_source == "Raw Files" %}
    # For Raw Files, use the path column as id
    df = df.select(
        df["path"].cast(str).alias("id"),
        df["{{ output_column }}"].alias("vector")
    )
    {% else %}
    df = df.select(
        df["{{ selected_columns[0] }}"].cast(str).alias("id"),
        df["{{ output_column }}"].alias("vector")
    )
    {% endif %}
    # TODO: Configure Turbopuffer namespace and API key
    df.write_turbopuffer("your_namespace", api_key="your_api_key")
    {% elif output_data_source == "Iceberg" %}
    # TODO: Configure Iceberg table
    # df.write_iceberg(table)
    {% elif output_data_source == "Delta Lake" %}
    df.write_deltalake("output_path")
    {% elif output_data_source == "Lance" %}
    df.write_lance("output.lance")
    {% elif output_data_source == "Clickhouse" %}
    # TODO: Configure Clickhouse connection
    # df.write_clickhouse("table_name", host="localhost", port=8123)
    {% elif output_data_source == "Huggingface" %}
    df.write_huggingface("your_username/your_repo")
    {% elif output_data_source == "Bigtable" %}
    # TODO: Configure Bigtable connection
    # df.write_bigtable(project_id="project", instance_id="instance", table_id="table", row_key_column="id")
    {% endif %}


if __name__ == "__main__":
    main()
