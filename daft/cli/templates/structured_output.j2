"""
Structured Output Generation Project
Generated by Daft CLI
"""

import daft
from daft.functions import prompt, file
from pydantic import BaseModel, Field

# Define Pydantic model for structured output
{% if pydantic_model_code %}
{{ pydantic_model_code }}
{% else %}
class StructuredOutput(BaseModel):
    pass
{% endif %}


def main():
    # Load data
    {% if data_source == "CSV" %}
    df = daft.read_csv("{{ data_source_path }}")
    {% elif data_source == "JSONL" %}
    df = daft.read_json("{{ data_source_path }}")
    {% elif data_source == "Parquet" %}
    df = daft.read_parquet("{{ data_source_path }}")
    {% elif data_source == "Huggingface" %}
    df = daft.read_huggingface("{{ data_source_path }}")
    {% elif data_source == "Raw Files" %}
    df = daft.from_glob_path("{{ data_source_path }}")
    # Create content column from file paths
    df = df.with_column("content", file(df["path"]))
    {% else %}
    # TODO: Load your data here
    # Example: df = daft.read_csv("path/to/your/data.csv")
    df = None
    {% endif %}

    # Generate structured output
    {% if selected_columns | length == 1 %}
    # Single column input
    df = df.with_column(
        "{{ output_column }}",
        prompt(
            df["{{ selected_columns[0] }}"],
            return_format=StructuredOutput,
            {% if system_prompt %}
            system_message="{{ system_prompt }}",
            {% endif %}
        )
    )
    {% else %}
    # Multiple columns input
    df = df.with_column(
        "{{ output_column }}",
        prompt(
            [{% for col in selected_columns %}df["{{ col }}"]{% if not loop.last %}, {% endif %}{% endfor %}],
            return_format=StructuredOutput,
            {% if system_prompt %}
            system_message="{{ system_prompt }}",
            {% endif %}
        )
    )
    {% endif %}

    # Write output
    {% if output_data_source == "Parquet" %}
    df.write_parquet("output.parquet", write_mode="overwrite")
    {% elif output_data_source == "CSV" %}
    df.write_csv("output.csv", write_mode="overwrite")
    {% elif output_data_source == "JSON" %}
    df.write_json("output.json", write_mode="overwrite")
    {% elif output_data_source == "Iceberg" %}
    # TODO: Configure Iceberg table
    # df.write_iceberg(table, mode="append")
    {% elif output_data_source == "Delta Lake" %}
    df.write_deltalake("output_path", mode="overwrite")
    {% elif output_data_source == "Lance" %}
    df.write_lance("output.lance", mode="overwrite")
    {% elif output_data_source == "Clickhouse" %}
    # TODO: Configure Clickhouse connection
    # df.write_clickhouse("table_name", host="localhost", port=8123)
    {% elif output_data_source == "Huggingface" %}
    df.write_huggingface("your_username/your_repo")
    {% elif output_data_source == "Bigtable" %}
    # TODO: Configure Bigtable connection
    # df.write_bigtable(project_id="project", instance_id="instance", table_id="table", row_key_column="id")
    {% endif %}


if __name__ == "__main__":
    main()
